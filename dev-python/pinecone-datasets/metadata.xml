<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Pinecone Datasets## install```bashpip install pinecone-datasets```##  Usage - LoadingYou can use Pinecone Datasets to load our public datasets or with your own datasets. Datasets library can be used in 2 main ways: ad-hoc loading of datasets from a path or as a catalog loader for datasets. ### Loading Pinecone Public Datasets (catalog)Pinecone hosts a public datasets catalog, you can load a dataset by name using `list_datasets` and `load_dataset` functions. This will use the default catalog endpoint (currently GCS) to list and load datasets.```pythonfrom pinecone_datasets import list_datasets, load_datasetlist_datasets()# [&quot;quora_all-MiniLM-L6-bm25&quot;, ... ]dataset = load_dataset(&quot;quora_all-MiniLM-L6-bm25&quot;)dataset.head()# Prints# â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”# â”‚ id  â”† values                    â”† sparse_values                       â”† metadata          â”† blob â”‚# â”‚     â”†                           â”†                                     â”†                   â”†      â”‚# â”‚ str â”† list[f32]                 â”† struct[2]                           â”† struct[3]         â”†      â”‚# â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•¡# â”‚ 0   â”† [0.118014, -0.069717, ... â”† {[470065541, 52922727, ... 22364... â”† {2017,12,&quot;other&quot;} â”† .... â”‚# â”‚     â”† 0.0060...                 â”†                                     â”†                   â”†      â”‚# â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜```### Expected dataset structurepinecone datasets can load dataset from every storage where it has access (using the default access: s3, gcs or local permissions) we expect data to be uploaded to the following directory structure:    â”œâ”€â”€ my-subdir                     # path to where all datasets    â”‚   â”œâ”€â”€ my-dataset                # name of dataset    â”‚   â”‚   â”œâ”€â”€ metadata.json         # dataset metadata (optional, only for listed)    â”‚   â”‚   â”œâ”€â”€ documents             # datasets documents    â”‚   â”‚   â”‚   â”œâ”€â”€ file1.parquet          â”‚   â”‚   â”‚   â””â”€â”€ file2.parquet          â”‚   â”‚   â”œâ”€â”€ queries               # dataset queries    â”‚   â”‚   â”‚   â”œâ”€â”€ file1.parquet      â”‚   â”‚   â”‚   â””â”€â”€ file2.parquet       â””â”€â”€ ...The data schema is expected to be as follows:- `documents` directory contains parquet files with the following schema:    - Mandatory: `id: str, values: list[float]`    - Optional: `sparse_values: Dict: indices: List[int], values: List[float]`, `metadata: Dict`, `blob: dict`        - note: blob is a dict that can contain any data, it is not returned when iterating over the dataset and is inteded to be used for storing additional data that is not part of the dataset schema. however, it is sometime useful to store additional data in the dataset, for example, a document text. In future version this may become a first class citizen in the dataset schema.- `queries` directory contains parquet files with the following schema:    - Mandatory: `vector: list[float], top_k: int`    - Optional: `sparse_vector: Dict: indices: List[int], values: List[float]`, `filter: Dict`        - note: filter is a dict that contain pinecone filters, for more information see [here](https://docs.pinecone.io/docs/metadata-filtering)in addition, a metadata file is expected to be in the dataset directory, for example: `s3://my-bucket/my-dataset/metadata.json````pythonfrom pinecone_datasets.catalog import DatasetMetadatameta = DatasetMetadata(    name=&quot;test_dataset&quot;,    created_at=&quot;2023-02-17 14:17:01.481785&quot;,    documents=2,    queries=2,    source=&quot;manual&quot;,    bucket=&quot;LOCAL&quot;,    task=&quot;unittests&quot;,    dense_model={&quot;name&quot;: &quot;bert&quot;, &quot;dimension&quot;: 3},    sparse_model={&quot;name&quot;: &quot;bm25&quot;},)```full metadata schema can be found in `pinecone_datasets.catalog.DatasetMetadata.schema`### Loading your own dataset from catalogTo set you own catalog endpoint, set the environment variable `DATASETS_CATALOG_BASEPATH` to your bucket. Note that pinecone uses the default authentication method for the storage type (gcsfs for GCS and s3fs for S3).```bashexport DATASETS_CATALOG_BASEPATH=&quot;s3://my-bucket/my-subdir&quot;``````pythonfrom pinecone_datasets import list_datasets, load_datasetlist_datasets()# [&quot;my-dataset&quot;, ... ]dataset = load_dataset(&quot;my-dataset&quot;)```additionally, you can load a dataset from the Dataset class```pythonfrom pinecone_datasets import Datasetdataset = Dataset.from_catalog(&quot;my-dataset&quot;)```### Loading your own dataset from pathYou can load your own dataset from a local path or a remote path (GCS or S3). Note that pinecone uses the default authentication method for the storage type (gcsfs for GCS and s3fs for S3).```pythonfrom pinecone_datasets import Datasetdataset = Dataset.from_path(&quot;s3://my-bucket/my-subdir/my-dataset&quot;)```This assumes that the path is structured as described in the Expected dataset structure section### Loading from a pandas dataframePinecone Datasets enables you to load a dataset from a pandas dataframe. This is useful for loading a dataset from a local file and saving it to a remote storage.The minimal required data is a documents dataset, and the minimal required columns are `id` and `values`. The `id` column is a unique identifier for the document, and the `values` column is a list of floats representing the document vector.```pythonimport pandas as pddf = pd.read_parquet(&quot;my-dataset.parquet&quot;)metadata = DatasetMetadata(**metadata_dict)dataset = Dataset.from_pandas(documents = df, quries = None, metadata = metadata)```Please check the documentation for more information on the expected dataframe schema. There's also a column mapping variable that can be used to map the dataframe columns to the expected schema.## Usage - Accessing dataPinecone Datasets is build on top of pandas. This means that you can use all the pandas API to access the data. In addition, we provide some helper functions to access the data in a more convenient way. ### Accessing documents and queries dataframesaccessing the documents and queries dataframes is done using the `documents` and `queries` properties. These properties are lazy and will only load the data when accessed. ```pythondocument_df: pd.DataFrame = dataset.documentsquery_df: pd.DataFrame = dataset.queries```## Usage - IteratingOne of the main use cases for Pinecone Datasets is iterating over a dataset. This is useful for upserting a dataset to an index, or for benchmarking. It is also useful for iterating over large datasets - as of today, datasets are not yet lazy, however we are working on it.```python# List Iterator, where every list of size N Dicts with (&quot;id&quot;, &quot;values&quot;, &quot;sparse_values&quot;, &quot;metadata&quot;)dataset.iter_documents(batch_size=n) # Dict Iterator, where every dict has (&quot;vector&quot;, &quot;sparse_vector&quot;, &quot;filter&quot;, &quot;top_k&quot;)dataset.iter_queries()```### The 'blob' columnPinecone dataset ship with a blob column which is inteneded to be used for storing additional data that is not part of the dataset schema. however, it is sometime useful to store additional data in the dataset, for example, a document text. We added a utility function to move data from the blob column to the metadata column. This is useful for example when upserting a dataset to an index and want to use the metadata to store text data.```pythonfrom pinecone_datasets import import_documents_keys_from_blob_to_metadatanew_dataset = import_documents_keys_from_blob_to_metadata(dataset, keys=[&quot;text&quot;])```## Usage savingyou can save your dataset to a catalog managed by you or to a local path or a remote path (GCS or S3). ### Saving to CatalogTo set you own catalog endpoint, set the environment variable `DATASETS_CATALOG_BASEPATH` to your bucket. Note that pinecone uses the default authentication method for the storage type (gcsfs for GCS and s3fs for S3).After this environment variable is set you can save your dataset to the catalog using the `save` function```pythonfrom pinecone_datasets import Datasetmetadata = DatasetMetadata(**{&quot;name&quot;: &quot;my-dataset&quot;, ...})```---ğŸš¨ *NOTE* Dataset name in the metadata must match the `dataset_id` parameter you pass to the catalog, in this example 'my-dataset'---```pythondataset = Dataset.from_pandas(documents, queries, metadata)dataset.to_catalog(&quot;my-dataset&quot;)```### Saving to PathYou can save your dataset to a local path or a remote path (GCS or S3). Note that pinecone uses the default authentication method for the storage type (gcsfs for GCS and s3fs for S3).```pythondataset = Dataset.from_pandas(documents, queries, metadata)dataset.to_path(&quot;s3://my-bucket/my-subdir/my-dataset&quot;)```### upserting to IndexWhen upserting a Dataset to an Index, only the document data will be upserted to the index. The queries data will be ignored. TODO: add example for API Key adn Environment Variables```pythonds = load_dataset(&quot;dataset_name&quot;)ds.to_pinecone_index(&quot;index_name&quot;)# or, if you run in notebook environmentawait ds.to_pinecone_index_async(&quot;index_name&quot;)```the `to_index` function also accepts additional parameters* `batch_size` and `concurrency` - for controlling the upserting process* `kwargs` - for passing additional parameters to the index creation process## For developersThis project is using poetry for dependency managemet. supported python version are 3.8+. To start developing, on project root directory run:```bashpoetry install --with dev```To run test locally run ```bashpoetry run pytest --cov pinecone_datasets```</longdescription>
</pkgmetadata>