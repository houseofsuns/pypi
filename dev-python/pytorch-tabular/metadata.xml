<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>![PyTorch Tabular](docs/imgs/pytorch_tabular_logo.png)[![pypi](https://img.shields.io/pypi/v/pytorch_tabular.svg)](https://pypi.python.org/pypi/pytorch_tabular)[![Testing](https://github.com/manujosephv/pytorch_tabular/actions/workflows/testing.yml/badge.svg?event=push)](https://github.com/manujosephv/pytorch_tabular/actions/workflows/testing.yml)[![documentation status](https://readthedocs.org/projects/pytorch_tabular/badge/?version=latest)](https://pytorch_tabular.readthedocs.io/en/latest/?badge=latest)[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/manujosephv/pytorch_tabular/main.svg)](https://results.pre-commit.ci/latest/github/manujosephv/pytorch_tabular/main)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/manujosephv/pytorch_tabular/blob/main/docs/tutorials/01-Basic_Usage.ipynb)![PyPI - Downloads](https://img.shields.io/pypi/dm/pytorch_tabular)[![DOI](https://zenodo.org/badge/321584367.svg)](https://zenodo.org/badge/latestdoi/321584367)[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat-square)](https://github.com/manujosephv/pytorch_tabular/issues)PyTorch Tabular aims to make Deep Learning with Tabular data easy and accessible to real-world cases and research alike. The core principles behind the design of the library are:- Low Resistance Useability- Easy Customization- Scalable and Easier to DeployIt has been built on the shoulders of giants like **PyTorch**(obviously), and **PyTorch Lightning**.## Table of Contents- [Installation](#installation)- [Documentation](#documentation)- [Available Models](#available-models)- [Usage](#usage)- [Blogs](#blogs)- [Citation](#citation)## InstallationAlthough the installation includes PyTorch, the best and recommended way is to first install PyTorch from [here](https://pytorch.org/get-started/locally/), picking up the right CUDA version for your machine.Once, you have got Pytorch installed, just use:```bashpip install -U pytorch_tabular[extra]```to install the complete library with extra dependencies (Weights&amp;Biases &amp; Plotly).And :```bashpip install -U pytorch_tabular```for the bare essentials.The sources for pytorch_tabular can be downloaded from the `Github repo`\_.You can either clone the public repository:```bashgit clone git://github.com/manujosephv/pytorch_tabular```Once you have a copy of the source, you can install it with:```bashcd pytorch_tabular &amp;&amp; pip install .[extra]```## DocumentationFor complete Documentation with tutorials visit [ReadTheDocs](https://pytorch-tabular.readthedocs.io/en/latest/)## Available Models- FeedForward Network with Category Embedding is a simple FF network, but with an Embedding layers for the categorical columns.- [Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data](https://arxiv.org/abs/1909.06312) is a model presented in ICLR 2020 and according to the authors have beaten well-tuned Gradient Boosting models on many datasets.- [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442) is another model coming out of Google Research which uses Sparse Attention in multiple steps of decision making to model the output.- [Mixture Density Networks](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf) is a regression model which uses gaussian components to approximate the target function and  provide a probabilistic prediction out of the box.- [AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks](https://arxiv.org/abs/1810.11921) is a model which tries to learn interactions between the features in an automated way and create a better representation and then use this representation in downstream task- [TabTransformer](https://arxiv.org/abs/2012.06678) is an adaptation of the Transformer model for Tabular Data which creates contextual representations for categorical features.- FT Transformer from [Revisiting Deep Learning Models for Tabular Data](https://arxiv.org/abs/2106.11959)- [Gated Additive Tree Ensemble](https://arxiv.org/abs/2207.08548) is a novel high-performance, parameter and computationally efficient deep learning architecture for tabular data. GATE uses a gating mechanism, inspired from GRU, as a feature representation learning unit with an in-built feature selection mechanism. We combine it with an ensemble of differentiable, non-linear decision trees, re-weighted with simple self-attention to predict our desired output.**Semi-Supervised Learning**- [Denoising AutoEncoder](https://www.kaggle.com/code/faisalalsrheed/denoising-autoencoders-dae-for-tabular-data) is an autoencoder which learns robust feature representation, to compensate any noise in the dataset.To implement new models, see the [How to implement new models tutorial](https://github.com/manujosephv/pytorch_tabular/blob/main/docs/tutorials/04-Implementing%20New%20Architectures.ipynb). It covers basic as well as advanced architectures.## Usage```pythonfrom pytorch_tabular import TabularModelfrom pytorch_tabular.models import CategoryEmbeddingModelConfigfrom pytorch_tabular.config import (    DataConfig,    OptimizerConfig,    TrainerConfig,    ExperimentConfig,)data_config = DataConfig(    target=[        &quot;target&quot;    ],  # target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented    continuous_cols=num_col_names,    categorical_cols=cat_col_names,)trainer_config = TrainerConfig(    auto_lr_find=True,  # Runs the LRFinder to automatically derive a learning rate    batch_size=1024,    max_epochs=100,)optimizer_config = OptimizerConfig()model_config = CategoryEmbeddingModelConfig(    task=&quot;classification&quot;,    layers=&quot;1024-512-512&quot;,  # Number of nodes in each layer    activation=&quot;LeakyReLU&quot;,  # Activation between each layers    learning_rate=1e-3,)tabular_model = TabularModel(    data_config=data_config,    model_config=model_config,    optimizer_config=optimizer_config,    trainer_config=trainer_config,)tabular_model.fit(train=train, validation=val)result = tabular_model.evaluate(test)pred_df = tabular_model.predict(test)tabular_model.save_model(&quot;examples/basic&quot;)loaded_model = TabularModel.load_from_checkpoint(&quot;examples/basic&quot;)```## Blogs- [PyTorch Tabular – A Framework for Deep Learning for Tabular Data](https://deep-and-shallow.com/2021/01/27/pytorch-tabular-a-framework-for-deep-learning-for-tabular-data/)- [Neural Oblivious Decision Ensembles(NODE) – A State-of-the-Art Deep Learning Algorithm for Tabular Data](https://deep-and-shallow.com/2021/02/25/neural-oblivious-decision-ensemblesnode-a-state-of-the-art-deep-learning-algorithm-for-tabular-data/)- [Mixture Density Networks: Probabilistic Regression for Uncertainty Estimation](https://deep-and-shallow.com/2021/03/20/mixture-density-networks-probabilistic-regression-for-uncertainty-estimation/)## Future Roadmap(Contributions are Welcome)1. Integrate Optuna Hyperparameter Tuning1. Integrate SHAP for interpretability1. Add Variable Importance1. Add ability to use custom activations in CategoryEmbeddingModel1. Add GaussRank as Feature Transformation1. ~~Add differential dropouts(layer-wise) in CategoryEmbeddingModel~~1. ~~Add Fourier Encoding for cyclic time variables~~1. ~~Add Text and Image Modalities for mixed modal problems~~## Contributors&lt;!-- readme: contributors -start --&gt;&lt;table&gt;&lt;tr&gt;    &lt;td align=&quot;center&quot;&gt;        &lt;a href=&quot;https://github.com/manujosephv&quot;&gt;            &lt;img src=&quot;https://avatars.githubusercontent.com/u/10508493?v=4&quot; width=&quot;100;&quot; alt=&quot;manujosephv&quot;/&gt;            &lt;br /&gt;            &lt;sub&gt;&lt;b&gt;Manu Joseph&lt;/b&gt;&lt;/sub&gt;        &lt;/a&gt;    &lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;        &lt;a href=&quot;https://github.com/wsad1&quot;&gt;            &lt;img src=&quot;https://avatars.githubusercontent.com/u/13963626?v=4&quot; width=&quot;100;&quot; alt=&quot;wsad1&quot;/&gt;            &lt;br /&gt;            &lt;sub&gt;&lt;b&gt;Jinu Sunil&lt;/b&gt;&lt;/sub&gt;        &lt;/a&gt;    &lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;        &lt;a href=&quot;https://github.com/Borda&quot;&gt;            &lt;img src=&quot;https://avatars.githubusercontent.com/u/6035284?v=4&quot; width=&quot;100;&quot; alt=&quot;Borda&quot;/&gt;            &lt;br /&gt;            &lt;sub&gt;&lt;b&gt;Jirka Borovec&lt;/b&gt;&lt;/sub&gt;        &lt;/a&gt;    &lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;        &lt;a href=&quot;https://github.com/fonnesbeck&quot;&gt;            &lt;img src=&quot;https://avatars.githubusercontent.com/u/81476?v=4&quot; width=&quot;100;&quot; alt=&quot;fonnesbeck&quot;/&gt;            &lt;br /&gt;            &lt;sub&gt;&lt;b&gt;Chris Fonnesbeck&lt;/b&gt;&lt;/sub&gt;        &lt;/a&gt;    &lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;        &lt;a href=&quot;https://github.com/jxtrbtk&quot;&gt;            &lt;img src=&quot;https://avatars.githubusercontent.com/u/40494970?v=4&quot; width=&quot;100;&quot; alt=&quot;jxtrbtk&quot;/&gt;            &lt;br /&gt;            &lt;sub&gt;&lt;b&gt;Null&lt;/b&gt;&lt;/sub&gt;        &lt;/a&gt;    &lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;        &lt;a href=&quot;https://github.com/ndrsfel&quot;&gt;            &lt;img src=&quot;https://avatars.githubusercontent.com/u/21068727?v=4&quot; width=&quot;100;&quot; alt=&quot;ndrsfel&quot;/&gt;            &lt;br /&gt;            &lt;sub&gt;&lt;b&gt;Andreas&lt;/b&gt;&lt;/sub&gt;        &lt;/a&gt;    &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td align=&quot;center&quot;&gt;        &lt;a href=&quot;https://github.com/JulianRein&quot;&gt;            &lt;img src=&quot;https://avatars.githubusercontent.com/u/35046938?v=4&quot; width=&quot;100;&quot; alt=&quot;JulianRein&quot;/&gt;            &lt;br /&gt;            &lt;sub&gt;&lt;b&gt;Null&lt;/b&gt;&lt;/sub&gt;        &lt;/a&gt;    &lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;        &lt;a href=&quot;https://github.com/krshrimali&quot;&gt;            &lt;img src=&quot;https://avatars.githubusercontent.com/u/19997320?v=4&quot; width=&quot;100;&quot; alt=&quot;krshrimali&quot;/&gt;            &lt;br /&gt;            &lt;sub&gt;&lt;b&gt;Kushashwa Ravi Shrimali&lt;/b&gt;&lt;/sub&gt;        &lt;/a&gt;    &lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;        &lt;a href=&quot;https://github.com/Actis92&quot;&gt;            &lt;img src=&quot;https://avatars.githubusercontent.com/u/46601193?v=4&quot; width=&quot;100;&quot; alt=&quot;Actis92&quot;/&gt;            &lt;br /&gt;            &lt;sub&gt;&lt;b&gt;Luca Actis Grosso&lt;/b&gt;&lt;/sub&gt;        &lt;/a&gt;    &lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;        &lt;a href=&quot;https://github.com/sgbaird&quot;&gt;            &lt;img src=&quot;https://avatars.githubusercontent.com/u/45469701?v=4&quot; width=&quot;100;&quot; alt=&quot;sgbaird&quot;/&gt;            &lt;br /&gt;            &lt;sub&gt;&lt;b&gt;Sterling G. Baird&lt;/b&gt;&lt;/sub&gt;        &lt;/a&gt;    &lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;        &lt;a href=&quot;https://github.com/yinyunie&quot;&gt;            &lt;img src=&quot;https://avatars.githubusercontent.com/u/25686434?v=4&quot; width=&quot;100;&quot; alt=&quot;yinyunie&quot;/&gt;            &lt;br /&gt;            &lt;sub&gt;&lt;b&gt;Yinyu Nie&lt;/b&gt;&lt;/sub&gt;        &lt;/a&gt;    &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;!-- readme: contributors -end --&gt;## CitationIf you use PyTorch Tabular for a scientific publication, we would appreciate citations to the published software and the following paper:- [arxiv Paper](https://arxiv.org/abs/2104.13638)```@misc{joseph2021pytorch,      title={PyTorch Tabular: A Framework for Deep Learning with Tabular Data},      author={Manu Joseph},      year={2021},      eprint={2104.13638},      archivePrefix={arXiv},      primaryClass={cs.LG}}```- Zenodo Software Citation```@article{manujosephv_2021,    title={manujosephv/pytorch_tabular: v0.7.0-alpha},    DOI={10.5281/zenodo.5359010},    abstractNote={&lt;p&gt;Added a few more SOTA models - TabTransformer, FTTransformer        Made improvements in the model save and load capability        Made installation less restrictive by unfreezing some dependencies.&lt;/p&gt;},    publisher={Zenodo},    author={manujosephv},    year={2021},    month={May}}```# History## 1.0.1 (2023-01-20)- Bugfix for default metric for binary classification## 1.0.0 (2023-01-18)- Added a new task - Self Supervised Learning (SSL) and a separate training API for it.- Added new SOTA model - Gated Additive Tree Ensembles (GATE).- Added one SSL model - Denoising AutoEncoder.- Added lots of new tutorials and updated entire documentation.- Improved code documentation and type hints.- Separated a Model into separate Embedding, Backbone, and Head.- Refactored all models to separate Backbone as native PyTorch Model(nn.Module).- Refactored commonly used modules (layers, activations etc. to a common module).- Changed MixedDensityNetworks completely (breaking change). Now MDN is a head you can use with any model.- Enabled a low level api for training model.- Enabled saving and loading of datamodule.- Added trainer_kwargs to pass any trainer argument PyTorch Lightning supports.- Added Early Stopping and Model Checkpoint kwargs to use all the arguments in PyTorch Lightining.- Enabled prediction using GPUs in predict method.- Added `reset_model` to reset model weights to random.- Added many save and load functions including ONNX(experimental).- Added random seed as a parameter.- Switched over completely to Rich progressbars from tqdm.- Fixed class-balancing / mu propagation and set default to 1.0.- Added PyTorch Profiler for debugging performance issues.- Fixed bugs with FTTransformer and TabTransformer.- Updated MixedDensityNetworks fixing a bug with lambda_pi.- Many CI/CD improvements including complete integration with GitHub Actions.- Upgraded all dependencies, including PyTorch Lightning, pandas, to latest versions and added dependabot to manage it going forward.- Added pre-commit to ensure code integrity and standardization.## 0.7.0 (2021-09-01)- Implemented TabTransformer and FTTransformer models- Included capability to save a model using GPU an load in CPU- Made the temp folder pytorch tabular specific to avoid conflicts with other tmp folders.- Some bug fixes- Edited an error out of Advanced Tutorial in docs## 0.6.0 (2021-06-21)- Upgraded versions of PyTorch Lightning to 1.3.6- Changed the way `gpus` parameter is handled to avoid confusion. `None` is CPU, `-1` is all GPUs, `int` is number of GPUs- Added a few more Trainer Params like `deterministic`, `auto_select_gpus`- Some bug fixes and changes to docs- Added `seed_everything` to the fit method to ensure reproducibility- Refactored data_aware_initialization to be part of the BaseModel. Inherited Models can override the method to implement data aware initialization techniques## 0.5.0 (2021-03-18)- Added more documentation- Added Zenodo citation## 0.4.0 (2021-03-18)- Added AutoInt Model- Added Mixture Density Networks- Refactored the classes to separate backbones from the head of the models- Changed the saving and loading model to work for custom parameters that you pass in `fit`## 0.3.0 (2021-03-02)- Fixed a bug on inference## 0.2.0 (2021-02-07)- Fixed an issue with torch.clip and torch version- Fixed an issue with `gpus` parameter in TrainerConfig, by setting default value to `None` for CPU- Added feature to use custom sampler in the training dataloader- Updated documentation and added a new tutorial for imbalanced classification## 0.0.1 (2021-01-26)- First release on PyPI.</longdescription>
</pkgmetadata>