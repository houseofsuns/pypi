<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># ğŸŒ¿ Konoha: Simple wrapper of Japanese Tokenizers[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/himkt/konoha/blob/main/example/Konoha_Example.ipynb)&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/5164000/120913279-e7d62380-c6d0-11eb-8d17-6571277cdf27.gif&quot; width=&quot;95%&quot;&gt;&lt;/p&gt;[![GitHub stars](https://img.shields.io/github/stars/himkt/konoha?style=social)](https://github.com/himkt/konoha/stargazers)[![Downloads](https://pepy.tech/badge/konoha)](https://pepy.tech/project/konoha)[![Downloads](https://pepy.tech/badge/konoha/month)](https://pepy.tech/project/konoha/month)[![Downloads](https://pepy.tech/badge/konoha/week)](https://pepy.tech/project/konoha/week)[![Build Status](https://github.com/himkt/konoha/workflows/Python%20package/badge.svg?style=flat-square)](https://github.com/himkt/konoha/actions)[![Documentation Status](https://readthedocs.org/projects/konoha/badge/?version=latest)](https://konoha.readthedocs.io/en/latest/?badge=latest)![Python](https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8-blue?logo=python)[![PyPI](https://img.shields.io/pypi/v/konoha.svg)](https://pypi.python.org/pypi/konoha)[![GitHub Issues](https://img.shields.io/github/issues/himkt/konoha.svg?cacheSeconds=60&amp;color=yellow)](https://github.com/himkt/konoha/issues)[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/himkt/konoha.svg?cacheSeconds=60&amp;color=yellow)](https://github.com/himkt/konoha/issues)`Konoha` is a Python library for providing easy-to-use integrated interface of various Japanese tokenizers,which enables you to switch a tokenizer and boost your pre-processing.## Supported tokenizers&lt;a href=&quot;https://github.com/buruzaemon/natto-py&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/MeCab-natto--py-ff69b4&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/chezou/Mykytea-python&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/KyTea-Mykytea--python-ff69b4&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/mocobeta/janome&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Janome-janome-ff69b4&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/WorksApplications/SudachiPy&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Sudachi-sudachipy-ff69b4&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/google/sentencepiece&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Sentencepiece-sentencepiece-ff69b4&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/taishi-i/nagisa&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/nagisa-nagisa-ff69b4&quot;&gt;&lt;/a&gt;Also, `konoha` provides rule-based tokenizers (whitespace, character) and a rule-based sentence splitter.## Quick Start with DockerSimply run followings on your computer:```bashdocker run --rm -p 8000:8000 -t himkt/konoha  # from DockerHub```Or you can build image on your machine:```bashgit clone https://github.com/himkt/konoha  # download konohacd konoha &amp;&amp; docker-compose up --build  # build and launch container```Tokenization is done by posting a json object to `localhost:8000/api/v1/tokenize`.You can also batch tokenize by passing `texts: [&quot;ï¼‘ã¤ç›®ã®å…¥åŠ›&quot;, &quot;ï¼’ã¤ç›®ã®å…¥åŠ›&quot;]` to `localhost:8000/api/v1/batch_tokenize`.(API documentation is available on `localhost:8000/redoc`, you can check it using your web browser)Send a request using `curl` on your terminal.Note that a path to an endpoint is changed in v4.6.4.Please check our release note (https://github.com/himkt/konoha/releases/tag/v4.6.4).```json$ curl localhost:8000/api/v1/tokenize -X POST -H &quot;Content-Type: application/json&quot; \    -d '{&quot;tokenizer&quot;: &quot;mecab&quot;, &quot;text&quot;: &quot;ã“ã‚Œã¯ãƒšãƒ³ã§ã™&quot;}'{  &quot;tokens&quot;: [    [      {        &quot;surface&quot;: &quot;ã“ã‚Œ&quot;,        &quot;part_of_speech&quot;: &quot;åè©&quot;      },      {        &quot;surface&quot;: &quot;ã¯&quot;,        &quot;part_of_speech&quot;: &quot;åŠ©è©&quot;      },      {        &quot;surface&quot;: &quot;ãƒšãƒ³&quot;,        &quot;part_of_speech&quot;: &quot;åè©&quot;      },      {        &quot;surface&quot;: &quot;ã§ã™&quot;,        &quot;part_of_speech&quot;: &quot;åŠ©å‹•è©&quot;      }    ]  ]}```## InstallationI recommend you to install konoha by `pip install 'konoha[all]'`.- Install konoha with a specific tokenizer: `pip install 'konoha[(tokenizer_name)]`.- Install konoha with a specific tokenizer and remote file support: `pip install 'konoha[(tokenizer_name),remote]'`If you want to install konoha with a tokenizer, please install konoha with a specific tokenizer(e.g. `konoha[mecab]`, `konoha[sudachi]`, ...etc) or install tokenizers individually.## Example### Word level tokenization```pythonfrom konoha import WordTokenizersentence = 'è‡ªç„¶è¨€èªå‡¦ç†ã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™'tokenizer = WordTokenizer('MeCab')print(tokenizer.tokenize(sentence))# =&gt; [è‡ªç„¶, è¨€èª, å‡¦ç†, ã‚’, å‹‰å¼·, ã—, ã¦, ã„, ã¾ã™]tokenizer = WordTokenizer('Sentencepiece', model_path=&quot;data/model.spm&quot;)print(tokenizer.tokenize(sentence))# =&gt; [â–, è‡ªç„¶, è¨€èª, å‡¦ç†, ã‚’, å‹‰å¼·, ã—, ã¦ã„ã¾ã™]```For more detail, please see the `example/` directory.### Remote filesKonoha supports dictionary and model on cloud storage (currently supports Amazon S3).It requires installing konoha with the `remote` option, see [Installation](#installation).```python# download user dictionary from S3word_tokenizer = WordTokenizer(&quot;mecab&quot;, user_dictionary_path=&quot;s3://abc/xxx.dic&quot;)print(word_tokenizer.tokenize(sentence))# download system dictionary from S3word_tokenizer = WordTokenizer(&quot;mecab&quot;, system_dictionary_path=&quot;s3://abc/yyy&quot;)print(word_tokenizer.tokenize(sentence))# download model file from S3word_tokenizer = WordTokenizer(&quot;sentencepiece&quot;, model_path=&quot;s3://abc/zzz.model&quot;)print(word_tokenizer.tokenize(sentence))```### Sentence level tokenization```pythonfrom konoha import SentenceTokenizersentence = &quot;ç§ã¯çŒ«ã ã€‚åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ã€‚ã ãŒï¼Œã€Œã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚&quot;tokenizer = SentenceTokenizer()print(tokenizer.tokenize(sentence))# =&gt; ['ç§ã¯çŒ«ã ã€‚', 'åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ã€‚', 'ã ãŒï¼Œã€Œã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚']```You can change symbols for a sentence splitter and bracket expression.1. sentence splitter```pythonsentence = &quot;ç§ã¯çŒ«ã ã€‚åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ï¼ã ãŒï¼Œã€Œã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚&quot;tokenizer = SentenceTokenizer(period=&quot;ï¼&quot;)print(tokenizer.tokenize(sentence))# =&gt; ['ç§ã¯çŒ«ã ã€‚åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ï¼', 'ã ãŒï¼Œã€Œã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚']```2. bracket expression```pythonsentence = &quot;ç§ã¯çŒ«ã ã€‚åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ã€‚ã ãŒï¼Œã€ã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚&quot;tokenizer = SentenceTokenizer(    patterns=SentenceTokenizer.PATTERNS + [re.compile(r&quot;ã€.*?ã€&quot;)],)print(tokenizer.tokenize(sentence))# =&gt; ['ç§ã¯çŒ«ã ã€‚', 'åå‰ãªã‚“ã¦ã‚‚ã®ã¯ãªã„ã€‚', 'ã ãŒï¼Œã€ã‹ã‚ã„ã„ã€‚ãã‚Œã§ååˆ†ã ã‚ã†ã€ã€‚']```## Test```python -m pytest```## Article- [ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ã„ã„æ„Ÿã˜ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª konoha ã‚’ä½œã£ãŸ](https://qiita.com/klis/items/bb9ffa4d9c886af0f531)- [æ—¥æœ¬èªè§£æãƒ„ãƒ¼ãƒ« Konoha ã« AllenNLP é€£æºæ©Ÿèƒ½ã‚’å®Ÿè£…ã—ãŸ](https://qiita.com/klis/items/f1d29cb431d1bf879898)## AcknowledgementSentencepiece model used in test is provided by @yoheikikuta. Thanks!</longdescription>
</pkgmetadata>