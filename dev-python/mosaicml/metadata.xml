<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;br /&gt;&lt;p align=&quot;center&quot;&gt;    &lt;a href=&quot;https://github.com/mosaicml/composer#gh-light-mode-only&quot; class=&quot;only-light&quot;&gt;      &lt;img src=&quot;https://storage.googleapis.com/docs.mosaicml.com/images/header_light.svg&quot; width=&quot;50%&quot;/&gt;    &lt;/a&gt;    &lt;/p&gt;&lt;h2&gt;&lt;p align=&quot;center&quot;&gt;A PyTorch Library for Efficient Neural Network Training&lt;/p&gt;&lt;/h2&gt;&lt;h3&gt;&lt;p align=&quot;center&quot;&gt;Train Faster, Reduce Cost, Get Better Models&lt;/p&gt;&lt;/h3&gt;&lt;h4&gt;&lt;p align='center'&gt;&lt;a href=&quot;https://www.mosaicml.com&quot;&gt;[Website]&lt;/a&gt;- &lt;a href=&quot;https://docs.mosaicml.com/en/stable/getting_started/installation.html&quot;&gt;[Getting Started]&lt;/a&gt;- &lt;a href=&quot;https://docs.mosaicml.com/&quot;&gt;[Docs]&lt;/a&gt;- &lt;a href=&quot;https://docs.mosaicml.com/en/stable/method_cards/methods_overview.html&quot;&gt;[Methods]&lt;/a&gt;- &lt;a href=&quot;https://www.mosaicml.com/team&quot;&gt;[We're Hiring!]&lt;/a&gt;&lt;/p&gt;&lt;/h4&gt;&lt;p align=&quot;center&quot;&gt;    &lt;a href=&quot;https://pypi.org/project/mosaicml/&quot;&gt;        &lt;img alt=&quot;PyPi Version&quot; src=&quot;https://img.shields.io/pypi/pyversions/mosaicml&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://pypi.org/project/mosaicml/&quot;&gt;        &lt;img alt=&quot;PyPi Package Version&quot; src=&quot;https://img.shields.io/pypi/v/mosaicml&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://pepy.tech/project/mosaicml/&quot;&gt;        &lt;img alt=&quot;PyPi Downloads&quot; src=&quot;https://static.pepy.tech/personalized-badge/mosaicml?period=month&amp;units=international_system&amp;left_color=grey&amp;right_color=blue&amp;left_text=Downloads/month&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://docs.mosaicml.com/en/stable/&quot;&gt;        &lt;img alt=&quot;Documentation&quot; src=&quot;https://readthedocs.org/projects/composer/badge/?version=stable&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://join.slack.com/t/mosaicml-community/shared_invite/zt-w0tiddn9-WGTlRpfjcO9J5jyrMub1dg&quot;&gt;        &lt;img alt=&quot;Chat @ Slack&quot; src=&quot;https://img.shields.io/badge/slack-chat-2eb67d.svg?logo=slack&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/mosaicml/composer/blob/dev/LICENSE&quot;&gt;        &lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/badge/License-Apache%202.0-green.svg?logo=slack&quot;&gt;    &lt;/a&gt;&lt;/p&gt;&lt;br /&gt;# üëã WelcomeComposer is a PyTorch library that enables you to &lt;b&gt;train neural networks faster, at lower cost, and to higher accuracy&lt;/b&gt;. We've implemented more than two dozen speedup methods that can be applied to your training loop in just a few lines of code, or used with our built-in Trainer. We continually integrate the latest state-of-the-art in efficient neural network training.Composer features:- 20+ methods for speeding up training networks for computer vision and natural language. Don't waste hours trying to reproduce research papers when Composer has done the work for you.- An easy-to-use trainer that has been written to be as performant as possible and [integrates best practices](https://www.mosaicml.com/blog/5-best-practices-for-efficient-model-training) for efficient, multi-GPU training.- Functional forms of all of our speedup methods that allow you to integrate them into your existing training loop.- Strong, reproducible baselines to get you started as quickly as possible.## Benefits&lt;!-- start main results --&gt;&lt;p align=&quot;center&quot;&gt;  &lt;a href=&quot;https://storage.googleapis.com/docs.mosaicml.com/images/composer_graph_light_06212022.svg?ref=Fiey0Xei#gh-light-mode-only&quot; class=&quot;only-light&quot;&gt;    &lt;img src=&quot;https://storage.googleapis.com/docs.mosaicml.com/images/composer_graph_light_06212022.svg?ref=Fiey0Xei&quot; width=&quot;75%&quot;/&gt;  &lt;/a&gt;  &lt;!-- link to the light mode image even on dark mode, so it will be readable in a new tab --&gt;  &lt;/p&gt;&lt;!-- end main results --&gt;With no additional tuning, you can apply our methods to:&lt;!-- start numbers --&gt;- Train ResNet-50 on ImageNet to the standard 76.6% top-one accuracy for \$15 in 27 minutes (_with vanilla PyTorch:_ \$116 in 3.5 hours) on AWS.- Train GPT-2 125M to the standard perplexity of 24.11 for \$145 in 4.5 hours (_with vanilla PyTorch_: \$255 in 7.8 hours) on AWS.- Train DeepLab-v3 on ADE20k to the standard mean IOU of 45.7 for \$36 in 1.1 hours (_with vanilla PyTorch_: \$110 in 3.5 hours) on AWS.&lt;!-- end numbers --&gt;# üöÄ Quickstart## üíæ InstallationComposer is available with Pip:&lt;!--pytest.mark.skip--&gt;```bashpip install mosaicml```Alternatively, install Composer with Conda:&lt;!--pytest.mark.skip--&gt;```bashconda install -c mosaicml mosaicml```---## üöå UsageYou can use Composer's speedup methods in two ways:* Through a standalone **Functional API** (similar to `torch.nn.functional`) that allows you to integrate them into your existing training code.* Using Composer's built-in **Trainer**, which is designed to be performant and automatically takes care of the details of using speedup methods.### Example: Functional API [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/functional_api.ipynb)Integrate our speedup methods into your training loop with just a few lines of code, and see the results. Here we easily apply [BlurPool](https://docs.mosaicml.com/en/stable/method_cards/blurpool.html) and [SqueezeExcite](https://docs.mosaicml.com/en/stable/method_cards/squeeze_excite.html):&lt;!-- begin_example_1 ---&gt;```pythonimport composer.functional as cffrom torchvision import modelsmy_model = models.resnet18()# add blurpool and squeeze excite layerscf.apply_blurpool(my_model)cf.apply_squeeze_excite(my_model)# your own training code starts here```&lt;!-- end_example_1 ---&gt;For more examples, see the [Composer Functional API Colab notebook](https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/functional_api.ipynb) and [Functional API guide](https://docs.mosaicml.com/en/latest/functional_api.html).### Example: Trainer [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/getting_started.ipynb)For the best experience and the most efficient possible training, we recommend using Composer's built-in trainer, which automatically takes care of the details of using speedup methods and provides useful abstractions that facilitate rapid experimentation.&lt;!-- begin_example_2 ---&gt;&lt;!--pytest.mark.gpu--&gt;&lt;!--pytest.mark.filterwarnings(r'ignore:Some targets have less than 1 total probability:UserWarning')--&gt;&lt;!--```pythonimport torch# adaptive_avg_pool2d_backward_cuda in mnist_classifier is not deterministictorch.use_deterministic_algorithms(False)```--&gt;&lt;!--pytest-codeblocks:cont--&gt;```pythonfrom torch.utils.data import DataLoaderfrom torchvision import datasets, transformsfrom composer import Trainerfrom composer.algorithms import ChannelsLast, CutMix, LabelSmoothingfrom composer.models import mnist_modeltransform = transforms.Compose([transforms.ToTensor()])train_dataset = datasets.MNIST(&quot;data&quot;, download=True, train=True, transform=transform)eval_dataset = datasets.MNIST(&quot;data&quot;, download=True, train=False, transform=transform)train_dataloader = DataLoader(train_dataset, batch_size=128)eval_dataloader = DataLoader(eval_dataset, batch_size=128)trainer = Trainer(    model=mnist_model(),    train_dataloader=train_dataloader,    eval_dataloader=eval_dataloader,    max_duration=&quot;1ep&quot;,    algorithms=[        ChannelsLast(),        CutMix(alpha=1.0),        LabelSmoothing(smoothing=0.1),    ])trainer.fit()```&lt;!-- end_example_2 --&gt;Composer's built-in [trainer](https://docs.mosaicml.com/en/stable/trainer/using_the_trainer.html) makes it easy to **add multiple speedup methods in a single line of code!**Trying out new methods or combinations of methods is as easy as changing a single list.Here are some examples of methods available in Composer ([_see here for the full list_](https://docs.mosaicml.com/en/latest/trainer/algorithms.html)):Name|Attribution|tl;dr|Example Benchmark|Speed Up*|----|-----------|-----|---------|---------|[Alibi](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/alibi)|[Press et al, 2021](https://arxiv.org/abs/2108.12409)|Replace attention with AliBi.|GPT-2|1.5x[BlurPool](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/blurpool)|[Zhang, 2019](https://arxiv.org/abs/1904.11486)|Applies an anti-aliasing filter before every downsampling operation.|ResNet-101|1.2x[ChannelsLast](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/channels_last)|[PyTorch](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)|Uses channels last memory format (NHWC).|ResNet-101|1.5x[CutOut](https://docs.mosaicml.com/en/latest/method_cards/cutout.html)|[DeVries et al, 2017](https://arxiv.org/abs/1708.04552)|Randomly erases rectangular blocks from the image.|ResNet-101|1.2x[LabelSmoothing](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/label_smoothing)|[Szegedy et al, 2015](https://arxiv.org/abs/1512.00567)|Smooths the labels with a uniform prior|ResNet-101|1.5x[MixUp](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/mixup)|[Zhang et al, 2017](https://arxiv.org/abs/1710.09412)|Blends pairs of examples and labels.|ResNet-101|1.5x[RandAugment](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/randaugment)|[Cubuk et al, 2020](https://openaccess.thecvf.com/content_CVPRW_2020/html/w40/Cubuk_Randaugment_Practical_Automated_Data_Augmentation_With_a_Reduced_Search_Space_CVPRW_2020_paper.html)|Applies a series of random augmentations to each image.|ResNet-101|1.3x[SAM](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/sam)|[Foret et al, 2021](https://arxiv.org/abs/2010.01412)|An optimization strategy that seeks flatter minima.|ResNet-101|1.4x[SeqLengthWarmup](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/seq_length_warmup)|[Li et al, 2021](https://arxiv.org/abs/2108.06084)|Progressively increase sequence length.|GPT-2|1.2x[Stochastic Depth](https://docs.mosaicml.com/en/latest/method_cards/stochastic_depth.html)|[Huang et al, 2016](https://arxiv.org/abs/1603.09382)|Replaces a specified layer with a stochastic version that randomly drops the layer or samples during training|ResNet-101|1.1x&lt;p align=&quot;right&quot;&gt;* = time-to-train to the same quality as the baseline.&lt;/p&gt;## üõ† Building Speedup RecipesGiven two methods that speed up training by 1.5x each, do they combine to provide a 2.25x (1.5x * 1.5x) speedup? Not necessarily.They may optimize the [same part of the training process](https://en.wikipedia.org/wiki/Amdahl's_law) and lead to diminishing returns, or they may even interact in ways that prove detrimental.Determining which methods to compose together isn't as simple as assembling a set of methods that perform best individually.**We have come up with compositions of methods that work especially well together** through rigorous exploration of the design space of recipes and research on the science behind composition.The [MosaicML Explorer](https://app.mosaicml.com/) contains all of the data we have collected so far on composition, and it highlights the compositions of methods that are _pareto-optimal_ - that provide the **best possible tradeoffs between training time or cost and the quality of the trained model**.Whether you want to reach the same quality faster or get better quality within your current budget, Explorer can help you decide which speedup methods to use.We update this data regularly as we add new methods and develop better recipes.&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/docs.mosaicml.com/images/methods/explorer.png&quot;/&gt;&lt;/p&gt;As an example, here are two performant recipes, one for ResNet-101 on ImageNet, and the other for GPT-2 on OpenWebText, on 8xA100s:### ResNet-101Name|Functional|tl;dr|Benchmark|Speed Up----|----------|-----|---------|--------[Blur Pool](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/blurpool)|`cf.apply_blurpool`|[Applies an anti-aliasing filter before every downsampling operation.](https://arxiv.org/abs/1904.11486)|ResNet-101|1.2x[Channels Last](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/channels_last)|`cf.apply_`&lt;br&gt;`channels_last`|[Uses channels last memory format (NHWC).](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)|ResNet-101|1.5x[Label Smoothing](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/label_smoothing)|`cf.smooth_labels`|[Smooths the labels with a uniform prior.](https://arxiv.org/abs/1512.00567)|ResNet-101|1.5x[MixUp](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/mixup)|`CF.mixup_batch`|[Blends pairs of examples and labels.](https://arxiv.org/abs/1710.09412)|ResNet-101|1.5x[Progressive Resizing](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/progressive_resizing)|`cf.resize_batch`|[Increases the input image size during training.](https://github.com/fastai/fastbook/blob/780b76bef3127ce5b64f8230fce60e915a7e0735/07_sizing_and_tta.ipynb)|ResNet-101|1.3x[SAM](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/sam)|`N/A`|[SAM optimizer measures sharpness of optimization space.](https://arxiv.org/abs/2010.01412)|ResNet-101|1.5x**Composition** | `N/A` | **Cheapest: \$49 @ 78.1% Acc** | **ResNet-101** | **3.5x**### GPT-2Name|Functional|tl;dr|Benchmark|Speed Up----|----------|-----|---------|--------[Alibi](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/alibi)|`cf.apply_alibi`|[Replace attention with AliBi.](https://arxiv.org/abs/2108.12409)|GPT-2|1.6x[Seq Length Warmup](https://github.com/mosaicml/composer/tree/dev/composer/algorithms/seq_length_warmup)|`cf.set_batch_`&lt;br&gt;`sequence_length`|[Progressively increase sequence length.](https://arxiv.org/abs/2108.06084)|GPT-2|1.5x**Composition** | `N/A` | **Cheapest: \$145 @ 24.11 PPL** | **GPT-2** | **1.7x**# ‚öôÔ∏è What benchmarks does Composer support?We'll use the word _benchmark_ to denote a specific model trained on a specific dataset, with model quality assessed using a specific metric.Composer features computer vision and natural language processing benchmarks including (but not limited to):&lt;div class=&quot;center&quot;&gt;&lt;table&gt;&lt;thead&gt;  &lt;tr&gt;    &lt;th&gt;Model&lt;/th&gt;    &lt;th&gt;Dataset&lt;/th&gt;    &lt;th&gt;Loss&lt;/th&gt;    &lt;th&gt;Task&lt;/th&gt;    &lt;th&gt;Evaluation Metrics&lt;/th&gt;  &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;  &lt;tr&gt;      &lt;td colspan=&quot;5&quot; align=&quot;center&quot;&gt;&lt;b&gt;Computer Vision&lt;/b&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;ResNet Family&lt;/td&gt;    &lt;td&gt;CIFAR-10&lt;/td&gt;    &lt;td&gt;Cross Entropy&lt;/td&gt;    &lt;td&gt;Image Classification&lt;/td&gt;    &lt;td&gt;Classification Accuracy&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;ResNet Family&lt;/td&gt;    &lt;td&gt;ImageNet&lt;/td&gt;    &lt;td&gt;Cross Entropy&lt;/td&gt;    &lt;td&gt;Image Classification&lt;/td&gt;    &lt;td&gt;Classification Accuracy&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;EfficientNet Family&lt;/td&gt;    &lt;td&gt;ImageNet&lt;/td&gt;    &lt;td&gt;Cross Entropy&lt;/td&gt;    &lt;td&gt;Image Classification&lt;/td&gt;    &lt;td&gt;Classification Accuracy&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;UNet&lt;/td&gt;    &lt;td&gt;BraTS&lt;/td&gt;    &lt;td&gt;Dice Loss&lt;/td&gt;    &lt;td&gt;Image Segmentation&lt;/td&gt;    &lt;td&gt;Dice Coefficient&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;DeepLab v3&lt;/td&gt;    &lt;td&gt;ADE20K&lt;/td&gt;    &lt;td&gt;Cross Entropy&lt;/td&gt;    &lt;td&gt;Image Segmentation&lt;/td&gt;    &lt;td&gt;mIoU&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;      &lt;td align=&quot;center&quot; colspan=&quot;5&quot;&gt;&lt;b&gt;Natural Language Processing&lt;/b&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;BERT Family&lt;/td&gt;    &lt;td&gt;{Wikipedia &amp;amp; BooksCorpus, C4}&lt;/td&gt;    &lt;td&gt;Cross Entropy&lt;/td&gt;    &lt;td&gt;Masked Language Modeling&lt;/td&gt;    &lt;td&gt;GLUE &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;GPT Family&lt;/td&gt;    &lt;td&gt;{OpenWebText, C4}&lt;/td&gt;    &lt;td&gt;Cross Entropy&lt;/td&gt;    &lt;td&gt;Language Modeling&lt;br&gt;&lt;/td&gt;    &lt;td&gt;Perplexity&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;# ü§î Why should I use Composer?### SpeedThe compute required to train a state-of-the-art machine learning model is [doubling every 6 months](https://arxiv.org/abs/2202.05924), putting such models further and further out of reach for most researchers and practitioners with each passing day.Composer addresses this challenge by focusing on training efficiency: it contains cutting-edge speedup methods that modify the training algorithm to reduce the time and cost necessary to train deep learning models.**When you use Composer, you can rest assured that you are training efficiently.**We have combed the literature, done the science, and built industrial-grade implementations to ensure this is the case.### FlexibilityEven after these speedup methods are implemented, assembling them together into recipes is nontrivial. We designed Composer with the **right abstractions for composing (and creating new) speedup methods.**Specifically, Composer uses two-way callbacks ([Howard et al, 2020](https://arxiv.org/abs/2002.04688)) to modify the **entire training state** at particular events in the training loop to effect speedups. We handle collisions between methods, proper method ordering, and more.Through this, methods can modify: - data inputs for batches (data augmentations, sequence length warmup, skipping examples, etc.) - neural network architecture (pruning, model surgery, etc.) - loss function (label smoothing, MixUp, CutMix, etc.) - optimizer (Sharpness Aware Minimization) - training dynamics (layer freezing, selective backprop, etc.)You can easily [add your own methods](https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/custom_speedup_methods.ipynb) or callbacks to try out your ideas or modify any part of the training loop.### SupportComposer is an active and ongoing project. We will respond quickly to issues filed in this repository.# üßê Why shouldn‚Äôt I use Composer?* Composer is mostly optimized for computer vision and natural language processing. If you work on, e.g., reinforcement learning, you might encounter rough edges when using Composer.* Composer currently only supports NVIDIA GPUs, although we're working on adding alternatives.* Since Composer is still in alpha, our API may not be stable. We recommend pegging your work to a Composer version.# üìö Learn MoreHere are some resources actively maintained by the Composer community to help you get started:&lt;table&gt;&lt;thead&gt;  &lt;tr&gt;      &lt;th&gt;&lt;b&gt;Resource&lt;/b&gt;&lt;/th&gt;      &lt;th&gt;&lt;b&gt;Details&lt;/b&gt;&lt;/th&gt;  &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;  &lt;tr&gt;    &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/getting_started.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Getting started with our Trainer&lt;/a&gt;&lt;/td&gt;    &lt;td&gt;A Colab Notebook showing how to use our Trainer&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/functional_api.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Getting started with our Functional API&lt;/a&gt;&lt;/td&gt;    &lt;td&gt;A Colab Notebook showing how to use our Functional API&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/custom_speedup_methods.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Building Speedup Methods&lt;/a&gt;&lt;/td&gt;    &lt;td&gt;A Colab Notebook showing how to build new training modifications on top of Composer&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/finetune_huggingface.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Training BERTs with Composer and ü§ó &lt;/a&gt;&lt;/td&gt;    &lt;td&gt;A Colab Notebook showing how to train BERT models with Composer and ü§ó!&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;If you have any questions, please feel free to reach out to us on [Twitter](https://twitter.com/mosaicml), [email](mailto:community@mosaicml.com), or our [Community Slack](https://join.slack.com/t/mosaicml-community/shared_invite/zt-1dc6mo5wg-arlv6Oo9JjEn_g4d5s7PXQ)!# üí´ ContributorsComposer is part of the broader Machine Learning community, and we welcome any contributions, pull requests, or issues!To start contributing, see our [Contributing](https://github.com/mosaicml/composer/blob/dev/CONTRIBUTING.md) page.P.S.: [We're hiring](https://mosaicml.com/jobs)!# ‚úçÔ∏è Citation```@misc{mosaicml2022composer,    author = {The Mosaic ML Team},    title = {composer},    year = {2021},    howpublished = {\url{https://github.com/mosaicml/composer/}},}```</longdescription>
</pkgmetadata>