<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># 从知网HTML格式的文献中提取结构化文本本仓库是一个从知网CNKI的HTML格式的 &lt;b&gt;期刊论文&lt;/b&gt; 中提取结构化文本，然后导出 &lt;b&gt;JSON&lt;/b&gt; 文件的工具，方便对中文期刊论文进行更细维度的分析。最近更新：  - 从 `0.04` 版本开始，支持 `raw` 模式，可以保留章节文本中的参考文献标签核心功能：- 从文献的html字符串中解析出包含两级子章节标题的结构化文本- 实现了一个基于selenium的爬虫，可以批量获取所需文献的结构化文本- 提供三种模式，既可以导出结构化文本，也可以导出非结构化的纯文本  - `structure` 导出结构化文本，包含一级、二级子章节标题及对应的正文和参考文献索引  - `raw` 与 `structure` 模式相似，但是会在正文中保留参考文献标签，例如[1,2]  - `plain` 导出纯文本，不包含任何章节标题和参考文献索引  - 以上三种模式都包含参考文献文献，可以参考examples文件夹中给出的样例使用场景：- 对中文期刊论文进行全文获取和分析，为下游任务提供支持- 对中文期刊论文进行引文分析使用限制：- 确保你所在的网络环境能正常使用知网- 仅能提取 &lt;b&gt;期刊论文&lt;/b&gt; 的文本，其他文献类型未做测试- 无法提取文献中的图片、公式等，因此提取的内容可能不完整- 本工具已经尽力覆盖大多数情况，但不能保证每一篇文献都能正常提取## 快速开始本工具使用 Python 开发，请确保你的电脑上已经安装了 `Python3.8` 或以上版本。如果使用爬虫，需要提前配置好对应浏览器的驱动，例如chrome的驱动为 [chromedriver](https://chromedriver.chromium.org/downloads)```console$ python3 -m pip install cnki_html2json```## 使用方法1、调用 `html2json` 函数```pythonfrom cnki_html2json import html2jsonwith open('paper.html', 'r', encoding='utf-8') as f:    html = f.read()print(html2json.extract(html, mode='structure'))````html2json` 参数说明：| 参数 | 说明 || --- | --- || `paper_html` | 一篇期刊论文的html字符串 || `mode` | 可选值为 `structure`, `plain`, `raw`，默认为 `structure` || `export` | 是否导出，默认 `False` || `export_path` | 保存结果的路径，默认为 `None` ，如果 `export` 设置为 `True`，该参数必须指定 |&gt; 函数返回值说明：参考examples文件夹中给出的样例，调用该函数得到的结果仅包含正文内容和参考文献，不包括文献元数据2、使用命令行工具，启动一个selenium爬虫，然后在弹出的浏览器窗口(默认为Chrome)中进行检索```console$ 无需设置任何参数，直接运行$ cnki-crawler``````console$ 设置论文提取的起始索引和终止索引，模式设置为structure，不记录日志$ cnki-crawler -s 1 -e 100 -m structure -l false$ 或者是$ cnki-crawler --start_paper_index 1 --end_paper_index 100 -mode structure --log false``````console$ 查看可选参数$ cnki-crawler --help```参数说明：| 简写参数 | 参数 | 说明 || --- | --- | --- || s | `start_paper_index` | 论文提取的起始索引，默认为 `1`，从第一篇开始下载 || e | `end_paper_index` | 论文提取的终止索引，默认为 `None`，爬取到最后 || m | `mode` | 模式，可选值为 `structure`， `plain`， `raw`，默认为 `structure` || b | `browser` | 浏览器类型，Chrome(默认),可选Edge， Firefox， 不支持Safari || l | `log` | 是否记录日志，默认为 `true`，日志将保存在 `save_path` 下的 `log` 文件夹中 || save | `save_path` | 下载文件的保存路径，默认为当前目录的 `data` 文件夹 || wait | `wait_time` | 为检索预留的等待时间，默认 `120` 秒 |&gt; 提取结果可以参考examples文件夹中给出的样例。如果未指定保存路径，将下载结果默认保存在当前目录下的data文件夹下;  &gt; 由于提取的是文献正文，1分钟大概能下载3篇文献，可以放到夜间运行 (可以自动过滑块验证)```python# 也可以在代码中调用爬虫，可选参数参考上方的参数说明from cnki_html2json.crawl import start_crawlstart_crawl(start_paper_index=1,end_paper_index=100,mode='structure',log='false')```## json文件字段说明| 一级字段 | 二级字段 |三级字段| 说明 || --- | --- | --- | --- || metadata | title |  |论文标题||  | authors |  |论文作者||  | orgs |  |作者所属机构||  | abstract |  |论文摘要||  | keywords |  |论文关键词||  | funds |  |基金资助||  | class_num |  |分类号||  | source |  |发表期刊||  | issue |  |发表刊号|| body_text | 0 |  |不包含章节标题的首段或没有篇章结构的全文||  | 1.xxx(一级章节标题) | 1.1xxx(二级章节标题) |citation对应本章节的参考文献索引；text对应本章节的文本||  | 2.xxx | 2.1xxx |||  | ...| ... ||| 参考文献 |  |  |参考文献|## 工具推荐| 工具  | 描述 || --- | --- || [vermin](https://github.com/netromdk/vermin) | 从语法层面检测一个项目最低支持的 `Python` 版本 |## 责任声明- 使用者使用本工具造成的一切后果，由使用者自行承担</longdescription>
</pkgmetadata>