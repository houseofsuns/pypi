<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># 从知网HTML格式的文献中提取结构化文本本仓库是一个从知网CNKI的HTML格式的 &lt;b&gt;期刊论文&lt;/b&gt; 中提取结构化文本，然后导出 &lt;b&gt;JSON&lt;/b&gt; 文件的工具，方便对中文期刊论文进行更细维度的分析。最近更新：  - `v0.1.5` 对解析论文html的方式进行重构，提高了解析的准确率；核心功能：- 从文献的html字符串中解析出包含两级子章节标题的结构化文本；- 实现了一个基于 `selenium` 的爬虫，可以批量获取所需文献的结构化文本；- 提供三种模式，既可以导出结构化文本，也可以导出非结构化的纯文本；  - `structure` 导出结构化文本，包含一级、二级子章节标题及对应的正文和参考文献索引；  - `raw` 与 `structure` 模式相似，但是会在正文中保留参考文献标签，如[1,2]，以及换行符；  - `plain` 导出纯文本，不包含章节标题和参考文献索引；  - 以上三种模式都包含参考文献，可以参考 [examples](examples) 文件夹中给出的样例；&lt;!-- 使用场景：- 对中文期刊论文进行全文获取和分析，为下游任务提供支持；- 对中文期刊论文进行引文分析； --&gt;使用限制：- 确保你所在的网络环境能正常使用知网；- 仅能提取 &lt;b&gt;期刊论文&lt;/b&gt; 的文本，其他文献类型未做测试；- 无法提取文献中的图片、表格等，因此提取的内容可能不完整；- 本工具已经尽力覆盖大多数情况，但不能保证每一篇文献都能正常提取；## 快速开始本工具使用 Python 开发，请确保你的电脑上已经安装了 `Python3.8` 或以上版本。如果使用爬虫，需要提前配置好对应浏览器的驱动，例如chrome的驱动为 [chromedriver](https://chromedriver.chromium.org/downloads)```console$ pip install cnki_html2json```## 使用方法1、调用函数```pythonfrom cnki_html2json.html2json import ExtractContentwith open('paper.html', 'r', encoding='utf-8') as f:    raw_html = f.read()print(ExtractContent(raw_html).extract(mode='raw',export_path=None))````extract` 函数参数说明：| 参数 | 说明 || --- | --- || `mode` | 可选值为 `structure`, `plain`, `raw`，默认为 `raw` || `export_path` | 保存结果的路径，默认为 `None` ，则不保存为json文件 |&gt; 函数返回值说明：参考 [examples](examples) 文件夹中给出的样例，调用该函数得到的结果仅包含正文内容和参考文献，不包括文献元数据2、使用命令行工具，启动一个selenium爬虫，然后在弹出的浏览器窗口(默认为Chrome)中进行检索```console$ 无需设置任何参数，直接运行$ cnki-crawler``````console$ 设置论文提取的起始索引和终止索引，模式设置为structure，记录日志$ cnki-crawler -s 1 -e 100 -m structure -l$ 或者是$ cnki-crawler --start_paper_index 1 --end_paper_index 100 -mode structure --log``````console$ 查看可选参数$ cnki-crawler --help```命令行工具参数说明：|  | 参数 | 说明 || --- | --- | --- || -s | --start_paper_index | 论文提取的起始索引，默认为 `1`，从第一篇开始下载 || -e | --end_paper_index | 论文提取的终止索引，默认为 `None`，爬取到最后 || -m | --mode | 模式，可选值为 `raw(default)`、`structure`、`plain`|| -b | --browser | 浏览器类型，可选值为 `Chrome(default)`、`Edge`、`Firefox`|| -save | --save_path | 下载文件的保存路径，默认为当前目录的 `dataset` 文件夹 || -wait | --wait_time | 为检索预留的等待时间，默认 `120` 秒 || -l | --log | 是否记录日志，指定该参数则保存日志，无需传值，&lt;/br&gt;日志将保存在 `save_path` 下的 `log` 文件夹中 |&gt; 如果未指定保存路径，将下载结果默认保存在当前目录下的 `dataset` 文件夹下;  &gt; 由于提取的是文献正文，1分钟大概能下载3篇文献，可以放到夜间运行 (自动过滑块验证)```python# 也可以在代码中调用爬虫，可选参数参考上方的参数说明from cnki_html2json.crawl import start_crawlstart_crawl(start_paper_index=1,end_paper_index=100,mode='raw',log=True)```## json文件字段说明| 一级字段 | 二级字段 | 三级字段 | 说明 || --- | --- | --- | --- || metadata | title |  | 论文标题 ||  | authors |  | 论文作者 ||  | orgs |  | 作者所属机构 ||  | abstract |  | 论文摘要 ||  | keywords |  | 论文关键词 ||  | funds |  | 基金资助 ||  | class_num |  | 分类号 ||  | source |  | 来源期刊 ||  | issue |  | 刊号 || body_text | 1.xxx(一级章节标题) | 1.1xxx(二级章节标题) | reference_index对应本章节的参考文献索引；text对应本章节的文本 ||  | 2.xxx | 2.1xxx |  ||  | ...| ... |  || other | 作者贡献声明 |  |  ||  | 利益冲突声明 |  |  ||  | 参考文献 |  |  |## 开源协议本项目使用 [MIT](LICENSE) 协议，具体可查看 [LICENSE](LICENSE) 文件。</longdescription>
</pkgmetadata>