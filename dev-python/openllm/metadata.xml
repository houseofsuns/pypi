<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;  &lt;a href=&quot;https://github.com/bentoml/openllm&quot;&gt;    &lt;img src=&quot;https://raw.githubusercontent.com/bentoml/openllm/main/assets/main-banner.png&quot; alt=&quot;Banner for OpenLLM&quot; /&gt;  &lt;/a&gt;&lt;/p&gt;&lt;div align=&quot;center&quot;&gt;    &lt;h1 align=&quot;center&quot;&gt;ü¶æ OpenLLM&lt;/h1&gt;    &lt;a href=&quot;https://pypi.org/project/openllm&quot;&gt;        &lt;img src=&quot;https://img.shields.io/pypi/v/openllm.svg?logo=pypi&amp;label=PyPI&amp;logoColor=gold&quot; alt=&quot;pypi_status&quot; /&gt;    &lt;/a&gt;&lt;a href=&quot;https://twitter.com/bentomlai&quot;&gt;        &lt;img src=&quot;https://badgen.net/badge/icon/@bentomlai/1DA1F2?icon=twitter&amp;label=Follow%20Us&quot; alt=&quot;Twitter&quot; /&gt;    &lt;/a&gt;&lt;a href=&quot;https://l.bentoml.com/join-openllm-discord&quot;&gt;        &lt;img src=&quot;https://badgen.net/badge/icon/OpenLLM/7289da?icon=discord&amp;label=Join%20Us&quot; alt=&quot;Discord&quot; /&gt;    &lt;/a&gt;&lt;a href=&quot;https://github.com/bentoml/OpenLLM/actions/workflows/ci.yml&quot;&gt;        &lt;img src=&quot;https://github.com/bentoml/OpenLLM/actions/workflows/ci.yml/badge.svg?branch=main&quot; alt=&quot;ci&quot; /&gt;    &lt;/a&gt;&lt;a href=&quot;https://results.pre-commit.ci/latest/github/bentoml/OpenLLM/main&quot;&gt;        &lt;img src=&quot;https://results.pre-commit.ci/badge/github/bentoml/OpenLLM/main.svg&quot; alt=&quot;pre-commit.ci status&quot; /&gt;    &lt;/a&gt;&lt;br&gt;    &lt;a href=&quot;https://pypi.org/project/openllm&quot;&gt;        &lt;img src=&quot;https://img.shields.io/pypi/pyversions/openllm.svg?logo=python&amp;label=Python&amp;logoColor=gold&quot; alt=&quot;python_version&quot; /&gt;    &lt;/a&gt;&lt;a href=&quot;https://github.com/pypa/hatch&quot;&gt;        &lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A5%9A-Hatch-4051b5.svg&quot; alt=&quot;Hatch&quot; /&gt;    &lt;/a&gt;&lt;a href=&quot;https://github.com/bentoml/OpenLLM/blob/main/STYLE.md&quot;&gt;        &lt;img src=&quot;https://img.shields.io/badge/code%20style-experimental-000000.svg&quot; alt=&quot;code style&quot; /&gt;    &lt;/a&gt;&lt;a href=&quot;https://github.com/astral-sh/ruff&quot;&gt;        &lt;img src=&quot;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json&quot; alt=&quot;Ruff&quot; /&gt;    &lt;/a&gt;&lt;a href=&quot;https://github.com/python/mypy&quot;&gt;        &lt;img src=&quot;https://img.shields.io/badge/types-mypy-blue.svg&quot; alt=&quot;types - mypy&quot; /&gt;    &lt;/a&gt;&lt;a href=&quot;https://github.com/microsoft/pyright&quot;&gt;        &lt;img src=&quot;https://img.shields.io/badge/types-pyright-yellow.svg&quot; alt=&quot;types - pyright&quot; /&gt;    &lt;/a&gt;&lt;br&gt;    &lt;p&gt;An open platform for operating large language models (LLMs) in production.&lt;/br&gt;    Fine-tune, serve, deploy, and monitor any LLMs with ease.&lt;/p&gt;    &lt;i&gt;&lt;/i&gt;&lt;/div&gt;## üìñ IntroductionWith OpenLLM, you can run inference with any open-source large-language models,deploy to the cloud or on-premises, and build powerful AI apps.üöÇ **State-of-the-art LLMs**: built-in supports a wide range of open-source LLMsand model runtime, including Llama 2ÔºåStableLM, Falcon, Dolly, Flan-T5, ChatGLM,StarCoder and more.üî• **Flexible APIs**: serve LLMs over RESTful API or gRPC with one command,query via WebUI, CLI, our Python/Javascript client, or any HTTP client.‚õìÔ∏è **Freedom To Build**: First-class support for LangChain, BentoML and HuggingFace that allows you to easily create your own AI apps by composing LLMs withother models and services.üéØ **Streamline Deployment**: Automatically generate your LLM server DockerImages or deploy as serverless endpoint via[‚òÅÔ∏è BentoCloud](https://l.bentoml.com/bento-cloud).ü§ñÔ∏è **Bring your own LLM**: Fine-tune any LLM to suit your needs with`LLM.tuning()`. (Coming soon)&lt;p align=&quot;center&quot;&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/bentoml/openllm/main/assets/output.gif&quot; alt=&quot;Gif showing OpenLLM Intro&quot; /&gt;&lt;/p&gt;## üèÉ Getting StartedTo use OpenLLM, you need to have Python 3.8 (or newer) and `pip` installed onyour system. We highly recommend using a Virtual Environment to prevent packageconflicts.You can install OpenLLM using pip as follows:```bashpip install openllm```To verify if it's installed correctly, run:```$ openllm -hUsage: openllm [OPTIONS] COMMAND [ARGS]...   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë  ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù  An open platform for operating large language models in production.  Fine-tune, serve, deploy, and monitor any LLMs with ease.```### Starting an LLM ServerTo start an LLM server, use `openllm start`. For example, to start a[`OPT`](https://huggingface.co/docs/transformers/model_doc/opt) server, do thefollowing:```bashopenllm start opt```Following this, a Web UI will be accessible at http://localhost:3000 where youcan experiment with the endpoints and sample input prompts.OpenLLM provides a built-in Python client, allowing you to interact with themodel. In a different terminal window or a Jupyter Notebook, create a client tostart interacting with the model:```pythonimport openllmclient = openllm.client.HTTPClient('http://localhost:3000')client.query('Explain to me the difference between &quot;further&quot; and &quot;farther&quot;')```You can also use the `openllm query` command to query the model from theterminal:```bashexport OPENLLM_ENDPOINT=http://localhost:3000openllm query 'Explain to me the difference between &quot;further&quot; and &quot;farther&quot;'```Visit `http://localhost:3000/docs.json` for OpenLLM's API specification.OpenLLM seamlessly supports many models and their variants. Users can alsospecify different variants of the model to be served, by providing the`--model-id` argument, e.g.:```bashopenllm start flan-t5 --model-id google/flan-t5-large```&gt; [!NOTE]&gt; `openllm` also supports all variants of fine-tuning weights,&gt; custom model path as well as quantized weights for any of the supported models&gt; as long as it can be loaded with the model architecture. Refer to&gt; [supported models](https://github.com/bentoml/OpenLLM/tree/main#-supported-models)&gt; section for models' architecture.Use the `openllm models` command to see the list of models and their variantssupported in OpenLLM.## üß© Supported ModelsThe following models are currently supported in OpenLLM. By default, OpenLLMdoesn't include dependencies to run all models. The extra model-specificdependencies can be installed with the instructions below:&lt;!-- update-readme.py: start --&gt;&lt;table align='center'&gt;&lt;tr&gt;&lt;th&gt;Model&lt;/th&gt;&lt;th&gt;Architecture&lt;/th&gt;&lt;th&gt;Model Ids&lt;/th&gt;&lt;th&gt;Installation&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=https://github.com/THUDM/ChatGLM-6B&gt;chatglm&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=https://github.com/THUDM/ChatGLM-6B&gt;&lt;code&gt;ChatGLMForConditionalGeneration&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=https://huggingface.co/thudm/chatglm-6b&gt;&lt;code&gt;thudm/chatglm-6b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/thudm/chatglm-6b-int8&gt;&lt;code&gt;thudm/chatglm-6b-int8&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/thudm/chatglm-6b-int4&gt;&lt;code&gt;thudm/chatglm-6b-int4&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/thudm/chatglm2-6b&gt;&lt;code&gt;thudm/chatglm2-6b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/thudm/chatglm2-6b-int4&gt;&lt;code&gt;thudm/chatglm2-6b-int4&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;```bashpip install &quot;openllm[chatglm]&quot;```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=https://github.com/databrickslabs/dolly&gt;dolly-v2&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=https://huggingface.co/docs/transformers/main/model_doc/gpt_neox#transformers.GPTNeoXForCausalLM&gt;&lt;code&gt;GPTNeoXForCausalLM&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=https://huggingface.co/databricks/dolly-v2-3b&gt;&lt;code&gt;databricks/dolly-v2-3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/databricks/dolly-v2-7b&gt;&lt;code&gt;databricks/dolly-v2-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/databricks/dolly-v2-12b&gt;&lt;code&gt;databricks/dolly-v2-12b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;```bashpip install openllm```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=https://falconllm.tii.ae/&gt;falcon&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=https://falconllm.tii.ae/&gt;&lt;code&gt;FalconForCausalLM&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=https://huggingface.co/tiiuae/falcon-7b&gt;&lt;code&gt;tiiuae/falcon-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/tiiuae/falcon-40b&gt;&lt;code&gt;tiiuae/falcon-40b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/tiiuae/falcon-7b-instruct&gt;&lt;code&gt;tiiuae/falcon-7b-instruct&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/tiiuae/falcon-40b-instruct&gt;&lt;code&gt;tiiuae/falcon-40b-instruct&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;```bashpip install &quot;openllm[falcon]&quot;```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=https://huggingface.co/docs/transformers/model_doc/flan-t5&gt;flan-t5&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=https://huggingface.co/docs/transformers/main/model_doc/t5#transformers.T5ForConditionalGeneration&gt;&lt;code&gt;T5ForConditionalGeneration&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=https://huggingface.co/google/flan-t5-small&gt;&lt;code&gt;google/flan-t5-small&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/google/flan-t5-base&gt;&lt;code&gt;google/flan-t5-base&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/google/flan-t5-large&gt;&lt;code&gt;google/flan-t5-large&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/google/flan-t5-xl&gt;&lt;code&gt;google/flan-t5-xl&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/google/flan-t5-xxl&gt;&lt;code&gt;google/flan-t5-xxl&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;```bashpip install &quot;openllm[flan-t5]&quot;```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=https://github.com/EleutherAI/gpt-neox&gt;gpt-neox&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=https://huggingface.co/docs/transformers/main/model_doc/gpt_neox#transformers.GPTNeoXForCausalLM&gt;&lt;code&gt;GPTNeoXForCausalLM&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=https://huggingface.co/eleutherai/gpt-neox-20b&gt;&lt;code&gt;eleutherai/gpt-neox-20b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;```bashpip install openllm```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=https://github.com/facebookresearch/llama&gt;llama&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaForCausalLM&gt;&lt;code&gt;LlamaForCausalLM&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=https://huggingface.co/meta-llama/Llama-2-70b-chat-hf&gt;&lt;code&gt;meta-llama/Llama-2-70b-chat-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/meta-llama/Llama-2-13b-chat-hf&gt;&lt;code&gt;meta-llama/Llama-2-13b-chat-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&gt;&lt;code&gt;meta-llama/Llama-2-7b-chat-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/meta-llama/Llama-2-70b-hf&gt;&lt;code&gt;meta-llama/Llama-2-70b-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/meta-llama/Llama-2-13b-hf&gt;&lt;code&gt;meta-llama/Llama-2-13b-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/meta-llama/Llama-2-7b-hf&gt;&lt;code&gt;meta-llama/Llama-2-7b-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/NousResearch/llama-2-70b-chat-hf&gt;&lt;code&gt;NousResearch/llama-2-70b-chat-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/NousResearch/llama-2-13b-chat-hf&gt;&lt;code&gt;NousResearch/llama-2-13b-chat-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/NousResearch/llama-2-7b-chat-hf&gt;&lt;code&gt;NousResearch/llama-2-7b-chat-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/NousResearch/llama-2-70b-hf&gt;&lt;code&gt;NousResearch/llama-2-70b-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/NousResearch/llama-2-13b-hf&gt;&lt;code&gt;NousResearch/llama-2-13b-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/NousResearch/llama-2-7b-hf&gt;&lt;code&gt;NousResearch/llama-2-7b-hf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/openlm-research/open_llama_7b_v2&gt;&lt;code&gt;openlm-research/open_llama_7b_v2&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/openlm-research/open_llama_3b_v2&gt;&lt;code&gt;openlm-research/open_llama_3b_v2&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/openlm-research/open_llama_13b&gt;&lt;code&gt;openlm-research/open_llama_13b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/huggyllama/llama-65b&gt;&lt;code&gt;huggyllama/llama-65b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/huggyllama/llama-30b&gt;&lt;code&gt;huggyllama/llama-30b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/huggyllama/llama-13b&gt;&lt;code&gt;huggyllama/llama-13b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/huggyllama/llama-7b&gt;&lt;code&gt;huggyllama/llama-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;```bashpip install &quot;openllm[llama]&quot;```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=https://huggingface.co/mosaicml&gt;mpt&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=https://huggingface.co/mosaicml&gt;&lt;code&gt;MPTForCausalLM&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=https://huggingface.co/mosaicml/mpt-7b&gt;&lt;code&gt;mosaicml/mpt-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/mosaicml/mpt-7b-instruct&gt;&lt;code&gt;mosaicml/mpt-7b-instruct&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/mosaicml/mpt-7b-chat&gt;&lt;code&gt;mosaicml/mpt-7b-chat&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/mosaicml/mpt-7b-storywriter&gt;&lt;code&gt;mosaicml/mpt-7b-storywriter&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/mosaicml/mpt-30b&gt;&lt;code&gt;mosaicml/mpt-30b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/mosaicml/mpt-30b-instruct&gt;&lt;code&gt;mosaicml/mpt-30b-instruct&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/mosaicml/mpt-30b-chat&gt;&lt;code&gt;mosaicml/mpt-30b-chat&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;```bashpip install &quot;openllm[mpt]&quot;```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=https://huggingface.co/docs/transformers/model_doc/opt&gt;opt&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=https://huggingface.co/docs/transformers/main/model_doc/opt#transformers.OPTForCausalLM&gt;&lt;code&gt;OPTForCausalLM&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=https://huggingface.co/facebook/opt-125m&gt;&lt;code&gt;facebook/opt-125m&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/facebook/opt-350m&gt;&lt;code&gt;facebook/opt-350m&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/facebook/opt-1.3b&gt;&lt;code&gt;facebook/opt-1.3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/facebook/opt-2.7b&gt;&lt;code&gt;facebook/opt-2.7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/facebook/opt-6.7b&gt;&lt;code&gt;facebook/opt-6.7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/facebook/opt-66b&gt;&lt;code&gt;facebook/opt-66b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;```bashpip install &quot;openllm[opt]&quot;```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=https://github.com/Stability-AI/StableLM&gt;stablelm&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=https://huggingface.co/docs/transformers/main/model_doc/gpt_neox#transformers.GPTNeoXForCausalLM&gt;&lt;code&gt;GPTNeoXForCausalLM&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b&gt;&lt;code&gt;stabilityai/stablelm-tuned-alpha-3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b&gt;&lt;code&gt;stabilityai/stablelm-tuned-alpha-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/stabilityai/stablelm-base-alpha-3b&gt;&lt;code&gt;stabilityai/stablelm-base-alpha-3b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/stabilityai/stablelm-base-alpha-7b&gt;&lt;code&gt;stabilityai/stablelm-base-alpha-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;```bashpip install openllm```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=https://github.com/bigcode-project/starcoder&gt;starcoder&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=https://huggingface.co/docs/transformers/main/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM&gt;&lt;code&gt;GPTBigCodeForCausalLM&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=https://huggingface.co/bigcode/starcoder&gt;&lt;code&gt;bigcode/starcoder&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/bigcode/starcoderbase&gt;&lt;code&gt;bigcode/starcoderbase&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;```bashpip install &quot;openllm[starcoder]&quot;```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=https://github.com/baichuan-inc/Baichuan-7B&gt;baichuan&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=https://github.com/baichuan-inc/Baichuan-7B&gt;&lt;code&gt;BaiChuanForCausalLM&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=https://huggingface.co/baichuan-inc/baichuan-7b&gt;&lt;code&gt;baichuan-inc/baichuan-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/baichuan-inc/baichuan-13b-base&gt;&lt;code&gt;baichuan-inc/baichuan-13b-base&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/baichuan-inc/baichuan-13b-chat&gt;&lt;code&gt;baichuan-inc/baichuan-13b-chat&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/fireballoon/baichuan-vicuna-chinese-7b&gt;&lt;code&gt;fireballoon/baichuan-vicuna-chinese-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/fireballoon/baichuan-vicuna-7b&gt;&lt;code&gt;fireballoon/baichuan-vicuna-7b&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=https://huggingface.co/hiyouga/baichuan-7b-sft&gt;&lt;code&gt;hiyouga/baichuan-7b-sft&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;```bashpip install &quot;openllm[baichuan]&quot;```&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;!-- update-readme.py: stop --&gt;### Runtime Implementations (Experimental)Different LLMs may have multiple runtime implementations. For instance, theymight use Pytorch (`pt`), Tensorflow (`tf`), or Flax (`flax`).If you wish to specify a particular runtime for a model, you can do so bysetting the `OPENLLM_{MODEL_NAME}_FRAMEWORK={runtime}` environment variablebefore running `openllm start`.For example, if you want to use the Tensorflow (`tf`) implementation for the`flan-t5` model, you can use the following command:```bashOPENLLM_FLAN_T5_FRAMEWORK=tf openllm start flan-t5```&gt; [!NOTE]&gt; For GPU support on Flax, refers to&gt; [Jax's installation](https://github.com/google/jax#pip-installation-gpu-cuda-installed-via-pip-easier)&gt; to make sure that you have Jax support for the corresponding CUDA version.### QuantisationOpenLLM supports quantisation with[bitsandbytes](https://github.com/TimDettmers/bitsandbytes) and[GPTQ](https://arxiv.org/abs/2210.17323)```bashopenllm start mpt --quantize int8```To run inference with `gptq`, simply pass `--quantize gptq`:```bashopenllm start falcon --model-id TheBloke/falcon-40b-instruct-GPTQ --quantize gptq --device 0```&gt; [!NOTE]&gt; In order to run GPTQ, make sure to install with&gt; `pip install &quot;openllm[gptq]&quot;`. The weights of all supported models should be&gt; quantized before serving. See&gt; [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa) for more&gt; information on GPTQ quantisation.### Fine-tuning support (Experimental)One can serve OpenLLM models with any PEFT-compatible layers with`--adapter-id`:```bashopenllm start opt --model-id facebook/opt-6.7b --adapter-id aarnphm/opt-6-7b-quotes```It also supports adapters from custom paths:```bashopenllm start opt --model-id facebook/opt-6.7b --adapter-id /path/to/adapters```To use multiple adapters, use the following format:```bashopenllm start opt --model-id facebook/opt-6.7b --adapter-id aarnphm/opt-6.7b-lora --adapter-id aarnphm/opt-6.7b-lora:french_lora```By default, the first adapter-id will be the default Lora layer, but optionallyusers can change what Lora layer to use for inference via `/v1/adapters`:```bashcurl -X POST http://localhost:3000/v1/adapters --json '{&quot;adapter_name&quot;: &quot;vn_lora&quot;}'```Note that for multiple adapter-name and adapter-id, it is recommended to updateto use the default adapter before sending the inference, to avoid anyperformance degradationTo include this into the Bento, one can also provide a `--adapter-id` into`openllm build`:```bashopenllm build opt --model-id facebook/opt-6.7b --adapter-id ...```&gt; [!NOTE]&gt; We will gradually roll out support for fine-tuning all models. The&gt; following models contain fine-tuning support: OPT, Falcon, LlaMA.### Integrating a New ModelOpenLLM encourages contributions by welcoming users to incorporate their customLLMs into the ecosystem. Check out[Adding a New Model Guide](https://github.com/bentoml/OpenLLM/blob/main/ADDING_NEW_MODEL.md)to see how you can do it yourself.### EmbeddingsOpenLLM tentatively provides embeddings endpoint for supported models. This canbe accessed via `/v1/embeddings`.To use via CLI, simply call `openllm embed`:```bashopenllm embed --endpoint http://localhost:3000 &quot;I like to eat apples&quot; -o json{  &quot;embeddings&quot;: [    0.006569798570126295,    -0.031249752268195152,    -0.008072729222476482,    0.00847396720200777,    -0.005293501541018486,    ...&lt;many embeddings&gt;...    -0.002078012563288212,    -0.00676426338031888,    -0.002022686880081892  ],  &quot;num_tokens&quot;: 9}```To invoke this endpoint, use `client.embed` from the Python SDK:```pythonimport openllmclient = openllm.client.HTTPClient(&quot;http://localhost:3000&quot;)client.embed(&quot;I like to eat apples&quot;)```&gt; [!NOTE]&gt; Currently, the following model family supports embeddings: Llama, T5 (Flan-T5, FastChat, etc.), ChatGLM## ‚öôÔ∏è IntegrationsOpenLLM is not just a standalone product; it's a building block designed tointegrate with other powerful tools easily. We currently offer integration with[BentoML](https://github.com/bentoml/BentoML),[LangChain](https://github.com/hwchase17/langchain), and[Transformers Agents](https://huggingface.co/docs/transformers/transformers_agents).### BentoMLOpenLLM models can be integrated as a[Runner](https://docs.bentoml.com/en/latest/concepts/runner.html) in yourBentoML service. These runners have a `generate` method that takes a string as aprompt and returns a corresponding output string. This will allow you to plugand play any OpenLLM models with your existing ML workflow.```pythonimport bentomlimport openllmmodel = &quot;opt&quot;llm_config = openllm.AutoConfig.for_model(model)llm_runner = openllm.Runner(model, llm_config=llm_config)svc = bentoml.Service(    name=f&quot;llm-opt-service&quot;, runners=[llm_runner])@svc.api(input=Text(), output=Text())async def prompt(input_text: str) -&gt; str:    answer = await llm_runner.generate(input_text)    return answer```### [LangChain](https://python.langchain.com/docs/ecosystem/integrations/openllm)To quickly start a local LLM with `langchain`, simply do the following:```pythonfrom langchain.llms import OpenLLMllm = OpenLLM(model_name=&quot;llama&quot;, model_id='meta-llama/Llama-2-7b-hf')llm(&quot;What is the difference between a duck and a goose? And why there are so many Goose in Canada?&quot;)```&gt; [!IMPORTANT]&gt; By default, OpenLLM use `safetensors` format for saving models. If the model doesn't support safetensors,&gt; make sure to pass `serialisation=&quot;legacy&quot;` to use the legacy PyTorch bin format.`langchain.llms.OpenLLM` has the capability to interact with remote OpenLLMServer. Given there is an OpenLLM server deployed elsewhere, you can connect toit by specifying its URL:```pythonfrom langchain.llms import OpenLLMllm = OpenLLM(server_url='http://44.23.123.1:3000', server_type='grpc')llm(&quot;What is the difference between a duck and a goose? And why there are so many Goose in Canada?&quot;)```To integrate a LangChain agent with BentoML, you can do the following:```pythonllm = OpenLLM(    model_name='flan-t5',    model_id='google/flan-t5-large',    embedded=False,    serialisation=&quot;legacy&quot;)tools = load_tools([&quot;serpapi&quot;, &quot;llm-math&quot;], llm=llm)agent = initialize_agent(    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)svc = bentoml.Service(&quot;langchain-openllm&quot;, runners=[llm.runner])@svc.api(input=Text(), output=Text())def chat(input_text: str):    return agent.run(input_text)```&gt; [!NOTE]&gt; You can find out more examples under the&gt; [examples](https://github.com/bentoml/OpenLLM/tree/main/examples) folder.### Transformers AgentsOpenLLM seamlessly integrates with[Transformers Agents](https://huggingface.co/docs/transformers/transformers_agents).&gt; [!WARNING]&gt; The Transformers Agent is still at an experimental stage. It is&gt; recommended to install OpenLLM with `pip install -r nightly-requirements.txt`&gt; to get the latest API update for HuggingFace agent.```pythonimport transformersagent = transformers.HfAgent(&quot;http://localhost:3000/hf/agent&quot;)  # URL that runs the OpenLLM serveragent.run(&quot;Is the following `text` positive or negative?&quot;, text=&quot;I don't like how this models is generate inputs&quot;)```&gt; [!IMPORTANT]&gt; Only `starcoder` is currently supported with Agent integration. The&gt; example above was also run with four T4s on EC2 `g4dn.12xlarge`If you want to use OpenLLM client to ask questions to the running agent, you canalso do so:```pythonimport openllmclient = openllm.client.HTTPClient(&quot;http://localhost:3000&quot;)client.ask_agent(    task=&quot;Is the following `text` positive or negative?&quot;,    text=&quot;What are you thinking about?&quot;,)```&lt;p align=&quot;center&quot;&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/bentoml/openllm/main/assets/agent.gif&quot; alt=&quot;Gif showing Agent integration&quot; /&gt;&lt;/p&gt;## üöÄ Deploying to ProductionThere are several ways to deploy your LLMs:### üê≥ Docker container1. **Building a Bento**: With OpenLLM, you can easily build a Bento for a   specific model, like `dolly-v2`, using the `build` command.:   ```bash   openllm build dolly-v2   ```   A   [Bento](https://docs.bentoml.com/en/latest/concepts/bento.html#what-is-a-bento),   in BentoML, is the unit of distribution. It packages your program's source   code, models, files, artefacts, and dependencies.2. **Containerize your Bento**   ```bash   bentoml containerize &lt;name:version&gt;   ```   This generates a OCI-compatible docker image that can be deployed anywhere   docker runs. For best scalability and reliability of your LLM service in   production, we recommend deploy with BentoCloud„ÄÇ### ‚òÅÔ∏è BentoCloudDeploy OpenLLM with [BentoCloud](https://www.bentoml.com/bento-cloud/), theserverless cloud for shipping and scaling AI applications.1. **Create a BentoCloud account:** [sign up here](https://bentoml.com/cloud)   for early access2. **Log into your BentoCloud account:**   ```bash   bentoml cloud login --api-token &lt;your-api-token&gt; --endpoint &lt;bento-cloud-endpoint&gt;   ```&gt; [!NOTE]&gt; Replace `&lt;your-api-token&gt;` and `&lt;bento-cloud-endpoint&gt;` with your&gt; specific API token and the BentoCloud endpoint respectively.3. **Bulding a Bento**: With OpenLLM, you can easily build a Bento for a   specific model, such as `dolly-v2`:   ```bash   openllm build dolly-v2   ```4. **Pushing a Bento**: Push your freshly-built Bento service to BentoCloud via   the `push` command:   ```bash   bentoml push &lt;name:version&gt;   ```5. **Deploying a Bento**: Deploy your LLMs to BentoCloud with a single   `bentoml deployment create` command following the   [deployment instructions](https://docs.bentoml.com/en/latest/reference/cli.html#bentoml-deployment-create).## üë• CommunityEngage with like-minded individuals passionate about LLMs, AI, and more on our[Discord](https://l.bentoml.com/join-openllm-discord)!OpenLLM is actively maintained by the BentoML team. Feel free to reach out andjoin us in our pursuit to make LLMs more accessible and easy to use üëâ[Join our Slack community!](https://l.bentoml.com/join-slack)## üéÅ ContributingWe welcome contributions! If you're interested in enhancing OpenLLM'scapabilities or have any questions, don't hesitate to reach out in our[discord channel](https://l.bentoml.com/join-openllm-discord).Checkout our[Developer Guide](https://github.com/bentoml/OpenLLM/blob/main/DEVELOPMENT.md)if you wish to contribute to OpenLLM's codebase.## üçá TelemetryOpenLLM collects usage data to enhance user experience and improve the product.We only report OpenLLM's internal API calls and ensure maximum privacy byexcluding sensitive information. We will never collect user code, model data, orstack traces. For usage tracking, check out the[code](./src/openllm/utils/analytics.py).You can opt out of usage tracking by using the `--do-not-track` CLI option:```bashopenllm [command] --do-not-track```Or by setting the environment variable `OPENLLM_DO_NOT_TRACK=True`:```bashexport OPENLLM_DO_NOT_TRACK=True```## üìî CitationIf you use OpenLLM in your research, we provide a [citation](./CITATION.cff) touse:```bibtex@software{Pham_OpenLLM_Operating_LLMs_2023,author = {Pham, Aaron and Yang, Chaoyu and Sheng, Sean and  Zhao, Shenyang and Lee, Sauyon and Jiang, Bo and Dong, Fog and Guan, Xipeng and Ming, Frost},license = {Apache-2.0},month = jun,title = {{OpenLLM: Operating LLMs in production}},url = {https://github.com/bentoml/OpenLLM},year = {2023}}```## Release Information### Changes- Runners server now will always spawn one instance regardless of the configuration of workers-per-resource  i.e: If CUDA_VISIBLE_DEVICES=0,1,2 and `--workers-per-resource=0.5`, then runners will only use `0,1` index  [#189](https://github.com/bentoml/openllm/issues/189)### Features- OpenLLM now can also be installed via brew tap:  ```bash  brew tap bentoml/openllm https://github.com/bentoml/openllm  brew install openllm  ```  [#190](https://github.com/bentoml/openllm/issues/190)---[Click me for full changelog](https://github.com/bentoml/openllm/blob/main/CHANGELOG.md)</longdescription>
</pkgmetadata>