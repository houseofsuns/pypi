<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;h1 align=&quot;center&quot;&gt;ğŸƒâ™€ï¸ Runhouse ğŸ &lt;/h1&gt;[//]: # (&lt;p align=&quot;center&quot;&gt;)[//]: # (  &lt;a href=&quot;https://discord.gg/RnhB6589Hs&quot;&gt; )[//]: # (    &lt;img alt=&quot;Join Discord&quot; src=&quot;https://img.shields.io/discord/1065833240625172600?label=Discord&amp;style=for-the-badge&quot;&gt;)[//]: # (  &lt;/a&gt;)[//]: # (&lt;/p&gt;)## ğŸ‘µ Welcome Home!PyTorch lets you send a model or tensor `.to(device)`, sowhy can't you do `my_fn.to('a_gcp_a100')` or `my_table.to('parquet_in_s3')`?Runhouse allows just that: send code and data to any of your compute ordata infra (with your own cloud creds), all in Python, and continue to use themeagerly exactly as they were.Runhouse is for ML Researchers, Engineers, and Data Scientists who are tired of: - ğŸšœ manually shuttling code and data around between their local machine, remote instances, and cloud storage, - ğŸ“¤ğŸ“¥ constantly spinning up and down boxes, - ğŸœ debugging over ssh and notebook tunnels, - ğŸ§‘ğŸ”§ translating their code into a pipeline DSL just to use multiple hardware types, - ğŸª¦ debugging in an orchestrator, - ğŸ‘©âœˆï¸ missing out on fancy LLM IDE features, - ğŸ•µï¸ and struggling to find their teammates' code and data artifacts.By way of a visual,[//]: # (![img.png]&amp;#40;docs/assets/img.png&amp;#41;)[//]: # (![img_1.png]&amp;#40;docs/assets/img_1.png&amp;#41;)![img.png](https://raw.githubusercontent.com/run-house/runhouse/main/docs/assets/img.png)![img_1.png](https://raw.githubusercontent.com/run-house/runhouse/main/docs/assets/img_1.png)Take a look at this code (adapted from our first [tutorial](https://github.com/run-house/tutorials/tree/main/t01_Stable_Diffusion)):```pythonimport runhouse as rhfrom diffusers import StableDiffusionPipelineimport torchdef sd_generate(prompt, num_images=1, steps=100, guidance_scale=7.5, model_id='stabilityai/stable-diffusion-2-base'):    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, revision='fp16').to('cuda')    return pipe([prompt] * num_images, num_inference_steps=steps, guidance_scale=guidance_scale).imagesif __name__ == &quot;__main__&quot;:    gpu = rh.cluster(name='rh-v100', instance_type='V100:1', provider='gcp')    generate_gpu = rh.function(fn=sd_generate).to(gpu, reqs=['./', 'torch==1.12.0', 'diffusers'])    images = generate_gpu('A digital illustration of a woman running on the roof of a house.', num_images=2, steps=50)    [image.show() for image in images]    generate_gpu.save(name='sd_generate')```By saving, I or anyone I share with can load and call into this service with a single line of code, from anywherewith a Python interpreter and internet connection (notebook, IDE, CI/CD, orchestrator node, etc.):```pythongenerate_gpu = rh.function(name='sd_generate')images = generate_gpu(&quot;A hot dog made of matcha.&quot;)```There's no magic yaml, DSL, code serialization, or &quot;submitting for execution.&quot; We'rejust spinning up the cluster for you (or using an existing cluster), syncing over your code,starting a gRPC connection, and running your code on the cluster.**_Runhouse does things for you that you'd spend time doing yourself, in as obvious a way as possible._**And because it's not stateless, we can pin the model to GPU memory, and get ~1.5s/imageinference before any compilation.On the data side, we can do things like:```python# Send a folder up to a cluster (rsync)rh.folder(path=input_images_dir).to(system=gpu, path=&quot;dreambooth/instance_images&quot;)# This goes directly cluster-&gt; s3, doesn't bounce to localoutputs_s3 = rh.folder(system=gpu, path=&quot;dreambooth/outputs&quot;).to(&quot;s3&quot;, path=&quot;runhouse/dreambooth/outputs&quot;)outputs_s3.save(&quot;dreambooth_outputs&quot;)# and later:rh.folder(name=&quot;dreambooth_outputs&quot;).to(&quot;here&quot;)# Load a table in from anywhere (S3, GCS, local, etc)my_table = rh.table(system=&quot;gcs&quot;, path=&quot;my_bucket/my_table.parquet&quot;).to(&quot;here&quot;)# preprocess...gcs_ds = rh.table(preprocessed_dataset).to(&quot;gcs&quot;, path=&quot;my_bucket/preprocessed_table.parquet&quot;)gcs_ds.save(&quot;preprocessed-tokenized-dataset&quot;)# later, on another machine:preprocessed_table = rh.table(name=&quot;preprocessed-tokenized-dataset&quot;)for batch in preprocessed_table.stream(batch_size=batch_size):    ...# Send a model checkpoint up to blob storagetrained_model = rh.blob(data=pickle.dumps(model))trained_model.to(&quot;s3&quot;, path=&quot;runhouse/my_bucket&quot;).save(name=&quot;yelp_fine_tuned_bert&quot;)```These APIs work from anywhere with a Python interpreter and an internet connection,so notebooks, scripts, pipeline DSLs, etc. are all fair game. We currently support AWS,GCP, Azure, and Lambda Labs credentials through SkyPilot, as well as BYO cluster (just dropin an ip address and ssh key).## ğŸš¨ This is an Alpha ğŸš¨Runhouse is heavily under development and we expect to iterateon the APIs before reaching beta (version 0.1.0).## ğŸ£ Getting Startedtldr;```commandlinepip install runhouse# Or &quot;runhouse[aws]&quot;, &quot;runhouse[gcp]&quot;, &quot;runhouse[azure]&quot;, &quot;runhouse[all]&quot;sky check# Optionally, for portability (e.g. Colab):runhouse login```### ğŸ”Œ Installationâš ï¸ On Apple M1 or M2 machines âš ï¸, you will need to install grpcio with condabefore you install Runhouse - more specifically, before you install Ray.If you already have Ray installed, you can skip this.[See here](https://docs.ray.io/en/master/ray-overview/installation.html#m1-mac-apple-silicon-support)for how to install grpc properly on Apple silicon. You'll only know if you didthis correctly if you run `ray.init()` in a Python interpreter. If you'rehaving trouble with this, let us know.Runhouse can be installed with:```pip install runhouse```Depending on which cloud providers you plan to use, you can also install the followingadditional dependencies (to install the right versions of tools like boto, gsutil, etc.):```commandlinepip install &quot;runhouse[aws]&quot;pip install &quot;runhouse[gcp]&quot;pip install &quot;runhouse[azure]&quot;# Orpip install &quot;runhouse[all]&quot;```As this is an alpha, we push feature updates every few weeks as new microversions.### âœˆï¸ Verifying your Cloud Setup with SkyPilotRunhouse supports both BYO cluster, where you interact with existing compute via theirIP address and SSH key, and autoscaled clusters, where we spin up and down cloud instancesin your own cloud account for you. If you only plan to use BYO clusters, you candisregard the following.Runhouse uses [SkyPilot](https://skypilot.readthedocs.io/en/latest/) formuch of the heavy lifting with launching and terminating cloud instances.We love it and you should [throw them a Github star â­ï¸](https://github.com/skypilot-org/skypilot/).To verify that your cloud credentials are set up correctly for autoscaling, run```sky check```in your command line. This will confirm which cloud providers are ready touse, and will give detailed instructions if any setup is incomplete. SkyPilot alsoprovides an excellent suite of CLI commands for basic instance management operations.There are a few that you'll be reaching for frequently when using Runhouse with autoscalingthat you should familiarize yourself with,[here](https://runhouse-docs.readthedocs-hosted.com/en/latest/overview/compute.html#on-demand-clusters).### ğŸ”’ Creating a Runhouse Account for Secrets and PortabilityUsing Runhouse with only the OSS Python package is perfectly fine. However,you can unlock some unique portability features by creating an (always free)account on [api.run.house](https://api.run.house) and saving your secrets and/orresource metadata there. For example, you can open a Google Colab, call `runhouse login`,and all of your secrets or resources will be ready to use there with no additional setup.Think of the OSS-package-only experience as akin to Microsoft Office,while creating an account will make your cloud resources sharable and accessiblefrom anywhere like Google Docs. Youcan see examples of this portability in the[Runhouse Tutorials](https://github.com/run-house/tutorials).To create an account, visit [api.run.house](https://api.run.house),or simply call `runhouse login` from the command line (or`rh.login()` from Python).&gt; **Note**:These portability features only ever store light metadata about your resources(e.g. my_folder_name -&gt; [provider, bucket, path]) on our API servers. All the actual data and computestays inside your own cloud account and never hits our servers. The Secrets service storesyour secrets in Hashicorp Vault (an industry standard for secrets management), and our secretsAPIs simply call Vault's APIs. We never store secrets on our API servers. We plan to addsupport for BYO secrets management shortly. Let us know if you need it and which system you use.## ğŸ‘¨ğŸ« Tutorials / API Walkthrough / Docs[Tutorials can be found here](https://github.com/run-house/tutorials). They have been structured to provide acomprehensive walkthrough of the APIs.[Docs can be found here](https://runhouse-docs.readthedocs-hosted.com/en/latest/index.html).They include both high-level overviews of the architecture and detailed API references.## ğŸª FunhouseCheck out [Funhouse](https://github.com/run-house/funhouse) for running fun applications using Runhouse --think the latest Stable Diffusion models, text generation models, launching Gradio spaces, and even more!## ğŸ™‹â™‚ï¸ Getting HelpPlease join our [discord server here](https://discord.gg/RnhB6589Hs)to message us, or email us (first name at run.house), or create an issue.## ğŸ‘·â™€ï¸ ContributingWe welcome contributions! Please check out [contributing](CONTRIBUTING.md) if you're interested.</longdescription>
</pkgmetadata>