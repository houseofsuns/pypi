<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test_onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test_onnxruntime.yml)# Hugging Face OptimumðŸ¤— Optimum is an extension of ðŸ¤— Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use.## InstallationðŸ¤— Optimum can be installed using `pip` as follows:```bashpython -m pip install optimum```If you'd like to use the accelerator-specific features of ðŸ¤— Optimum, you can install the required dependencies according to the table below:| Accelerator                                                                                                            | Installation                                      ||:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|| [ONNX Runtime](https://onnxruntime.ai/docs/)                                                                           | `pip install --upgrade-strategy eager optimum[onnxruntime]`       || [Intel Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html)       | `pip install --upgrade-strategy eager optimum[neural-compressor]`|| [OpenVINO](https://docs.openvino.ai/latest/index.html)                                                                 | `pip install --upgrade-strategy eager optimum[openvino,nncf]`    || [Habana Gaudi Processor (HPU)](https://habana.ai/training/)                                                            | `pip install --upgrade-strategy eager optimum[habana]`           || [FuriosaAI](https://www.furiosa.ai/)                                                                                   | `pip install --upgrade-strategy eager optimum[furiosa]`          |The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.To install from source:```bashpython -m pip install git+https://github.com/huggingface/optimum.git```For the accelerator-specific features, append `optimum[accelerator_type]` to the above command:```bashpython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git```## Accelerated InferenceðŸ¤— Optimum provides multiple tools to export and run optimized models on various ecosystems: - [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)- TensorFlow Lite- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference)The [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line.### Features summary| Features                           | [ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview)| [Neural Compressor](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)| [OpenVINO](https://huggingface.co/docs/optimum/main/en/intel/inference)| [TensorFlow Lite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview)||:----------------------------------:|:------------------:|:------------------:|:------------------:|:------------------:|| Graph optimization                 | :heavy_check_mark: | N/A                | :heavy_check_mark: | N/A                || Post-training dynamic quantization | :heavy_check_mark: | :heavy_check_mark: | N/A                | :heavy_check_mark: || Post-training static quantization  | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: || Quantization Aware Training (QAT)  | N/A                | :heavy_check_mark: | :heavy_check_mark: | N/A                || FP16 (half precision)              | :heavy_check_mark: | N/A                | :heavy_check_mark: | :heavy_check_mark: || Pruning                            | N/A                | :heavy_check_mark: | :heavy_check_mark: | N/A                || Knowledge Distillation             | N/A                | :heavy_check_mark: | :heavy_check_mark: | N/A                |### OpenVINOThis requires to install the OpenVINO extra by doing `pip install --upgrade-strategy eager optimum[openvino,nncf]`To load a model and run inference with OpenVINO Runtime, you can just replace your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. To load a PyTorch checkpoint and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model.```diff- from transformers import AutoModelForSequenceClassification+ from optimum.intel import OVModelForSequenceClassification  from transformers import AutoTokenizer, pipeline  model_id = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;  tokenizer = AutoTokenizer.from_pretrained(model_id)- model = AutoModelForSequenceClassification.from_pretrained(model_id)+ model = OVModelForSequenceClassification.from_pretrained(model_id, export=True)  model.save_pretrained(&quot;./distilbert&quot;)  classifier = pipeline(&quot;text-classification&quot;, model=model, tokenizer=tokenizer)  results = classifier(&quot;He's a dreadful magician.&quot;)```You can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/inference) and in the [examples](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino).### Neural CompressorThis requires to install the Neural Compressor extra by doing `pip install --upgrade-strategy eager optimum[neural-compressor]`Dynamic quantization can be applied on your model:```bashoptimum-cli inc quantize --model distilbert-base-cased-distilled-squad --output ./quantized_distilbert```To load a model quantized with Intel Neural Compressor, hosted locally or on the ðŸ¤— hub, you can do as follows :```pythonfrom optimum.intel import INCModelForSequenceClassificationmodel_id = &quot;Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-dynamic&quot;model = INCModelForSequenceClassification.from_pretrained(model_id)```You can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/optimization_inc) and in the [examples](https://github.com/huggingface/optimum-intel/tree/main/examples/neural_compressor).### ONNX + ONNX RuntimeThis requires to install the ONNX Runtime extra by doing `pip install optimum[exporters,onnxruntime]`It is possible to export ðŸ¤— Transformers models to the [ONNX](https://onnx.ai/) format and perform graph optimization as well as quantization easily:```plainoptimum-cli export onnx -m deepset/roberta-base-squad2 --optimize O2 roberta_base_qa_onnx```The model can then be quantized using `onnxruntime`:```bashoptimum-cli onnxruntime quantize \  --avx512 \  --onnx_model roberta_base_qa_onnx \  -o quantized_roberta_base_qa_onnx```These commands will export `deepset/roberta-base-squad2` and perform [O2 graph optimization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization#optimization-configuration) on the exported model, and finally quantize it with the [avx512 configuration](https://huggingface.co/docs/optimum/main/en/onnxruntime/package_reference/configuration#optimum.onnxruntime.AutoQuantizationConfig.avx512).For more information on the ONNX export, please check the [documentation](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model).#### Run the exported model using ONNX RuntimeOnce the model is exported to the ONNX format, we provide Python classes enabling you to run the exported ONNX model in a seemless manner using [ONNX Runtime](https://onnxruntime.ai/) in the backend:```diff- from transformers import AutoModelForQuestionAnswering+ from optimum.onnxruntime import ORTModelForQuestionAnswering  from transformers import AutoTokenizer, pipeline  model_id = &quot;deepset/roberta-base-squad2&quot;  tokenizer = AutoTokenizer.from_pretrained(model_id)- model = AutoModelForQuestionAnswering.from_pretrained(model_id)+ model = ORTModelForQuestionAnswering.from_pretrained(&quot;roberta_base_qa_onnx&quot;)  qa_pipe = pipeline(&quot;question-answering&quot;, model=model, tokenizer=tokenizer)  question = &quot;What's Optimum?&quot;  context = &quot;Optimum is an awesome library everyone should use!&quot;  results = qa_pipe(question=question, context=context)```More details on how to run ONNX models with `ORTModelForXXX` classes [here](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models).### TensorFlow LiteThis requires to install the Exporters extra by doing `pip install optimum[exporters-tf]`Just as for ONNX, it is possible to export models to [TensorFlow Lite](https://www.tensorflow.org/lite) and quantize them:```plainoptimum-cli export tflite \  -m deepset/roberta-base-squad2 \  --sequence_length 384  \  --quantize int8-dynamic roberta_tflite_model```## Accelerated trainingðŸ¤— Optimum provides wrappers around the original ðŸ¤— Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) to enable training on powerful hardware easily.We support many providers:- Habana's Gaudi processors- ONNX Runtime (optimized for GPUs)### HabanaThis requires to install the Habana extra by doing `pip install --upgrade-strategy eager optimum[habana]````diff- from transformers import Trainer, TrainingArguments+ from optimum.habana import GaudiTrainer, GaudiTrainingArguments  # Download a pretrained model from the Hub  model = AutoModelForXxx.from_pretrained(&quot;bert-base-uncased&quot;)  # Define the training arguments- training_args = TrainingArguments(+ training_args = GaudiTrainingArguments(      output_dir=&quot;path/to/save/folder/&quot;,+     use_habana=True,+     use_lazy_mode=True,+     gaudi_config_name=&quot;Habana/bert-base-uncased&quot;,      ...  )  # Initialize the trainer- trainer = Trainer(+ trainer = GaudiTrainer(      model=model,      args=training_args,      train_dataset=train_dataset,      ...  )  # Use Habana Gaudi processor for training!  trainer.train()```You can find more examples in the [documentation](https://huggingface.co/docs/optimum/habana/quickstart) and in the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples).### ONNX Runtime```diff- from transformers import Trainer, TrainingArguments+ from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments  # Download a pretrained model from the Hub  model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;)  # Define the training arguments- training_args = TrainingArguments(+ training_args = ORTTrainingArguments(      output_dir=&quot;path/to/save/folder/&quot;,      optim=&quot;adamw_ort_fused&quot;,      ...  )  # Create a ONNX Runtime Trainer- trainer = Trainer(+ trainer = ORTTrainer(      model=model,      args=training_args,      train_dataset=train_dataset,+     feature=&quot;sequence-classification&quot;, # The model type to export to ONNX      ...  )  # Use ONNX Runtime for training!  trainer.train()```You can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer) and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training).</longdescription>
</pkgmetadata>