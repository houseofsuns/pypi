<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Truss**The simplest way to serve AI/ML models in production**[![PyPI version](https://badge.fury.io/py/truss.svg)](https://badge.fury.io/py/truss)[![ci_status](https://github.com/basetenlabs/truss/actions/workflows/release.yml/badge.svg)](https://github.com/basetenlabs/truss/actions/workflows/release.yml)## Why Truss?* **Write once, run anywhere:** Package and test model code, weights, and dependencies with a model server that behaves the same in development and production.* **Fast developer loop:** Implement your model with fast feedback from a live reload server, and skip Docker and Kubernetes configuration with a batteries-included model serving environment.* **Support for all Python frameworks**: From `transformers` and `diffusors` to `PyTorch` and `Tensorflow` to `XGBoost` and `sklearn`, Truss supports models created with any framework, even entirely custom models.See Trusses for popular models including:* ðŸ¦™ [Llama 2 7B](https://github.com/basetenlabs/truss-examples/tree/main/model_library/llama-2-7b-chat) ([13B](https://github.com/basetenlabs/truss-examples/tree/main/model_library/llama-2-13b-chat)) ([70B](https://github.com/basetenlabs/truss-examples/tree/main/model_library/llama-2-70b-chat))* ðŸŽ¨ [Stable Diffusion XL](https://github.com/basetenlabs/truss-examples/tree/main/stable-diffusion-xl-1.0)* ðŸ—£ [Whisper](https://github.com/basetenlabs/truss-examples/tree/main/whisper-truss)and [dozens more examples](examples/).## InstallationInstall Truss with:```pip install --upgrade truss```## QuickstartAs a quick example, we'll package a [text classification pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) from the open-source [`transformers` package](https://github.com/huggingface/transformers).### Create a TrussTo get started, create a Truss with the following terminal command:```shtruss init text-classification```When prompted, give your Truss a name like `Text classification`.Then, navigate to the newly created directory:```shcd text-classification```### Implement the modelOne of the two essential files in a Truss is `model/model.py`. In this file, you write a `Model` class: an interface between the ML model that you're packaging and the model server that you're running it on.There are two member functions that you must implement in the `Model` class:* `load()` loads the model onto the model server. It runs exactly once when the model server is spun up or patched.* `predict()` handles model inference. It runs every time the model server is called.Here's the complete `model/model.py` for the text classification model:```pythonfrom transformers import pipelineclass Model:    def __init__(self, **kwargs):        self._model = None    def load(self):        self._model = pipeline(&quot;text-classification&quot;)    def predict(self, model_input):        return self._model(model_input)```### Add model dependenciesThe other essential file in a Truss is `config.yaml`, which configures the model serving environment. For a complete list of the config options, see [the config reference](https://truss.baseten.co/reference/config).The pipeline model relies on [Transformers](https://huggingface.co/docs/transformers/index) and [PyTorch](https://pytorch.org/). These dependencies must be specified in the Truss config.In `config.yaml`, find the line `requirements`. Replace the empty list with:```yamlrequirements:  - torch==2.0.1  - transformers==4.30.0```No other configuration is needed.## DeploymentTruss is maintained by [Baseten](https://baseten.co), which provides infrastructure for running ML models in production. We'll use Baseten as the remote host for your model.Other remotes are coming soon, starting with AWS SageMaker.### Get an API keyTo set up the Baseten remote, you'll need a [Baseten API key](https://app.baseten.co/settings/account/api_keys). If you don't have a Baseten account, no worries, just [sign up for an account](https://app.baseten.co/signup/) and you'll be issued plenty of free credits to get you started.### Run `truss push`With your Baseten API key ready to paste when prompted, you can deploy your model:```shtruss push```You can monitor your model deployment from [your model dashboard on Baseten](https://app.baseten.co/models/).### Invoke the modelAfter the model has finished deploying, you can invoke it from the terminal.**Invocation**```shtruss predict -d '&quot;Truss is awesome!&quot;'```**Response**```json[  {    &quot;label&quot;: &quot;POSITIVE&quot;,    &quot;score&quot;: 0.999873161315918  }]```## Truss contributorsTruss is backed by Baseten and built in collaboration with ML engineers worldwide. Special thanks to [Stephan Auerhahn](https://github.com/palp) @ [stability.ai](https://stability.ai/) and [Daniel Sarfati](https://github.com/dsarfati) @ [Salad Technologies](https://salad.com/) for their contributions.We enthusiastically welcome contributions in accordance with our [contributors' guide](CONTRIBUTING.md) and [code of conduct](CODE_OF_CONDUCT.md).</longdescription>
</pkgmetadata>