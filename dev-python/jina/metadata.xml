<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;&lt;!-- survey banner start --&gt;&lt;a href=&quot;https://10sw1tcpld4.typeform.com/to/EGAEReM7?utm_source=readme&amp;utm_medium=github&amp;utm_campaign=user%20experience&amp;utm_term=feb2023&amp;utm_content=survey&quot;&gt;  &lt;img src=&quot;./.github/banner.svg?raw=true&quot;&gt;&lt;/a&gt;&lt;!-- survey banner start --&gt;&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.jina.ai&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/jina/blob/master/docs/_static/logo-light.svg?raw=true&quot; alt=&quot;Jina logo: Build multimodal AI services via cloud native technologies Â· Neural Search Â· Generative AI Â· Cloud Native&quot; width=&quot;150px&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;&lt;b&gt;Build multimodal AI services with cloud native technologies&lt;/b&gt;&lt;/p&gt;&lt;p align=center&gt;&lt;a href=&quot;https://pypi.org/project/jina/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/jina?label=Release&amp;style=flat-square&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://codecov.io/gh/jina-ai/jina&quot;&gt;&lt;img alt=&quot;Codecov branch&quot; src=&quot;https://img.shields.io/codecov/c/github/jina-ai/jina/master?&amp;logo=Codecov&amp;logoColor=white&amp;style=flat-square&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://jina.ai/slack&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-3.6k-blueviolet?logo=slack&amp;amp;logoColor=white&amp;style=flat-square&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://pypistats.org/packages/jina&quot;&gt;&lt;img alt=&quot;PyPI - Downloads from official pypistats&quot; src=&quot;https://img.shields.io/pypi/dm/jina?style=flat-square&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/jina-ai/jina/actions/workflows/cd.yml&quot;&gt;&lt;img alt=&quot;Github CD status&quot; src=&quot;https://github.com/jina-ai/jina/actions/workflows/cd.yml/badge.svg&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;!-- start jina-description --&gt;Jina is an MLOps framework to build multimodal AI microservice-based applications written in Python that can communicate via gRPC, HTTP and WebSocket protocols.It allows developers to build and serve **services** and **pipelines** while **scaling** and **deploying** them to a production while removing the complexity, letting them focus on the logic/algorithmic part, saving valuable time and resources for engineering teams.Jina aims to provide a smooth Pythonic experience transitioning from local deployment to deploying to advanced orchestration frameworks such as Docker-Compose, Kubernetes, or Jina AI Cloud.It handles the infrastructure complexity, making advanced solution engineering and cloud-native technologies accessible to every developer.&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;&lt;a href=&quot;#build-ai-services&quot;&gt;Build and deploy a gRPC microservice&lt;/a&gt; â€¢ &lt;a href=&quot;#build-a-pipeline&quot;&gt;Build and deploy a pipeline&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;Applications built with Jina enjoy the following features out of the box:ğŸŒŒ **Universal**  - Build applications that deliver fresh insights from multiple data types such as text, image, audio, video, 3D mesh, PDF with [LF's DocArray](https://github.com/docarray/docarray).  - Support for all mainstream deep learning frameworks.  - Polyglot gateway that supports gRPC, Websockets, HTTP, GraphQL protocols with TLS.âš¡ **Performance**  - Intuitive design pattern for high-performance microservices.  - Easy scaling: set replicas, sharding in one line.   - Duplex streaming between client and server.  - Async and non-blocking data processing over dynamic flows.â˜ï¸ **Cloud native**  - Seamless Docker container integration: sharing, exploring, sandboxing, versioning and dependency control via [Executor Hub](https://cloud.jina.ai).  - Full observability via OpenTelemetry, Prometheus and Grafana.  - Fast deployment to Kubernetes and Docker Compose.ğŸ± **Ecosystem**  - Improved engineering efficiency thanks to the Jina AI ecosystem, so you can focus on innovating with the data applications you build.  - Free CPU/GPU hosting via [Jina AI Cloud](https://cloud.jina.ai).Jina's value proposition may seem quite similar to that of FastAPI. However, there are several fundamental differences: **Data structure and communication protocols**  - FastAPI communication relies on Pydantic and Jina relies on [DocArray](https://github.com/docarray/docarray) allowing Jina to support multiple protocols  to expose its services. **Advanced orchestration and scaling capabilities**  - Jina lets you deploy applications formed from multiple microservices that can be containerized and scaled independently.  - Jina allows you to easily containerize and orchestrate your services, providing concurrency and scalability. **Journey to the cloud**  - Jina provides a smooth transition from local development (using [DocArray](https://github.com/docarray/docarray)) to local serving using (Jina's orchestration layer)  to having production-ready services by using Kubernetes capacity to orchestrate the lifetime of containers.  - By using [Jina AI Cloud](https://cloud.jina.ai) you have access to scalable and serverless deployments of your applications in one command.&lt;!-- end jina-description --&gt;&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;#&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/jina/blob/master/.github/readme/core-tree-graph.svg?raw=true&quot; alt=&quot;Jina in Jina AI neural search ecosystem&quot; width=&quot;100%&quot;&gt;&lt;/a&gt;&lt;/p&gt;## [Documentation](https://docs.jina.ai)## Install ```bashpip install jina transformers sentencepiece```Find more install options on [Apple Silicon](https://docs.jina.ai/get-started/install/apple-silicon-m1-m2/)/[Windows](https://docs.jina.ai/get-started/install/windows/).## Get Started### Basic ConceptsJina has four fundamental concepts:- A [**Document**](https://docarray.jina.ai/) (from [DocArray](https://github.com/docarray/docarray)) is the input/output format in Jina.- An [**Executor**](https://docs.jina.ai/concepts/executor/) is a Python class that transforms and processes Documents.- A [**Deployment**](https://docs.jina.ai/concepts/executor/serve/#serve-directly) serves a single Executor, while a [**Flow**](https://docs.jina.ai/concepts/flow/) serves Executors chained into a pipeline.[The full glossary is explained here](https://docs.jina.ai/concepts/preliminaries/#).---&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.jina.ai&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/jina/blob/master/.github/readme/streamline-banner.png?raw=true&quot; alt=&quot;Jina: Streamline AI &amp; ML Product Delivery&quot; width=&quot;100%&quot;&gt;&lt;/a&gt;&lt;/p&gt;### Build AI Services&lt;!-- start build-ai-services --&gt;[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jina-ai/jina/blob/master/.github/getting-started/notebook.ipynb)Let's build a fast, reliable and scalable gRPC-based AI service. In Jina we call this an **[Executor](https://docs.jina.ai/concepts/executor/)**. Our simple Executor will use Facebook's mBART-50 model to translate French to English. We'll then use a **Deployment** to serve it.&gt; **Note**&gt; A Deployment serves just one Executor. To combine multiple Executors into a pipeline and serve that, use a [Flow](#build-a-pipeline).&gt; **Note**&gt; Run the [code in Colab](https://colab.research.google.com/github/jina-ai/jina/blob/master/.github/getting-started/notebook.ipynb#scrollTo=0l-lkmz4H-jW) to install all dependencies.Let's implement the service's logic:&lt;table&gt;&lt;tr&gt;&lt;th&gt;&lt;code&gt;translate_executor.py&lt;/code&gt; &lt;/th&gt; &lt;tr&gt;&lt;td&gt;```pythonfrom docarray import DocumentArrayfrom jina import Executor, requestsfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLMclass Translator(Executor):    def __init__(self, **kwargs):        super().__init__(**kwargs)        self.tokenizer = AutoTokenizer.from_pretrained(            &quot;facebook/mbart-large-50-many-to-many-mmt&quot;, src_lang=&quot;fr_XX&quot;        )        self.model = AutoModelForSeq2SeqLM.from_pretrained(            &quot;facebook/mbart-large-50-many-to-many-mmt&quot;        )    @requests    def translate(self, docs: DocumentArray, **kwargs):        for doc in docs:            doc.text = self._translate(doc.text)    def _translate(self, text):        encoded_en = self.tokenizer(text, return_tensors=&quot;pt&quot;)        generated_tokens = self.model.generate(            **encoded_en, forced_bos_token_id=self.tokenizer.lang_code_to_id[&quot;en_XX&quot;]        )        return self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[            0        ]```&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;Then we deploy it with either the Python API or YAML:&lt;div class=&quot;table-wrapper&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;th&gt; Python API: &lt;code&gt;deployment.py&lt;/code&gt; &lt;/th&gt; &lt;th&gt; YAML: &lt;code&gt;deployment.yml&lt;/code&gt; &lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;```pythonfrom jina import Deploymentfrom translate_executor import Translatorwith Deployment(uses=Translator, timeout_ready=-1) as dep:    dep.block()```&lt;/td&gt;&lt;td&gt;```yamljtype: Deploymentwith:  uses: Translator  py_modules:    - translate_executor.py # name of the module containing Translator  timeout_ready: -1```And run the YAML Deployment with the CLI: `jina deployment --uses deployment.yml`&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;```textâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ‰ Deployment is ready to serve! â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ”— Endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®â”‚  â›“      Protocol                   GRPC â”‚â”‚  ğŸ         Local          0.0.0.0:12345  â”‚â”‚  ğŸ”’      Private      172.28.0.12:12345  â”‚â”‚  ğŸŒ       Public    35.230.97.208:12345  â”‚â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯```Use [Jina Client](https://docs.jina.ai/concepts/client/) to make requests to the service:```pythonfrom docarray import Documentfrom jina import Clientfrench_text = Document(    text='un astronaut est en train de faire une promenade dans un parc')client = Client(port=12345)  # use port from output aboveresponse = client.post(on='/', inputs=[french_text])print(response[0].text)``````textan astronaut is walking in a park```&lt;!-- end build-ai-services --&gt;&gt; **Note**&gt; In a notebook, one cannot use `deployment.block()` and then make requests to the client. Please refer to the colab link above for reproducible Jupyter Notebook code snippets.### Build a pipeline&lt;!-- start build-pipelines --&gt;[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jina-ai/jina/blob/master/.github/getting-started/notebook.ipynb#scrollTo=YfNm1nScH30U)Sometimes you want to chain microservices together into a pipeline. That's where a [Flow](https://docs.jina.ai/concepts/flow/) comes in.A Flow is a [DAG](https://de.wikipedia.org/wiki/DAG) pipeline, composed of a set of steps, It orchestrates a set of [Executors](https://docs.jina.ai/concepts/executor/) and a [Gateway](https://docs.jina.ai/concepts/gateway/) to offer an end-to-end service.&gt; **Note**&gt; If you just want to serve a single Executor, you can use a [Deployment](#build-ai--ml-services).For instance, let's combine [our French translation service](#build-ai--ml-services) with a Stable Diffusion image generation service from Jina AI's [Executor Hub](https://cloud.jina.ai/executors). Chaining these services together into a [Flow](https://docs.jina.ai/concepts/flow/) will give us a multilingual image generation service.Build the Flow with either Python or YAML:&lt;div class=&quot;table-wrapper&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;th&gt; Python API: &lt;code&gt;flow.py&lt;/code&gt; &lt;/th&gt; &lt;th&gt; YAML: &lt;code&gt;flow.yml&lt;/code&gt; &lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;```pythonfrom jina import Flowflow = (    Flow()    .add(uses=Translator, timeout_ready=-1)    .add(        uses='jinaai://jina-ai/TextToImage',        timeout_ready=-1,        install_requirements=True,    ))  # use the Executor from Executor hubwith flow:    flow.block()```&lt;/td&gt;&lt;td&gt;```yamljtype: Flowexecutors:  - uses: Translator    timeout_ready: -1    py_modules:      - translate_executor.py  - uses: jinaai://jina-ai/TextToImage    timeout_ready: -1    install_requirements: true```Then run the YAML Flow with the CLI: `jina flow --uses flow.yml`&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;```textâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ‰ Flow is ready to serve! â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ”— Endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®â”‚  â›“      Protocol                   GRPC  â”‚â”‚  ğŸ         Local          0.0.0.0:12345  â”‚â”‚  ğŸ”’      Private      172.28.0.12:12345  â”‚â”‚  ğŸŒ       Public    35.240.201.66:12345  â”‚â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯```Then, use [Jina Client](https://docs.jina.ai/concepts/client/) to make requests to the Flow:```pythonfrom jina import Client, Documentclient = Client(port=12345)  # use port from output abovefrench_text = Document(    text='un astronaut est en train de faire une promenade dans un parc')response = client.post(on='/', inputs=[french_text])response[0].display()```![stable-diffusion-output.png](https://raw.githubusercontent.com/jina-ai/jina/master/.github/stable-diffusion-output.png)You can also deploy a Flow to JCloud.First, turn the `flow.yml` file into a [JCloud-compatible YAML](https://docs.jina.ai/concepts/jcloud/yaml-spec/) by specifying resource requirements and using containerized Hub Executors.Then, use `jina cloud deploy` command to deploy to the cloud:```shellwget https://raw.githubusercontent.com/jina-ai/jina/master/.github/getting-started/jcloud-flow.ymljina cloud deploy jcloud-flow.yml```âš ï¸ **Caution: Make sure to delete/clean up the Flow once you are done with this tutorial to save resources and credits.**Read more about [deploying Flows to JCloud](https://docs.jina.ai/concepts/jcloud/#deploy).&lt;!-- end build-pipelines --&gt;Check [the getting-started project source code](https://github.com/jina-ai/jina/tree/master/.github/getting-started).---&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.jina.ai&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/jina/blob/master/.github/readme/no-complexity-banner.png?raw=true&quot; alt=&quot;Jina: No Infrastructure Complexity, High Engineering Efficiency&quot; width=&quot;100%&quot;&gt;&lt;/a&gt;&lt;/p&gt;Why not just use standard Python to build that microservice and pipeline? Jina accelerates time to market of your application by making it more scalable and cloud-native. Jina also handles the infrastructure complexity in production and other Day-2 operations so that you can focus on the data application itself.&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.jina.ai&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/jina/blob/master/.github/readme/scalability-banner.png?raw=true&quot; alt=&quot;Jina: Scalability and concurrency with ease&quot; width=&quot;100%&quot;&gt;&lt;/a&gt;&lt;/p&gt;### Easy scalability and concurrencyJina comes with scalability features out of the box like [replicas](https://docs.jina.ai/concepts/flow/scale-out/#replicate-executors), [shards](https://docs.jina.ai/concepts/flow/scale-out/#customize-polling-behaviors) and [dynamic batching](https://docs.jina.ai/concepts/executor/dynamic-batching/).This lets you easily increase your application's throughput.Let's scale a Stable Diffusion Executor deployment with replicas and dynamic batching:* Create two replicas, with [a GPU assigned for each](https://docs.jina.ai/concepts/flow/scale-out/#replicate-on-multiple-gpus).* Enable dynamic batching to process incoming parallel requests together with the same model inference.&lt;div class=&quot;table-wrapper&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;th&gt; Normal Deployment &lt;/th&gt; &lt;th&gt; Scaled Deployment &lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;```yamljtype: Deploymentwith:  timeout_ready: -1  uses: jinaai://jina-ai/TextToImage  install_requirements: true```&lt;/td&gt;&lt;td&gt;```yamljtype: Deploymentwith:  timeout_ready: -1  uses: jinaai://jina-ai/TextToImage  install_requirements: true  env:   CUDA_VISIBLE_DEVICES: RR  replicas: 2  uses_dynamic_batching: # configure dynamic batching    /default:      preferred_batch_size: 10      timeout: 200```&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;Assuming your machine has two GPUs, using the scaled deployment YAML will give better throughput compared to the normal deployment.These features apply to both [Deployment YAML](https://docs.jina.ai/concepts/executor/deployment-yaml-spec/#deployment-yaml-spec) and [Flow YAML](https://docs.jina.ai/concepts/flow/yaml-spec/). Thanks to the YAML syntax, you can inject deployment configurations regardless of Executor code.---&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.jina.ai&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/jina/blob/master/.github/readme/container-banner.png?raw=true&quot; alt=&quot;Jina: Seamless Container Integration&quot; width=&quot;100%&quot;&gt;&lt;/a&gt;&lt;/p&gt;### Seamless container integrationUse [Executor Hub](https://cloud.jina.ai) to share your Executors or use public/private Executors, with no need to worry about dependencies.To create an Executor:```bashjina hub new ```To push it to Executor Hub:```bashjina hub push .```To use a Hub Executor in your Flow:|        | Docker container                           | Sandbox                                     | Source                              ||--------|--------------------------------------------|---------------------------------------------|-------------------------------------|| YAML   | `uses: jinaai+docker://&lt;username&gt;/MyExecutor`        | `uses: jinaai+sandbox://&lt;username&gt;/MyExecutor`        | `uses: jinaai://&lt;username&gt;/MyExecutor`        || Python | `.add(uses='jinaai+docker://&lt;username&gt;/MyExecutor')` | `.add(uses='jinaai+sandbox://&lt;username&gt;/MyExecutor')` | `.add(uses='jinaai://&lt;username&gt;/MyExecutor')` |Executor Hub manages everything on the backend:- Automated builds on the cloud- Store, deploy, and deliver Executors cost-efficiently;- Automatically resolve version conflicts and dependencies;- Instant delivery of any Executor via [Sandbox](https://docs.jina.ai/concepts/executor/hub/sandbox/) without pulling anything to local.---&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.jina.ai&quot;&gt;&lt;img src=&quot;.github/readme/cloud-native-banner.png?raw=true&quot; alt=&quot;Jina: Seamless Container Integration&quot; width=&quot;100%&quot;&gt;&lt;/a&gt;&lt;/p&gt;### Get on the fast lane to cloud-nativeUsing Kubernetes with Jina is easy:```bashjina export kubernetes flow.yml ./my-k8skubectl apply -R -f my-k8s```And so is Docker Compose:```bashjina export docker-compose flow.yml docker-compose.ymldocker-compose up```&gt; **Note**&gt; You can also export Deployment YAML to [Kubernetes](https://docs.jina.ai/concepts/executor/serve/#serve-via-kubernetes) and [Docker Compose](https://docs.jina.ai/concepts/executor/serve/#serve-via-docker-compose).Likewise, tracing and monitoring with OpenTelemetry is straightforward:```pythonfrom docarray import DocumentArrayfrom jina import Executor, requestsclass Encoder(Executor):    @requests    def encode(self, docs: DocumentArray, **kwargs):        with self.tracer.start_as_current_span(            'encode', context=tracing_context        ) as span:            with self.monitor(                'preprocessing_seconds', 'Time preprocessing the requests'            ):                docs.tensors = preprocessing(docs)            with self.monitor(                'model_inference_seconds', 'Time doing inference the requests'            ):                docs.embedding = model_inference(docs.tensors)```You can integrate Jaeger or any other distributed tracing tools to collect and visualize request-level and application level service operation attributes. This helps you analyze request-response lifecycle, application behavior and performance.To use Grafana, [download this JSON](https://github.com/jina-ai/example-grafana-prometheus/blob/main/grafana-dashboards/flow-histogram-metrics.json) and import it into Grafana:&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.jina.ai&quot;&gt;&lt;img src=&quot;.github/readme/grafana-histogram-metrics.png?raw=true&quot; alt=&quot;Jina: Seamless Container Integration&quot; width=&quot;70%&quot;&gt;&lt;/a&gt;&lt;/p&gt;To trace requests with Jaeger:&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.jina.ai&quot;&gt;&lt;img src=&quot;.github/readme/jaeger-tracing-example.png?raw=true&quot; alt=&quot;Jina: Seamless Container Integration&quot; width=&quot;70%&quot;&gt;&lt;/a&gt;&lt;/p&gt;What cloud-native technology is still challenging to you? [Tell us](https://github.com/jina-ai/jina/issues) and we'll handle the complexity and make it easy for you.&lt;!-- start support-pitch --&gt;## Support- Join our [Slack community](https://jina.ai/slack) and chat with other community members about ideas.- Join our [Engineering All Hands](https://youtube.com/playlist?list=PL3UBBWOUVhFYRUa_gpYYKBqEAkO4sxmne) meet-up to discuss your use case and learn Jina's new features.    - **When?** The second Tuesday of every month    - **Where?**      Zoom ([see our public events calendar](https://calendar.google.com/calendar/embed?src=c_1t5ogfp2d45v8fit981j08mcm4%40group.calendar.google.com&amp;ctz=Europe%2FBerlin)/[.ical](https://calendar.google.com/calendar/ical/c_1t5ogfp2d45v8fit981j08mcm4%40group.calendar.google.com/public/basic.ics))      and [live stream on YouTube](https://youtube.com/c/jina-ai)- Subscribe to the latest video tutorials on our [YouTube channel](https://youtube.com/c/jina-ai)## Join UsJina is backed by [Jina AI](https://jina.ai) and licensed under [Apache-2.0](./LICENSE).&lt;!-- end support-pitch --&gt;</longdescription>
</pkgmetadata>