<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;  Intel¬Æ Extension for Transformers===========================&lt;h3&gt;An Innovative Transformer-based Toolkit to Accelerate GenAI/LLM Everywhere&lt;/h3&gt;[![](https://dcbadge.vercel.app/api/server/Wxk3J3ZJkU?compact=true&amp;style=flat-square)](https://discord.gg/Wxk3J3ZJkU)[![Release Notes](https://img.shields.io/github/v/release/intel/intel-extension-for-transformers)](https://github.com/intel/intel-extension-for-transformers/releases)[üè≠Architecture](./docs/architecture.md)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[üí¨NeuralChat](./intel_extension_for_transformers/neural_chat)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[üòÉInference](./intel_extension_for_transformers/llm/runtime/graph)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[üíªExamples](./docs/examples.md)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[üìñDocumentations](https://intel.github.io/intel-extension-for-transformers/latest/docs/Welcome.html)&lt;/div&gt;## üöÄLatest News* &lt;b&gt;NeuralChat has been showcased in [Intel Innovation‚Äô23 Keynote](https://www.youtube.com/watch?v=RbKRELWP9y8&amp;t=2954s) and [Google Cloud Next'23](https://cloud.google.com/blog/topics/google-cloud-next/welcome-to-google-cloud-next-23) to demonstrate GenAI/LLM capabilities on Intel Xeon Scalable Processors.&lt;/b&gt;* &lt;b&gt;NeuralChat supports custom chatbot development and deployment on broad Intel HWs such as Xeon Scalable Processors, Gaudi2,¬†Xeon CPU Max Series,¬†Data Center GPU Max Series, Arc Series, and Core Processors. Check out [Notebooks](./intel_extension_for_transformers/neural_chat/docs/full_notebooks.md) and below sample code. &lt;/b&gt;```python# pip install intel-extension-for-transformersfrom intel_extension_for_transformers.neural_chat import build_chatbotchatbot = build_chatbot()response = chatbot.predict(&quot;Tell me about Intel Xeon Scalable Processors.&quot;)```* &lt;b&gt;LLM runtime extends Hugging Face Transformers API to provide seamless low precision inference for popular LLMs, supporting mainstream low precision data types such as INT8/FP8/INT4/FP4/NF4.&lt;/b&gt;---&lt;div align=&quot;left&quot;&gt;## üèÉInstallation### Quick Install from Pypi```bashpip install intel-extension-for-transformers```&gt; For more installation methods, please refer to [Installation Page](docs/installation.md)## üåüIntroductionIntel¬Æ Extension for Transformers is an innovative toolkit to accelerate Transformer-based models on Intel platforms, in particular effective on 4th Intel Xeon Scalable processor¬†Sapphire Rapids (codenamed [Sapphire Rapids](https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/4th-gen-xeon-scalable-processors.html)). The toolkit provides the below key features and examples:*  Seamless user experience of model compressions on Transformer-based models by extending [Hugging Face transformers](https://github.com/huggingface/transformers)¬†APIs and leveraging [Intel¬Æ Neural Compressor](https://github.com/intel/neural-compressor)*  Advanced software optimizations and unique compression-aware runtime (released with NeurIPS 2022's paper [Fast Distilbert on CPUs](https://arxiv.org/abs/2211.07715) and [QuaLA-MiniLM: a Quantized Length Adaptive MiniLM](https://arxiv.org/abs/2210.17114), and NeurIPS 2021's paper [Prune Once for All: Sparse Pre-Trained Language Models](https://arxiv.org/abs/2111.05754))*  Optimized Transformer-based model packages such as [Stable Diffusion](examples/huggingface/pytorch/text-to-image/deployment/stable_diffusion), [GPT-J-6B](examples/huggingface/pytorch/text-generation/deployment), [GPT-NEOX](examples/huggingface/pytorch/language-modeling/quantization#2-validated-model-list), [BLOOM-176B](examples/huggingface/pytorch/language-modeling/inference#BLOOM-176B), [T5](examples/huggingface/pytorch/summarization/quantization#2-validated-model-list), [Flan-T5](examples/huggingface/pytorch/summarization/quantization#2-validated-model-list) and end-to-end workflows such as [SetFit-based text classification](docs/tutorials/pytorch/text-classification/SetFit_model_compression_AGNews.ipynb) and [document level sentiment analysis (DLSA)](workflows/dlsa) *  [NeuralChat](intel_extension_for_transformers/neural_chat), a customizable chatbot framework to create your own chatbot within minutes by leveraging a rich set of plugins [Knowledge Retrieval](./intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval/README.md), [Speech Interaction](./intel_extension_for_transformers/neural_chat/pipeline/plugins/audio/README.md), [Query Caching](./intel_extension_for_transformers/neural_chat/pipeline/plugins/caching/README.md), [Security Guardrail](./intel_extension_for_transformers/neural_chat/pipeline/plugins/security/README.md).*  [Inference](intel_extension_for_transformers/llm/runtime/graph) of Large Language Model (LLM) in pure C/C++ with weight-only quantization kernels, supporting [GPT-NEOX](intel_extension_for_transformers/llm/runtime/graph/models/gptneox), [LLAMA](intel_extension_for_transformers/llm/runtime/graph/models/llama), [MPT](intel_extension_for_transformers/llm/runtime/graph/models/mpt), [FALCON](intel_extension_for_transformers/llm/runtime/graph/models/falcon), [BLOOM-7B](intel_extension_for_transformers/llm/runtime/graph/models/bloom), [OPT](intel_extension_for_transformers/llm/runtime/graph/models/opt), [ChatGLM2-6B](intel_extension_for_transformers/llm/runtime/graph/models/chatglm), [GPT-J-6B](intel_extension_for_transformers/llm/runtime/graph/models/gptj) and [Dolly-v2-3B](intel_extension_for_transformers/llm/runtime/graph/models/gptneox)## üå±Getting StartedBelow are the sample code to enable weight-only low precision inference. See more [examples](intel_extension_for_transformers/llm/runtime/graph).### INT4 Inference ```pythonfrom transformers import AutoTokenizerfrom intel_extension_for_transformers.transformers import AutoModel, WeightOnlyQuantConfigmodel_name = &quot;EleutherAI/gpt-j-6B&quot;config = WeightOnlyQuantConfig(compute_dtype=&quot;int8&quot;, weight_dtype=&quot;int4&quot;)prompt = &quot;Once upon a time, a little girl&quot;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_idsmodel = AutoModel.from_pretrained(model_name, quantization_config=config)gen_tokens = model.generate(inputs, max_new_tokens=300)gen_text = tokenizer.batch_decode(gen_tokens)```### INT8 Inference```pythonfrom transformers import AutoTokenizerfrom intel_extension_for_transformers.transformers import AutoModel, WeightOnlyQuantConfigmodel_name = &quot;EleutherAI/gpt-j-6B&quot; config = WeightOnlyQuantConfig(compute_dtype=&quot;bf16&quot;, weight_dtype=&quot;int8&quot;)prompt = &quot;Once upon a time, a little girl&quot;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_idsmodel = AutoModel.from_pretrained(model_name, quantization_config=config)gen_tokens = model.generate(inputs, max_new_tokens=300)gen_text = tokenizer.batch_decode(gen_tokens)```## üéØValidated  ModelsHere is the average accuracy of validated models on Lambada (OpenAI), HellaSwag, Winogrande, PIQA, and WikiText.The next token latency is based on 32 input tokens and greedy search on Intel's 4th Generation Xeon Scalable Sapphire Rapids processor.| Model |  FP32         | INT4 (Group size 32) | INT4 (Group size 128) | Next Token Latency   | |---------------------|:----------------------:|:-----------------------:|:----------------------------:|:------------:| | [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B) | 0.643 | 0.644 | 0.64 | 21.98ms|| [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) | 0.69 | 0.69 | 0.685 | 24.55ms|| [decapoda-research/llama-7b-hf](https://huggingface.co/decapoda-research/llama-7b-hf) | 0.689 | 0.682 | 0.68 | 24.84ms|| [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) | 0.674 | 0.672 | 0.669 | 80.16ms|| [mosaicml/mpt-7b-chat](https://huggingface.co/mosaicml/mpt-7b-chat) | 0.672 | 0.67 | 0.666 | 35.84ms|| [tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b) | 0.698 | 0.694 | 0.693 |36.1ms|| [baichuan-inc/baichuan-7B](https://huggingface.co/baichuan-inc/baichuan-7B) | 0.474 | 0.471 | 0.47 | Coming Soon || [facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b) | 0.65 | 0.647 | 0.643 | Coming Soon || [databricks/dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b) | 0.613 | 0.609 | 0.609 |22.02ms|| [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) | 0.756 | 0.757 | 0.755 | Coming Soon |Find other models like ChatGLM, ChatGLM2, StarCoder... in [LLM Runtime](./intel_extension_for_transformers/llm/runtime/graph) ## üìñDocumentation&lt;table&gt;&lt;thead&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;OVERVIEW&lt;/th&gt;  &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs&quot;&gt;Model Compression&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/neural_chat&quot;&gt;NeuralChat&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs&quot;&gt;Neural Engine&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/kernels/README.md&quot;&gt;Kernel Libraries&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;MODEL COMPRESSION&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/quantization.md&quot;&gt;Quantization&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/pruning.md&quot;&gt;Pruning&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/distillation.md&quot;&gt;Distillation&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;examples/huggingface/pytorch/text-classification/orchestrate_optimizations/README.md&quot;&gt;Orchestration&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;examples/huggingface/pytorch/language-modeling/nas/README.md&quot;&gt;Neural Architecture Search&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/export.md&quot;&gt;Export&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/metrics.md&quot;&gt;Metrics&lt;/a&gt;/&lt;a href=&quot;docs/objectives.md&quot;&gt;Objectives&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/pipeline.md&quot;&gt;Pipeline&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;NEURAL ENGINE&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs/onnx_compile.md&quot;&gt;Model Compilation&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs/add_customized_pattern.md&quot;&gt;Custom Pattern&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs/deploy_and_integration.md&quot;&gt;Deployment&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs/engine_profiling.md&quot;&gt;Profiling&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;KERNEL LIBRARIES&lt;/th&gt;  &lt;/tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/kernels/docs/kernel_desc&quot;&gt;Sparse GEMM Kernels&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/kernels/docs/kernel_desc&quot;&gt;Custom INT8 Kernels&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/kernels/docs/profiling.md&quot;&gt;Profiling&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/test/kernels/benchmark/benchmark.md&quot;&gt;Benchmark&lt;/a&gt;&lt;/td&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;ALGORITHMS&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td align=&quot;center&quot; colspan=&quot;4&quot;&gt;&lt;a href=&quot;examples/huggingface/pytorch/question-answering/dynamic/README.md&quot;&gt;Length Adaptive&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;4&quot;&gt;&lt;a href=&quot;docs/data_augmentation.md&quot;&gt;Data Augmentation&lt;/a&gt;&lt;/td&gt;      &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;TUTORIALS AND RESULTS&lt;/a&gt;&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/tutorials/pytorch&quot;&gt;Tutorials&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/examples.md&quot;&gt;Supported Models&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs/validated_model.md&quot;&gt;Model Performance&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/kernels/docs/validated_data.md&quot;&gt;Kernel Performance&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;## üìÉSelected Publications/Events* Intel Innovation'23 Keynote: [Intel Innovation 2023 Keynote by Greg Lavender](https://www.youtube.com/watch?v=RbKRELWP9y8&amp;t=2954s) (Sep 2023)* Blog on Intel Community: [NeuralChat: A Customizable Chatbot Framework](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/NeuralChat-A-Customizable-Chatbot-Framework/post/1526789) (Sep 2023)* Blog published on Medium: [NeuralChat: A Customizable Chatbot Framework](https://medium.com/intel-analytics-software/make-your-own-chatbot-within-a-few-minutes-with-neuralchat-a-customizable-chatbot-framework-139b4bdec8d1) (Sep 2023)* Blog published on Medium: [Faster Stable Diffusion Inference with Intel Extension for Transformers](https://medium.com/intel-analytics-software/faster-stable-diffusion-inference-with-intel-extension-for-transformers-on-intel-platforms-7e0f563186b0) (July 2023)* Blog of Intel Developer News: [The Moat Is Trust, Or Maybe Just Responsible AI](https://www.intel.com/content/www/us/en/developer/articles/technical/moat-is-trust-minimizing-risks-generative-ai.html) (July 2023)* Blog of Intel Developer News: [Create Your Own Custom Chatbot](https://www.intel.com/content/www/us/en/developer/articles/technical/train-large-language-models-create-custom-chatbot.html) (July 2023)* Blog of Intel Developer News: [Accelerate Llama 2 with Intel AI Hardware and Software Optimizations](https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html) (July 2023)* Arxiv: [An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs](https://arxiv.org/abs/2306.16601) (June 2023)* Blog published on Medium: [Simplify Your Custom Chatbot Deployment](https://medium.com/intel-analytics-software/simplify-your-custom-chatbot-deployment-on-intel-platforms-c8a911d906cf) (June 2023)&gt; View [Full Publication List](./docs/publication.md).## Additional Content* [Release Information](./docs/release.md)* [Contribution Guidelines](./docs/contributions.md)* [Legal Information](./docs/legal.md)* [Security Policy](SECURITY.md)## Acknowledgements* Excellent open-source projects: [bitsandbytes](https://github.com/TimDettmers/bitsandbytes), [FastChat](https://github.com/lm-sys/FastChat), [fastRAG](https://github.com/IntelLabs/fastRAG), [ggml](https://github.com/ggerganov/ggml), [gptq](https://github.com/IST-DASLab/gptq), [llama.cpp](https://github.com/ggerganov/llama.cpp), [lm-evauation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [peft](https://github.com/huggingface/peft), [trl](https://github.com/huggingface/trl), and many others.## üíÅCollaborationsWelcome to raise any interesting ideas on model compression techniques and LLM-based chatbot development! Feel free to reach [us](mailto:itrex.maintainers@intel.com) and look forward to our collaborations on Intel Extension for Transformers!</longdescription>
</pkgmetadata>