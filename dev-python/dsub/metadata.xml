<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># dsub: simple batch jobs with Docker[![License](https://img.shields.io/badge/license-Apache%202.0-brightgreen.svg)](https://github.com/DataBiosphere/dsub/blob/main/LICENSE)## Overview`dsub` is a command-line tool that makes it easy to submit and run batch scriptsin the cloud.The `dsub` user experience is modeled after traditional high-performancecomputing job schedulers like Grid Engine and Slurm. You write a script andthen submit it to a job scheduler from a shell prompt on your local machine.Today `dsub` supports Google Cloud as the backend batch job runner, along with alocal provider for development and testing. With help from the community, we'dlike to add other backends, such as a Grid Engine, Slurm, Amazon Batch,and Azure Batch.## Getting started`dsub` is written in Python and requires Python 3.7 or higher.* The last version to support Python 3.6 was `dsub` [0.4.7](https://github.com/DataBiosphere/dsub/releases/tag/v0.4.7).* For earlier versions of Python 3, use `dsub` [0.4.1](https://github.com/DataBiosphere/dsub/releases/tag/v0.4.1).* For Python 2, use `dsub` [0.3.10](https://github.com/DataBiosphere/dsub/releases/tag/v0.3.10).### Pre-installation steps#### Create a Python virtual environmentThis is optional, but whether installing from PyPI or from github,you are strongly encouraged to use a[Python virtual environment](https://docs.python.org/3/library/venv.html).You can do this in a directory of your choosing.        python3 -m venv dsub_libs        source dsub_libs/bin/activateUsing a Python virtual environment isolates `dsub` library dependencies fromother Python applications on your system.Activate this virtual environment in any shell session before running `dsub`.To deactivate the virtual environment in your shell, run the command:        deactivateAlternatively, a set of convenience scripts are provided that activate thevirutalenv before calling `dsub`, `dstat`, and `ddel`. They are in the[bin](https://github.com/DataBiosphere/dsub/tree/main/bin) directory. You canuse these scripts if you don't want to activate the virtualenv explicitly inyour shell.#### Install the Google Cloud SDKWhile not used directly by `dsub` for the `google-v2` or `google-cls-v2` providers, you are likely to want to install the command line tools found in the [GoogleCloud SDK](https://cloud.google.com/sdk/).If you will be using the `local` provider for faster job development,you *will* need to install the Google Cloud SDK, which uses `gsutil` to ensurefile operation semantics consistent with the Google `dsub` providers.1. [Install the Google Cloud SDK](https://cloud.google.com/sdk/)2. Run        gcloud init    `gcloud` will prompt you to set your default project and to grant    credentials to the Google Cloud SDK.### Install `dsub`Choose **one** of the following:#### Install from PyPI1.  If necessary, [install pip](https://pip.pypa.io/en/stable/installing/).1.  Install `dsub`         pip install dsub#### Install from github1.  Be sure you have git installed    Instructions for your environment can be found on the    [git website](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).1.  Clone this repository.        git clone https://github.com/DataBiosphere/dsub        cd dsub1.  Install dsub (this will also install the dependencies)        python -m pip install .1.  Set up Bash tab completion (optional).        source bash_tab_complete### Post-installation steps1.  Minimally verify the installation by running:        dsub --help1.  (Optional) [Install Docker](https://docs.docker.com/engine/installation/).    This is necessary only if you're going to create your own Docker images or    use the `local` provider.### MakefileAfter cloning the dsub repo, you can also use the[Makefile](https://github.com/DataBiosphere/dsub/blob/main/Makefile)by running:        makeThis will create a Python virtual environment and install `dsub` into adirectory named `dsub_libs`.### Getting started with the local providerWe think you'll find the `local` provider to be very helpful when buildingyour `dsub` tasks. Instead of submitting a request to run your command on acloud VM, the `local` provider runs your `dsub` tasks on your local machine.The `local` provider is not designed for running at scale. It is designedto emulate running on a cloud VM such that you can rapidly iterate.You'll get quicker turnaround times and won't incur cloud charges using it.1. Run a `dsub` job and wait for completion.    Here is a very simple &quot;Hello World&quot; test:        dsub \          --provider local \          --logging &quot;${TMPDIR:-/tmp}/dsub-test/logging/&quot; \          --output OUT=&quot;${TMPDIR:-/tmp}/dsub-test/output/out.txt&quot; \          --command 'echo &quot;Hello World&quot; &gt; &quot;${OUT}&quot;' \          --wait    Note: `TMPDIR` is commonly set to `/tmp` by default on most Unix systems,    although it is also often left unset.    On some versions of MacOS TMPDIR is set to a location under `/var/folders`.    Note: The above syntax `${TMPDIR:-/tmp}` is known to be supported by Bash, zsh, ksh.    The shell will expand `TMPDIR`, but if it is unset, `/tmp` will be used.1. View the output file.        cat &quot;${TMPDIR:-/tmp}/dsub-test/output/out.txt&quot;### Getting started on Google Cloud`dsub` supports the use of two different APIs from Google Cloud for runningtasks. Google Cloud is transitioning from `Genomics v2alpha1`to [Cloud Life Sciences v2beta](https://cloud.google.com/life-sciences/docs/reference/rest).`dsub` supports both APIs with the (old) `google-v2` and (new) `google-cls-v2`providers respectively. `google-v2` is the current default provider. `dsub`will be transitioning to make `google-cls-v2` the default in coming releases.The steps for getting started differ slightly as indicated in the steps below:1.  Sign up for a Google account and    [create a project](https://console.cloud.google.com/project?).1.  Enable the APIs:    - For the `v2alpha1` API (provider: `google-v2`):     [Enable the Genomics, Storage, and Compute APIs](https://console.cloud.google.com/flows/enableapi?apiid=genomics,storage_component,compute_component&amp;redirect=https://console.cloud.google.com).    - For the `v2beta` API (provider: `google-cls-v2`):     [Enable the Cloud Life Sciences, Storage, and Compute APIs](https://console.cloud.google.com/flows/enableapi?apiid=lifesciences.googleapis.com,storage_component,compute_component&amp;redirect=https://console.cloud.google.com)1. Provide [credentials](https://developers.google.com/identity/protocols/application-default-credentials)    so `dsub` can call Google APIs:        gcloud auth application-default login1.  Create a [Google Cloud Storage](https://cloud.google.com/storage) bucket.    The dsub logs and output files will be written to a bucket. Create a    bucket using the [storage browser](https://console.cloud.google.com/storage/browser?project=)    or run the command-line utility [gsutil](https://cloud.google.com/storage/docs/gsutil),    included in the Cloud SDK.        gsutil mb gs://my-bucket    Change `my-bucket` to a unique name that follows the    [bucket-naming conventions](https://cloud.google.com/storage/docs/bucket-naming).    (By default, the bucket will be in the US, but you can change or    refine the [location](https://cloud.google.com/storage/docs/bucket-locations)    setting with the `-l` option.)1.  Run a very simple &quot;Hello World&quot; `dsub` job and wait for completion.    - For the `v2alpha1` API (provider: `google-v2`):            dsub \              --provider google-v2 \              --project my-cloud-project \              --regions us-central1 \              --logging gs://my-bucket/logging/ \              --output OUT=gs://my-bucket/output/out.txt \              --command 'echo &quot;Hello World&quot; &gt; &quot;${OUT}&quot;' \              --wait    Change `my-cloud-project` to your Google Cloud project, and `my-bucket` to    the bucket you created above.    - For the `v2beta` API (provider: `google-cls-v2`):            dsub \              --provider google-cls-v2 \              --project my-cloud-project \              --regions us-central1 \              --logging gs://my-bucket/logging/ \              --output OUT=gs://my-bucket/output/out.txt \              --command 'echo &quot;Hello World&quot; &gt; &quot;${OUT}&quot;' \              --wait    Change `my-cloud-project` to your Google Cloud project, and `my-bucket` to    the bucket you created above.    The output of the script command will be written to the `OUT` file in Cloud    Storage that you specify.1. View the output file.        gsutil cat gs://my-bucket/output/out.txt## Backend providersWhere possible, `dsub` tries to support users being able to develop and testlocally (for faster iteration) and then progressing to running at scale.To this end, `dsub` provides multiple &quot;backend providers&quot;, each of whichimplements a consistent runtime environment. The current providers are:- local- google-v2 (the default)- google-cls-v2 (*new*)More details on the runtime environment implemented by the backend providerscan be found in [dsub backend providers](https://github.com/DataBiosphere/dsub/blob/main/docs/providers/README.md).### Differences between `google-v2` and `google-cls-v2`The `google-cls-v2` provider is built on the Cloud Life Sciences `v2beta` API.This API is very similar to its predecessor, the Genomics `v2alpha1` API.Details of the differences can be found in the[Migration Guide](https://cloud.google.com/life-sciences/docs/how-tos/migration).`dsub` largely hides the differences between the two APIs, but there are afew difference to note:- `v2beta` is a regional service, `v2alpha1` is a global serviceWhat this means is that with `v2alpha1`, the metadata about your tasks(called &quot;operations&quot;), is stored in a global database, while with `v2beta`, themetadata about your tasks are stored in a regional database. If your operationinformation needs to stay in a particular region, use the `v2beta` API(the `google-cls-v2` provider), and specify the `--location` where youroperation information should be stored.- The `--regions` and `--zones` flags can be omitted when using `google-cls-v2`The `--regions` and `--zones` flags for `dsub` specify where the tasks shouldrun. More specifically, this specifies what Compute Engine Zones to use forthe VMs that run your tasks.With the `google-v2` provider, there is no default region or zone, and thusone of the `--regions` or `--zones` flags is required.With `google-cls-v2`, the `--location` flag defaults to `us-central1`, andif the `--regions` and `--zones` flags are omitted, the `location` will beused as the default `regions` list.## `dsub` featuresThe following sections show how to run more complex jobs.### Defining what code to runYou can provide a shell command directly in the dsub command-line, as in thehello example above.You can also save your script to a file, like `hello.sh`. Then you can run:    dsub \        ... \        --script hello.shIf your script has dependencies that are not stored in your Docker image,you can transfer them to the local disk. See the instructions below forworking with input and output files and folders.### Selecting a Docker imageTo get started more easily, `dsub` uses a stock Ubuntu Docker image.This default image may change at any time in future releases, so forreproducible production workflows, you should always specify the imageexplicitly.You can change the image by passing the `--image` flag.    dsub \        ... \        --image ubuntu:16.04 \        --script hello.shNote: your `--image` must include the[Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) shell interpreter.For more information on using the`--image` flag, see the[image section in Scripts, Commands, and Docker](https://github.com/DataBiosphere/dsub/blob/main/docs/code.md#--image-docker-image)### Passing parameters to your scriptYou can pass environment variables to your script using the `--env` flag.    dsub \        ... \        --env MESSAGE=hello \        --command 'echo ${MESSAGE}'The environment variable `MESSAGE` will be assigned the value `hello` whenyour Docker container runs.Your script or command can reference the variable like any other Linuxenvironment variable, as `${MESSAGE}`.**Be sure to enclose your command string in single quotes and not doublequotes. If you use double quotes, the command will be expanded in your localshell before being passed to dsub. For more information on using the`--command` flag, see [Scripts, Commands, and Docker](https://github.com/DataBiosphere/dsub/blob/main/docs/code.md)**To set multiple environment variables, you can repeat the flag:    --env VAR1=value1 \    --env VAR2=value2You can also set multiple variables, space-delimited, with a single flag:    --env VAR1=value1 VAR2=value2### Working with input and output files and foldersdsub mimics the behavior of a shared file system using cloud storagebucket paths for input and output files and folders. You specifythe cloud storage bucket path. Paths can be:* file paths like `gs://my-bucket/my-file`* folder paths like `gs://my-bucket/my-folder`* wildcard paths like `gs://my-bucket/my-folder/*`See the [inputs and outputs](https://github.com/DataBiosphere/dsub/blob/main/docs/input_output.md)documentation for more details.### Transferring input files to a Google Cloud Storage bucket.If your script expects to read local input files that are not alreadycontained within your Docker image, the files must be available in GoogleCloud Storage.If your script has dependent files, you can make them available to your scriptby: * Building a private Docker image with the dependent files and publishing the   image to a public site, or privately to Google Container Registry * Uploading the files to Google Cloud StorageTo upload the files to Google Cloud Storage, you can use the[storage browser](https://console.cloud.google.com/storage/browser?project=) or[gsutil](https://cloud.google.com/storage/docs/gsutil). You can also run on datathatâ€™s public or shared with your service account, an email address that youcan find in the [Google Cloud Console](https://console.cloud.google.com).#### FilesTo specify input and output files, use the `--input` and `--output` flags:    dsub \        ... \        --input INPUT_FILE_1=gs://my-bucket/my-input-file-1 \        --input INPUT_FILE_2=gs://my-bucket/my-input-file-2 \        --output OUTPUT_FILE=gs://my-bucket/my-output-file \        --command 'cat &quot;${INPUT_FILE_1}&quot; &quot;${INPUT_FILE_2}&quot; &gt; &quot;${OUTPUT_FILE}&quot;'In this example:- a file will be copied from `gs://my-bucket/my-input-file-1` to a path on the data disk- the path to the file on the data disk will be set in the environment variable `${INPUT_FILE_1}`- a file will be copied from `gs://my-bucket/my-input-file-2` to a path on the data disk- the path to the file on the data disk will be set in the environment variable `${INPUT_FILE_2}`The `--command` can reference the file paths using the environment variables.Also in this example:- a path on the data disk will be set in the environment variable `${OUTPUT_FILE}`- the output file will written to the data disk at the location given by `${OUTPUT_FILE}`After the `--command` completes, the output file will be copied to the bucket path `gs://my-bucket/my-output-file`Multiple `--input`, and `--output` parameters can be specified andthey can be specified in any order.#### FoldersTo copy folders rather than files, use the `--input-recursive` and`output-recursive` flags:    dsub \        ... \        --input-recursive FOLDER=gs://my-bucket/my-folder \        --command 'find ${FOLDER} -name &quot;foo*&quot;'Multiple `--input-recursive`, and `--output-recursive` parameters can bespecified and they can be specified in any order.#### Mounting &quot;resource data&quot;While explicitly specifying inputs improves tracking provenance of your data,there are cases where you might not want to expliclty localize all inputsfrom Cloud Storage to your job VM.For example, if you have:- a large set of resource files- your code only reads a subset of those files- runtime decisions of which files to readOR- a large input file over which your code makes a single read passOR- a large input file that your code does not read in its entiretythen you may find it more efficient or convenient to access this data bymounting read-only:- a Google Cloud Storage bucket- a persistent disk that you pre-create and populate- a persistent disk that gets created from a[Compute Engine Image](https://cloud.google.com/compute/docs/images) that youpre-create.The `google-v2` and `google-cls-v2` providers support these methods ofproviding access to resource data.The `local` provider supports mounting alocal directory in a similar fashion to support your local development.##### Mounting a Google Cloud Storage bucketTo have the `google-v2` or `google-cls-v2` provider mount a Cloud Storage bucket usingCloud Storage FUSE, use the `--mount` command line flag:    --mount RESOURCES=gs://mybucketThe bucket will be mounted into the Docker container running your `--script`or `--command` and the location made available via the environment variable`${RESOURCES}`. Inside your script, you can reference the mounted path using theenvironment variable. Please read[Key differences from a POSIX file system](https://cloud.google.com/storage/docs/gcs-fuse#notes)and [Semantics](https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/semantics.md)before using Cloud Storage FUSE.##### Mounting an existing peristent diskTo have the `google-v2` or `google-cls-v2` provider mount a persistent disk thatyou have pre-created and populated, use the `--mount` command line flag and theurl of the source disk:    --mount RESOURCES=&quot;https://www.googleapis.com/compute/v1/projects/your-project/zones/your_disk_zone/disks/your-disk&quot;##### Mounting a persistent disk, created from an imageTo have the `google-v2` or `google-cls-v2` provider mount a persistent disk created from an image,use the `--mount` command line flag and the url of the source image and the size(in GB) of the disk:    --mount RESOURCES=&quot;https://www.googleapis.com/compute/v1/projects/your-project/global/images/your-image 50&quot;The image will be used to create a new persistent disk, which will be attachedto a Compute Engine VM. The disk will mounted into the Docker container runningyour `--script` or `--command` and the location made available by theenvironment variable `${RESOURCES}`. Inside your script, you can reference themounted path using the environment variable.To create an image, see [Creating a custom image](https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images).##### Mounting a local directory (`local` provider)To have the `local` provider mount a directory read-only, use the `--mount`command line flag and a `file://` prefix:    --mount RESOURCES=file://path/to/my/dirThe local directory will be mounted into the Docker container running your`--script`or `--command` and the location made available via the environmentvariable `${RESOURCES}`. Inside your script, you can reference the mountedpath using the environment variable.### Setting resource requirements`dsub` tasks run using the `local` provider will use the resources available onyour local machine.`dsub` tasks run using the `google`, `google-v2`, or `google-cls-v2` providers can take advantageof a wide range of CPU, RAM, disk, and hardware accelerator (eg. GPU) options.See the [Compute Resources](https://github.com/DataBiosphere/dsub/blob/main/docs/compute_resources.md)documentation for details.### Job IdentifiersBy default, `dsub` generates a `job-id` with the form`job-name--userid--timestamp` where the `job-name` is truncated at 10 charactersand the `timestamp` is of the form `YYMMDD-HHMMSS-XX`, unique to hundredths of asecond. If you are submitting multiple jobs concurrently, you may still run intosituations where the `job-id` is not unique. If you require a unique `job-id`for this situation, you may use the `--unique-job-id` parameter.If the `--unique-job-id` parameter is set, `job-id` will instead be a unique 32character UUID created by https://docs.python.org/3/library/uuid.html. Becausesome providers require that the `job-id` begin with a letter, `dsub` willreplace any starting digit with a letter in a manner that preserves uniqueness.### Submitting a batch jobEach of the examples above has demonstrated submitting a single task witha single set of variables, inputs, and outputs. If you have a batch of inputsand you want to run the same operation over them, `dsub` allows youto create a batch job.Instead of calling `dsub` repeatedly, you can createa tab-separated values (TSV) file containing the variables,inputs, and outputs for each task, and then call `dsub` once.The result will be a single `job-id` with multiple tasks. The tasks willbe scheduled and run independently, but can be[monitored](https://github.com/DataBiosphere/dsub#viewing-job-status) and[deleted](https://github.com/DataBiosphere/dsub#deleting-a-job) as a group.#### Tasks file formatThe first line of the TSV file specifies the names and types of theparameters. For example:    --env SAMPLE_ID&lt;tab&gt;--input VCF_FILE&lt;tab&gt;--output OUTPUT_PATHEach addition line in the file should provide the variable, input, and outputvalues for each task. Each line beyond the header represents the values for aseparate task.Multiple `--env`, `--input`, and `--output` parameters can be specified andthey can be specified in any order. For example:    --env SAMPLE&lt;tab&gt;--input A&lt;tab&gt;--input B&lt;tab&gt;--env REFNAME&lt;tab&gt;--output O    S1&lt;tab&gt;gs://path/A1.txt&lt;tab&gt;gs://path/B1.txt&lt;tab&gt;R1&lt;tab&gt;gs://path/O1.txt    S2&lt;tab&gt;gs://path/A2.txt&lt;tab&gt;gs://path/B2.txt&lt;tab&gt;R2&lt;tab&gt;gs://path/O2.txt#### Tasks parameterPass the TSV file to dsub using the `--tasks` parameter. This parameteraccepts both the file path and optionally a range of tasks to process.The file may be read from the local filesystem (on the machine you're calling`dsub` from), or from a bucket in Google Cloud Storage (file name starts with&quot;gs://&quot;).For example, suppose `my-tasks.tsv` contains 101 lines: a one-line header and100 lines of parameters for tasks to run. Then:    dsub ... --tasks ./my-tasks.tsvwill create a job with 100 tasks, while:    dsub ... --tasks ./my-tasks.tsv 1-10will create a job with 10 tasks, one for each of lines 2 through 11.The task range values can take any of the following forms:*   `m` indicates to submit task `m` (line m+1)*   `m-` indicates to submit all tasks starting with task `m`*   `m-n` indicates to submit all tasks from `m` to `n` (inclusive).### LoggingThe `--logging` flag points to a location for `dsub` task log files. For detailson how to specify your logging path, see [Logging](https://github.com/DataBiosphere/dsub/blob/main/docs/logging.md).### Job controlIt's possible to wait for a job to complete before starting another.For details, see [job control with dsub](https://github.com/DataBiosphere/dsub/blob/main/docs/job_control.md).### RetriesIt is possible for `dsub` to automatically retry failed tasks.For details, see [retries with dsub](https://github.com/DataBiosphere/dsub/blob/main/docs/retries.md).### Labeling jobs and tasksYou can add custom labels to jobs and tasks, which allows you to monitor andcancel tasks using your own identifiers. In addition, with the Googleproviders, labeling a task will label associated compute resources such asvirtual machines and disks.For more details, see [Checking Status and Troubleshooting Jobs](https://github.com/DataBiosphere/dsub/blob/main/docs/troubleshooting.md)### Viewing job statusThe `dstat` command displays the status of jobs:    dstat --provider google-v2 --project my-cloud-projectWith no additional arguments, dstat will display a list of *running* jobs forthe current `USER`.To display the status of a specific job, use the `--jobs` flag:    dstat --provider google-v2 --project my-cloud-project --jobs job-idFor a batch job, the output will list all *running* tasks.Each job submitted by dsub is given a set of metadata values that can beused for job identification and job control. The metadata associated witheach job includes:*   `job-name`: defaults to the name of your script file or the first word of    your script command; it can be explicitly set with the `--name` parameter.*   `user-id`: the `USER` environment variable value.*   `job-id`: identifier of the job, which can be used in calls to `dstat` and    `ddel` for job monitoring and canceling respectively. See    [Job Identifiers](https://github.com/DataBiosphere/dsub#job-identifiers) for more    details on the `job-id` format.*   `task-id`: if the job is submitted with the `--tasks` parameter, each task    gets a sequential value of the form &quot;task-*n*&quot; where *n* is 1-based.Note that the job metadata values will be modified to conform with the &quot;LabelRestrictions&quot; listed in the [Checking Status and Troubleshooting Jobs](https://github.com/DataBiosphere/dsub/blob/main/docs/troubleshooting.md)guide.Metadata can be used to cancel a job or individual tasks within a batch job.For more details, see [Checking Status and Troubleshooting Jobs](https://github.com/DataBiosphere/dsub/blob/main/docs/troubleshooting.md)#### Summarizing job statusBy default, dstat outputs one line per task. If you're using a batch job withmany tasks then you may benefit from `--summary`.```$ dstat --provider google-v2 --project my-project --status '*' --summaryJob Name        Status         Task Count-------------   -------------  -------------my-job-name     RUNNING        2my-job-name     SUCCESS        1```In this mode, dstat prints one line per (job name, task status) pair. You cansee at a glance how many tasks are finished, how many are still running, andhow many are failed/canceled.### Deleting a jobThe `ddel` command will delete running jobs.By default, only jobs submitted by the current user will be deleted.Use the `--users` flag to specify other users, or `'*'` for all users.To delete a running job:    ddel --provider google-v2 --project my-cloud-project --jobs job-idIf the job is a batch job, all running tasks will be deleted.To delete specific tasks:    ddel \        --provider google-v2 \        --project my-cloud-project \        --jobs job-id \        --tasks task-id1 task-id2To delete all running jobs for the current user:    ddel --provider google-v2 --project my-cloud-project --jobs '*'## Service Accounts and Scope (Google providers only)When you run the `dsub` command with the `google-v2` or `google-cls-v2`provider, there are two different sets of credentials to consider:- Account submitting the `pipelines.run()` request to run your command/script on a VM- Account accessing Cloud resources (such as files in GCS) when executing your command/scriptThe account used to submit the `pipelines.run()` request is typically yourend user credentials. You would have set this up by running:    gcloud auth application-default loginThe account used on the VM is a [service account](https://cloud.google.com/iam/docs/service-accounts).The image below illustrates this:![Pipelines Runner Architecture](./docs/images/pipelines_runner_architecture.png)By default, `dsub` will use the [default Compute Engine service account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account)as the authorized service account on the VM instance. You can choose to specifythe email address of another service acount using `--service-account`.By default, `dsub` will grant the following access scopes to the service account:- https://www.googleapis.com/auth/bigquery- https://www.googleapis.com/auth/compute- https://www.googleapis.com/auth/devstorage.full_control- https://www.googleapis.com/auth/genomics- https://www.googleapis.com/auth/logging.write- https://www.googleapis.com/auth/monitoring.writeIn addition, [the API](https://cloud.google.com/life-sciences/docs/reference/rest/v2beta/projects.locations.pipelines/run#serviceaccount) will always add this scope:- https://www.googleapis.com/auth/cloud-platformYou can choose to specify scopes using `--scopes`.### Recommendations for service accountsWhile it is straightforward to use the default service account, this account alsohas broad privileges granted to it by default. Following the[Principle of Least Privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege)you may want to create and use a service account that has only sufficient privilegesgranted in order to run your `dsub` command/script.To create a new service account, follow the steps below:1. Execute the `gcloud iam service-accounts create` command. The email addressof the service account will be `sa-name@project-id.iam.gserviceaccount.com`.        gcloud iam service-accounts create &quot;sa-name&quot;2. Grant IAM access on buckets, etc. to the service account.        gsutil iam ch serviceAccount:sa-name@project-id.iam.gserviceaccount.com:roles/storage.objectAdmin gs://bucket-name3. Update your `dsub` command to include `--service-account`        dsub \          --service-account sa-name@project-id.iam.gserviceaccount.com          ...## What next?*   See the examples:    *   [Custom scripts](https://github.com/DataBiosphere/dsub/tree/main/examples/custom_scripts)    *   [Decompress files](https://github.com/DataBiosphere/dsub/tree/main/examples/decompress)    *   [FastQC](https://github.com/DataBiosphere/dsub/tree/main/examples/fastqc)    *   [Samtools index](https://github.com/DataBiosphere/dsub/tree/main/examples/samtools)*   See more documentation for:    *   [Scripts, Commands, and Docker](https://github.com/DataBiosphere/dsub/blob/main/docs/code.md)    *   [Input and Output File Handling](https://github.com/DataBiosphere/dsub/blob/main/docs/input_output.md)    *   [Logging](https://github.com/DataBiosphere/dsub/blob/main/docs/logging.md)    *   [Compute Resources](https://github.com/DataBiosphere/dsub/blob/main/docs/compute_resources.md)    *   [Compute Quotas](https://github.com/DataBiosphere/dsub/blob/main/docs/compute_quotas.md)    *   [Job Control](https://github.com/DataBiosphere/dsub/blob/main/docs/job_control.md)    *   [Retries](https://github.com/DataBiosphere/dsub/blob/main/docs/retries.md)    *   [Checking Status and Troubleshooting Jobs](https://github.com/DataBiosphere/dsub/blob/main/docs/troubleshooting.md)    *   [Backend providers](https://github.com/DataBiosphere/dsub/blob/main/docs/providers/README.md)</longdescription>
</pkgmetadata>