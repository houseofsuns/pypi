<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Lexikos - ŒªŒµŒæŒπŒ∫œåœÇ /lek.si.k√≥s/A collection of pronunciation dictionaries and neural grapheme-to-phoneme models.&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;https://github.com/bookbot-hive/lexikos/raw/main/assets/lexikos.png&quot; alt=&quot;logo&quot; width=&quot;300&quot;/&gt;&lt;/p&gt;## Install LexikosInstall from PyPI```shpip install lexikos```Editable install from Source```shgit clone https://github.com/bookbot-hive/lexikos.gitpip install -e lexikos```## Usage### Lexicon```py&gt;&gt;&gt; from lexikos import Lexicon&gt;&gt;&gt; lexicon = Lexicon()&gt;&gt;&gt; print(lexicon[&quot;added&quot;]){'Àà√¶ d …™ d', 'Àà√¶ …æ …ô d', '√¶ …æ …™ d', 'a d …™ d', 'Ààa d …™ d', '√¶ …æ …ô d', 'Ààa d …ô d', 'a d …ô d', 'Àà√¶ d …ô d', '√¶ d …ô d', '√¶ d …™ d', 'Àà√¶ …æ …™ d'}&gt;&gt;&gt; print(lexicon[&quot;runner&quot;]){'…π  å n …ö', '…π  å n …ô', '…π  å n …ù', 'Ààr  å n …ù'}&gt;&gt;&gt; print(lexicon[&quot;water&quot;]){'Àà ã aÀê  à …ô r …Ø', 'Ààw oÀê t …ô', 'w …ë t …ô …π', 'Ààw aÀê  à …ô r …Ø', 'Ààw …î t …ù', 'w …î t …ô …π', 'Ààw …ë t …ô …π', 'w …î t …ù', 'w …ë …æ …ö', 'Ààw …ë …æ …ö', 'Àà ã …î  à …ô r', 'w …î …æ …ö', 'w …îÀê t …ô', 'Ààw oÀê …æ …ô', 'Ààw …î  à …ô r'}```To get a lexicon where phonemes are normalized (diacritics removed, digraphs split):```py&gt;&gt;&gt; from lexikos import Lexicon&gt;&gt;&gt; lexicon = Lexicon(normalize_phonemes=True)&gt;&gt;&gt; print(lexicon[&quot;added&quot;]){'√¶ …æ …™ d', 'a d …™ d', 'a d …ô d', '√¶ …æ …ô d', '√¶ d …ô d', '√¶ d …™ d'}&gt;&gt;&gt; print(lexicon[&quot;runner&quot;]){'…π  å n …ö', '…π  å n …ô', 'r  å n …ù', '…π  å n …ù'}&gt;&gt;&gt; print(lexicon[&quot;water&quot;]){'w o …æ …ô', 'w …î t …ô', ' ã …î  à …ô r', 'w a  à …ô r …Ø', 'w …î t …ô …π', ' ã a  à …ô r …Ø', 'w …ë …æ …ö', 'w o t …ô', 'w …î t …ù', 'w …î  à …ô r', 'w …î …æ …ö', 'w …ë t …ô …π'}```To include synthetic (non-dictionary-based) pronunciations:```py&gt;&gt;&gt; from lexikos import Lexicon&gt;&gt;&gt; lexicon = Lexicon(include_synthetic=True)&gt;&gt;&gt; print(lexicon[&quot;athletic&quot;]){'√¶ t l …õ t …™ k', '√¶ Œ∏ Ààl …õ t …™ k', '√¶ Œ∏ l …õ t …™ k'}```### Phonemization```py&gt;&gt;&gt; from lexikos import G2p&gt;&gt;&gt; g2p = G2p(lang=&quot;en-us&quot;)&gt;&gt;&gt; g2p(&quot;Hello there! $100 is not a lot of money in 2023.&quot;)['h …õ l o  ä', '√∞ …õ …ô …π', 'w  å n', 'h  å n d …π …™ d', 'd …ë l …ö z', '…™ z', 'n …í t', '…ô', 'l …ë t', ' å v', 'm  å n i', '…™ n', 't w …õ n t i', 't w …õ n t i', 'Œ∏ …π iÀê']&gt;&gt;&gt; g2p = G2p(lang=&quot;en-au&quot;)&gt;&gt;&gt; g2p(&quot;Hi there mate! Have a g'day!&quot;)['h a …™', 'Œ∏ …õ …ô …π', 'm e …™ t', 'h e …™ v', '…ô', '…° …ô Ààd √¶ …™']```## Dictionaries &amp; Models### English `(en)`| Language | Dictionary | Phone Set | Corpus                                       | G2P Model                                                                                           || -------- | ---------- | --------- | -------------------------------------------- | --------------------------------------------------------------------------------------------------- || en       | Wikipron   | IPA       | [Link](./lexikos/dict/wikipron/eng_latn.tsv) | [bookbot/byt5-small-wikipron-eng-latn](https://huggingface.co/bookbot/byt5-small-wikipron-eng-latn) |### English `(en-US)`| Language       | Dictionary   | Phone Set | Corpus                                                                                                                     | G2P Model                                                                                                             || -------------- | ------------ | --------- | -------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- || en-US          | CMU Dict     | ARPA      | [External Link](https://github.com/microsoft/CNTK/blob/master/Examples/SequenceToSequence/CMUDict/Data/cmudict-0.7b.train) | [bookbot/byt5-small-cmudict](https://huggingface.co/bookbot/byt5-small-cmudict)                                       || en-US          | CMU Dict IPA | IPA       | [External Link](https://github.com/menelik3/cmudict-ipa/blob/master/cmudict-0.7b-ipa.txt)                                  |                                                                                                                       || en-US          | CharsiuG2P   | IPA       | [External Link](https://github.com/lingjzhu/CharsiuG2P/blob/main/dicts/eng-us.tsv)                                         | [charsiu/g2p_multilingual_byT5_small_100](https://huggingface.co/charsiu/g2p_multilingual_byT5_small_100)             || en-US (Broad)  | Wikipron     | IPA       | [External Link](https://github.com/CUNY-CL/wikipron/blob/master/data/scrape/tsv/eng_latn_us_broad.tsv)                     | [bookbot/byt5-small-wikipron-eng-latn-us-broad](https://huggingface.co/bookbot/byt5-small-wikipron-eng-latn-us-broad) || en-US (Narrow) | Wikipron     | IPA       | [External Link](https://github.com/CUNY-CL/wikipron/blob/master/data/scrape/tsv/eng_latn_us_narrow.tsv)                    || en-US          | LibriSpeech  | IPA       | [Link](./lexikos/dict/cmudict-ipa/librispeech-lexicon-200k-allothers-g2p-ipa.tsv)                                          |                                                                                                                       |### English `(en-UK)`| Language       | Dictionary | Phone Set | Corpus                                                                                                  | G2P Model                                                                                                             || -------------- | ---------- | --------- | ------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- || en-UK          | CharsiuG2P | IPA       | [External Link](https://github.com/lingjzhu/CharsiuG2P/blob/main/dicts/eng-uk.tsv)                      | [charsiu/g2p_multilingual_byT5_small_100](https://huggingface.co/charsiu/g2p_multilingual_byT5_small_100)             || en-UK (Broad)  | Wikipron   | IPA       | [External Link](https://github.com/CUNY-CL/wikipron/blob/master/data/scrape/tsv/eng_latn_uk_broad.tsv)  | [bookbot/byt5-small-wikipron-eng-latn-uk-broad](https://huggingface.co/bookbot/byt5-small-wikipron-eng-latn-uk-broad) || en-UK (Narrow) | Wikipron   | IPA       | [External Link](https://github.com/CUNY-CL/wikipron/blob/master/data/scrape/tsv/eng_latn_uk_narrow.tsv) |                                                                                                                       |### English `(en-AU)`| Language       | Dictionary | Phone Set | Corpus                                                 | G2P Model                                                                                                             || -------------- | ---------- | --------- | ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- || en-AU (Broad)  | Wikipron   | IPA       | [Link](./lexikos/dict/wikipron/eng_latn_au_broad.tsv)  | [bookbot/byt5-small-wikipron-eng-latn-au-broad](https://huggingface.co/bookbot/byt5-small-wikipron-eng-latn-au-broad) || en-AU (Narrow) | Wikipron   | IPA       | [Link](./lexikos/dict/wikipron/eng_latn_au_narrow.tsv) |                                                                                                                       || en-AU          | AusTalk    | IPA       | [Link](./lexikos/dict/asr-data/austalk_en_au.tsv)      |                                                                                                                       || en-AU          | SC-CW      | IPA       | [Link](./lexikos/dict/asr-data/sc_cw_en_au.tsv)        |                                                                                                                       |### English `(en-CA)`| Language       | Dictionary | Phone Set | Corpus                                                 | G2P Model                                                                                                             || -------------- | ---------- | --------- | ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- || en-CA (Broad)  | Wikipron   | IPA       | [Link](./lexikos/dict/wikipron/eng_latn_ca_broad.tsv)  | [bookbot/byt5-small-wikipron-eng-latn-ca-broad](https://huggingface.co/bookbot/byt5-small-wikipron-eng-latn-ca-broad) || en-CA (Narrow) | Wikipron   | IPA       | [Link](./lexikos/dict/wikipron/eng_latn_ca_narrow.tsv) |                                                                                                                       |### English `(en-NZ)`| Language       | Dictionary | Phone Set | Corpus                                                 | G2P Model                                                                                                             || -------------- | ---------- | --------- | ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- || en-NZ (Broad)  | Wikipron   | IPA       | [Link](./lexikos/dict/wikipron/eng_latn_nz_broad.tsv)  | [bookbot/byt5-small-wikipron-eng-latn-nz-broad](https://huggingface.co/bookbot/byt5-small-wikipron-eng-latn-nz-broad) || en-NZ (Narrow) | Wikipron   | IPA       | [Link](./lexikos/dict/wikipron/eng_latn_nz_narrow.tsv) |                                                                                                                       |### English `(en-IN)`| Language       | Dictionary | Phone Set | Corpus                                                 | G2P Model                                                                                                             || -------------- | ---------- | --------- | ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- || en-IN (Broad)  | Wikipron   | IPA       | [Link](./lexikos/dict/wikipron/eng_latn_in_broad.tsv)  | [bookbot/byt5-small-wikipron-eng-latn-in-broad](https://huggingface.co/bookbot/byt5-small-wikipron-eng-latn-in-broad) || en-IN (Narrow) | Wikipron   | IPA       | [Link](./lexikos/dict/wikipron/eng_latn_in_narrow.tsv) |                                                                                                                       |## Training G2P ModelWe modified the sequence-to-sequence training script of [ü§ó HuggingFace](https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation.py) for the purpose of training G2P models. Refer to their [installation requirements](https://github.com/huggingface/transformers/tree/main/examples/pytorch/translation) for more details.Training a new G2P model generally follow this recipe:```diffpython run_translation.py \+   --model_name_or_path $PRETRAINED_MODEL \+   --dataset_name $DATASET_NAME \    --output_dir $OUTPUT_DIR \    --per_device_train_batch_size 128 \    --per_device_eval_batch_size 32 \    --learning_rate 2e-4 \    --lr_scheduler_type linear \    --warmup_ratio 0.1 \    --num_train_epochs 10 \    --evaluation_strategy epoch \    --save_strategy epoch \    --logging_strategy epoch \    --max_source_length 64 \    --max_target_length 64 \    --val_max_target_length 64 \    --pad_to_max_length True \    --overwrite_output_dir \    --do_train --do_eval \    --bf16 \    --predict_with_generate \    --report_to tensorboard \    --push_to_hub \+   --hub_model_id $HUB_MODEL_ID \    --use_auth_token```### Example: Fine-tune ByT5 on CMU Dict```shpython run_translation.py \    --model_name_or_path google/byt5-small \    --dataset_name bookbot/cmudict-0.7b \    --output_dir ./byt5-small-cmudict \    --per_device_train_batch_size 128 \    --per_device_eval_batch_size 32 \    --learning_rate 2e-4 \    --lr_scheduler_type linear \    --warmup_ratio 0.1 \    --num_train_epochs 10 \    --evaluation_strategy epoch \    --save_strategy epoch \    --logging_strategy epoch \    --max_source_length 64 \    --max_target_length 64 \    --val_max_target_length 64 \    --pad_to_max_length True \    --overwrite_output_dir \    --do_train --do_eval \    --bf16 \    --predict_with_generate \    --report_to tensorboard \    --push_to_hub \    --hub_model_id bookbot/byt5-small-cmudict \    --use_auth_token```## Evaluating G2P ModelThen to evaluate:```diffpython eval.py \+   --model $PRETRAINED_MODEL \+   --dataset_name $DATASET_NAME \    --source_text_column_name source \    --target_text_column_name target \    --max_length 64 \    --batch_size 64```### Example: Evaluate ByT5 on CMU Dict```shpython eval.py \    --model bookbot/byt5-small-cmudict \    --dataset_name bookbot/cmudict-0.7b \    --source_text_column_name source \    --target_text_column_name target \    --max_length 64 \    --batch_size 64```## Corpus Roadmap### Wikipron| Language Family        | Code                              | Region                                                | Corpus | G2P Model || ---------------------- | --------------------------------- | ----------------------------------------------------- | :----: | :-------: || African English        | en-ZA                             | South Africa                                          |        |           || Australian English     | en-AU                             | Australia                                             |   ‚úÖ    |     ‚úÖ     || East Asian English     | en-CN, en-HK, en-JP, en-KR, en-TW | China, Hong Kong, Japan, South Korea, Taiwan          |        |           || European English       | en-UK, en-HU, en-IE               | United Kingdom, Hungary, Ireland                      |   üöß    |     üöß     || Mexican English        | en-MX                             | Mexico                                                |        |           || New Zealand English    | en-NZ                             | New Zealand                                           |   ‚úÖ    |     ‚úÖ     || North American         | en-CA, en-US                      | Canada, United States                                 |   ‚úÖ    |     ‚úÖ     || Middle Eastern English | en-EG, en-IL                      | Egypt, Israel                                         |        |           || Southeast Asian        | en-TH, en-ID, en-MY, en-PH, en-SG | Thailand, Indonesia, Malaysia, Philippines, Singapore |        |           || South Asian English    | en-IN                             | India                                                 |   ‚úÖ    |     ‚úÖ     |  ## Resources- [CharsiuG2P](https://github.com/lingjzhu/CharsiuG2P)- [Microsoft CNTK](https://github.com/microsoft/CNTK/tree/master)- [CMU Pronouncing Dictionary - IPA](https://github.com/menelik3/cmudict-ipa)- [Wikipron](https://github.com/CUNY-CL/wikipron/tree/master)## References```bibtex@inproceedings{lee-etal-2020-massively,    title = &quot;Massively Multilingual Pronunciation Modeling with {W}iki{P}ron&quot;,    author = &quot;Lee, Jackson L.  and      Ashby, Lucas F.E.  and      Garza, M. Elizabeth  and      Lee-Sikka, Yeonju  and      Miller, Sean  and      Wong, Alan  and      McCarthy, Arya D.  and      Gorman, Kyle&quot;,    booktitle = &quot;Proceedings of LREC&quot;,    year = &quot;2020&quot;,    publisher = &quot;European Language Resources Association&quot;,    pages = &quot;4223--4228&quot;,}``````bibtex@misc{zhu2022byt5,    title={ByT5 model for massively multilingual grapheme-to-phoneme conversion},     author={Jian Zhu and Cong Zhang and David Jurgens},    year={2022},    eprint={2204.03067},    archivePrefix={arXiv},    primaryClass={cs.CL}}```</longdescription>
</pkgmetadata>