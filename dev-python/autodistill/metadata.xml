<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;  &lt;p&gt;    &lt;a align=&quot;center&quot; href=&quot;&quot; target=&quot;_blank&quot;&gt;      &lt;img        width=&quot;850&quot;        src=&quot;https://media.roboflow.com/open-source/autodistill/autodistill-banner.jpg?2&quot;      &gt;    &lt;/a&gt;  &lt;/p&gt;&lt;/div&gt;[![version](https://badge.fury.io/py/autodistill.svg?)](https://badge.fury.io/py/autodistill)[![downloads](https://img.shields.io/pypi/dm/autodistill)](https://pypistats.org/packages/autodistill)[![license](https://img.shields.io/pypi/l/autodistill?)](https://github.com/autodistill/autodistill/blob/main/LICENSE)[![python-version](https://img.shields.io/pypi/pyversions/autodistill)](https://badge.fury.io/py/autodistill)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb)Autodistill uses big, slower foundation models to train small, faster supervised models. Using `autodistill`, you can go from unlabeled images to inference on a custom model running at the edge with no human intervention in between.&lt;div align=&quot;center&quot;&gt;  &lt;p&gt;    &lt;a align=&quot;center&quot; href=&quot;&quot; target=&quot;_blank&quot;&gt;      &lt;img        width=&quot;850&quot;        src=&quot;https://media.roboflow.com/open-source/autodistill/steps.jpg&quot;      &gt;    &lt;/a&gt;  &lt;/p&gt;&lt;/div&gt;Currently, `autodistill` supports vision tasks like object detection and instance segmentation, but in the future it can be expanded to support language (and other) models.## üîó Quicklinks| [Tutorial](https://blog.roboflow.com/autodistill)| [Docs](https://docs.autodistill.com)| [Supported Models](#-available-models)  | [Contribute](CONTRIBUTING.md)|:---:|:---:|:---:|:---:|## üëÄ Example OutputHere are example predictions of a Target Model detecting milk bottles and bottlecaps after being trained on an auto-labeled dataset using Autodistill (see [the Autodistill YouTube video](https://www.youtube.com/watch?v=gKTYMfwPo4M) for a full walkthrough):&lt;div align=&quot;center&quot;&gt;  &lt;p&gt;    &lt;a align=&quot;center&quot; href=&quot;https://www.youtube.com/watch?v=gKTYMfwPo4M&quot; target=&quot;_blank&quot;&gt;      &lt;img        width=&quot;850&quot;        src=&quot;https://media.roboflow.com/open-source/autodistill/milk-480.gif&quot;      &gt;    &lt;/a&gt;  &lt;/p&gt;&lt;/div&gt;## üöÄ Features* üîå Pluggable interface to connect models together* ü§ñ Automatically label datasets* üê∞ Train fast supervised models* üîí Own your model* üöÄ Deploy distilled models to the cloud or the edge## üìö Basic ConceptsTo use `autodistill`, you input unlabeled data into a Base Model which uses an Ontology to label a Dataset that is used to train a Target Model which outputs a Distilled Model fine-tuned to perform a specific Task.&lt;div align=&quot;center&quot;&gt;  &lt;p&gt;    &lt;a align=&quot;center&quot; href=&quot;&quot; target=&quot;_blank&quot;&gt;      &lt;img        width=&quot;850&quot;        src=&quot;https://media.roboflow.com/open-source/autodistill/overview.jpg&quot;      &gt;    &lt;/a&gt;  &lt;/p&gt;&lt;/div&gt;Autodistill defines several basic primitives:* **Task** - A Task defines what a Target Model will predict. The Task for each component (Base Model, Ontology, and Target Model) of an `autodistill` pipeline must match for them to be compatible with each other. Object Detection and Instance Segmentation are currently supported through the `detection` task. `classification` support will be added soon.* **Base Model** - A Base Model is a large foundation model that knows a lot about a lot. Base models are often multimodal and can perform many tasks. They're large, slow, and expensive. Examples of Base Models are GroundedSAM and GPT-4's upcoming multimodal variant. We use a Base Model (along with unlabeled input data and an Ontology) to create a Dataset.* **Ontology** - an Ontology defines how your Base Model is prompted, what your Dataset will describe, and what your Target Model will predict. A simple Ontology is the `CaptionOntology` which prompts a Base Model with text captions and maps them to class names. Other Ontologies may, for instance, use a CLIP vector or example images instead of a text caption.* **Dataset** - a Dataset is a set of auto-labeled data that can be used to train a Target Model. It is the output generated by a Base Model.* **Target Model** - a Target Model is a supervised model that consumes a Dataset and outputs a distilled model that is ready for deployment. Target Models are usually small, fast, and fine-tuned to perform a specific task very well (but they don't generalize well beyond the information described in their Dataset). Examples of Target Models are YOLOv8 and DETR.* **Distilled Model** - a Distilled Model is the final output of the `autodistill` process; it's a set of weights fine-tuned for your task that can be deployed to get predictions.## üí° Theory and LimitationsHuman labeling is one of the biggest barriers to broad adoption of computer vision. It can take thousands of hours to craft a dataset suitable for training a production model. The process of distillation for training supervised models is not new, in fact, traditional human labeling is just another form of distillation from an extremely capable Base Model (the human brain üß†).Foundation models know a lot about a lot, but for production we need models that know a lot about a little.As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process. We need tools for steering, utilizing, and comparing these models. Additionally, these foundation models are big, expensive, and often gated behind private APIs. For many production use-cases, we need models that can run cheaply and in realtime at the edge.&lt;div align=&quot;center&quot;&gt;  &lt;p&gt;    &lt;a align=&quot;center&quot; href=&quot;&quot; target=&quot;_blank&quot;&gt;      &lt;img        width=&quot;850&quot;        src=&quot;https://media.roboflow.com/open-source/autodistill/connections.jpg&quot;      &gt;    &lt;/a&gt;  &lt;/p&gt;&lt;/div&gt;Autodistill's Base Models can already create datasets for many common use-cases (and through creative prompting and few-shotting we can expand their utility to many more), but they're not perfect yet. There's still a lot of work to do; this is just the beginning and we'd love your help testing and expanding the capabilities of the system!## üíø InstallationAutodistill is modular. You'll need to install the `autodistill` package (which defines the interfaces for the above concepts) along with [Base Model and Target Model plugins](#-available-models) (which implement specific models).By packaging these separately as plugins, dependency and licensing incompatibilities are minimized and new models can be implemented and maintained by anyone.Example: ```bashpip install autodistill autodistill-grounded-sam autodistill-yolov8```&lt;details close&gt;&lt;summary&gt;Install from source&lt;/summary&gt;You can also clone the project from GitHub for local development:```bashgit clone https://github.com/roboflow/autodistillcd autodistillpip install -e .```&lt;/details&gt;Additional Base and Target models are [enumerated below](#-available-models).## üöÄ QuickstartSee the [demo Notebook](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb) for a quick introduction to `autodistill`. This notebook walks through building a milk container detection model with no labeling.Below, we have condensed key parts of the notebook for a quick introduction to `autodistill`.### Install PackagesFor this example, we'll show how to distill [GroundedSAM](https://github.com/IDEA-Research/Grounded-Segment-Anything) into a small [YOLOv8](https://github.com/ultralytics/ultralytics) model using [autodistill-grounded-sam](https://github.com/autodistill/autodistill-grounded-sam) and [autodistill-yolov8](https://github.com/autodistill/autodistill-yolov8).```pip install autodistill autodistill-grounded-sam autodistill-yolov8```### Distill a Model```pythonfrom autodistill_grounded_sam import GroundedSAMfrom autodistill.detection import CaptionOntologyfrom autodistill_yolov8 import YOLOv8# define an ontology to map class names to our GroundingDINO prompt# the ontology dictionary has the format {caption: class}# where caption is the prompt sent to the base model, and class is the label that will# be saved for that caption in the generated annotationsbase_model = GroundedSAM(ontology=CaptionOntology({&quot;shipping container&quot;: &quot;container&quot;}))# label all images in a folder called `context_images`base_model.label(  input_folder=&quot;./images&quot;,  output_folder=&quot;./dataset&quot;)target_model = YOLOv8(&quot;yolov8n.pt&quot;)target_model.train(&quot;./dataset/data.yaml&quot;, epochs=200)# run inference on the new modelpred = target_model.predict(&quot;./dataset/valid/your-image.jpg&quot;, confidence=0.5)print(pred)# optional: upload your model to Roboflow for deploymentfrom roboflow import Roboflowrf = Roboflow(api_key=&quot;API_KEY&quot;)project = rf.workspace().project(&quot;PROJECT_ID&quot;)project.version(DATASET_VERSION).deploy(model_type=&quot;yolov8&quot;, model_path=f&quot;./runs/detect/train/&quot;)```&lt;details close&gt;&lt;summary&gt;Visualize Predictions&lt;/summary&gt;To plot the annotations for a single image using `autodistill`, you can use the code below. This code is helpful to visualize the annotations generated by your base model (i.e. GroundedSAM) and the results from your target model (i.e. YOLOv8).```pythonimport supervision as svimport cv2img_path = &quot;./images/your-image.jpeg&quot;image = cv2.imread(img_path)detections = base_model.predict(img_path)# annotate image with detectionsbox_annotator = sv.BoxAnnotator()labels = [    f&quot;{base_model.ontology.classes()[class_id]} {confidence:0.2f}&quot;    for _, _, confidence, class_id, _ in detections]annotated_frame = box_annotator.annotate(    scene=image.copy(), detections=detections, labels=labels)sv.plot_image(annotated_frame, (16, 16))```&lt;/details&gt;## üìç Available ModelsOur goal is for `autodistill` to support using all foundation models as Base Models and most SOTA supervised models as Target Models. We focused on object detection and segmentationtasks first but plan to launch classification support soon! In the future, we hope `autodistill` will also be used for models beyond computer vision.* ‚úÖ - complete (click row/column header to go to repo)* üöß - work in progress### object detection| base / target | [YOLOv8](https://github.com/autodistill/autodistill-yolov8) | [YOLO-NAS](https://github.com/autodistill/autodistill-yolonas) | [YOLOv5](https://github.com/autodistill/autodistill-yolov5) | [DETR](https://github.com/autodistill/autodistill-detr) | YOLOv7 | MT-YOLOv6 ||:---:|:---:|:---:|:---:|:---:|:---:|:---:|| [DETIC](https://github.com/autodistill/autodistill-detic) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  || [GroundedSAM](https://github.com/autodistill/autodistill-grounded-sam) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  || [GroundingDINO](https://github.com/autodistill/autodistill-grounding-dino) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  || [OWL-ViT](https://github.com/autodistill/autodistill-owl-vit) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  || [SAM-CLIP](https://github.com/autodistill/autodistill-sam-clip) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  || Azure DenseCaptions |  |  |  |  |  |  || GLIPv2 |  |  |  |  |  |  |### instance segmentation| base / target | [YOLOv8](https://github.com/autodistill/autodistill-yolov8) | [YOLO-NAS](https://github.com/autodistill/autodistill-yolonas) | [YOLOv5](https://github.com/autodistill/autodistill-yolov5) | YOLOv7 | Segformer ||:---:|:---:|:---:|:---:|:---:|:---:|| [GroundedSAM](https://github.com/autodistill/autodistill-grounded-sam) | ‚úÖ | üöß | üöß |  |  || SAM-CLIP | üöß | üöß | üöß |  |  |### classification| base / target | [ViT](https://github.com/autodistill/autodistill-vit) | [YOLOv8](https://github.com/autodistill/autodistill-yolov8) | [YOLOv5](https://github.com/autodistill/autodistill-yolov5) ||:---:|:---:|:---:|:---:|| [CLIP](https://github.com/autodistill/autodistill-clip) | üöß | üöß | üöß || [DINOv2](https://github.com/autodistill/autodistill-dinov2) | üöß | üöß | üöß || BLIP | üöß | üöß | üöß || ALBEF | üöß | üöß | üöß || GPT-4 |  |  |  || Open Flamingo |  |  |  || PaLM-2 |  |  |  |## Roboflow Model Deployment SupportYou can optionally deploy some Target Models trained using Autodistill on Roboflow. Deploying on Roboflow allows you to use a range of concise SDKs for using your model on the edge, from [roboflow.js](https://docs.roboflow.com/inference/web-browser) for web deployment to [NVIDIA Jetson](https://docs.roboflow.com/inference/nvidia-jetson) devices.The following Autodistill Target Models are supported by Roboflow for deployment:| model name | Supported? ||:---:|:---:|| YOLOv8 Object Detection | ‚úÖ || YOLOv8 Instance Segmentation | ‚úÖ || YOLOv5 Object Detection | ‚úÖ || YOLOv5 Instance Segmentation | ‚úÖ || YOLOv8 Classification |  |## üé¨ Video Guides&lt;p align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=gKTYMfwPo4M&quot; title=&quot;Autodistill: Train YOLOv8 with ZERO Annotations&quot;&gt;&lt;img src=&quot;https://i.ytimg.com/vi/gKTYMfwPo4M/maxresdefault.jpg&quot; alt=&quot;Autodistill: Train YOLOv8 with ZERO Annotations&quot; width=&quot;300px&quot; align=&quot;left&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;https://youtu.be/oEQYStnF2l8&quot;&gt;&lt;strong&gt;Autodistill: Train YOLOv8 with ZERO Annotations&lt;/strong&gt;&lt;/a&gt;&lt;div&gt;&lt;strong&gt;Published: 8 June 2023&lt;/strong&gt;&lt;/div&gt;&lt;br/&gt;In this video, we will show you how to use a new library to train a YOLOv8 model to detect bottles moving on a conveyor line. Yes, that's right - zero annotation hours are required! We dive deep into Autodistill's functionality, covering topics from setting up your Python environment and preparing your images, to the thrilling automatic annotation of images. &lt;/p&gt; ## üí° Community Resources- [Distill Large Vision Models into Smaller, Efficient Models with Autodistill](https://blog.roboflow.com/autodistill/): Announcement post with written guide on how to use Autodistill- [Comparing AI-Labeled Data to Human-Labeled Data](https://blog.roboflow.com/ai-vs-human-labeled-data/): A qualitative evaluation of Grounding DINO used with Autodistill across various tasks and domains.- [How to Evaluate Autodistill Prompts with CVevals](https://blog.roboflow.com/autodistill-prompt-evaluation/): Evaluate Autodistill prompts.- [Autodistill: Label and Train a Computer Vision Model in Under 20 Minutes](https://www.youtube.com/watch?v=M_QZ_Q0zT0k): Building a model to detect planes in under 20 minutes.- File a PR to add your own resources here!## üó∫Ô∏è RoadmapApart from adding new models, there are several areas we plan to explore with `autodistill` including:* üí° Ontology creation &amp; prompt engineering* üë©üíª Human in the loop support* ü§î Model evaluation* üîÑ Active learning* üí¨ Language tasks## üèÜ ContributingWe love your input! Please see our [contributing guide](CONTRIBUTING.md) to get started. Thank you üôè to all our contributors!## üë©‚öñÔ∏è LicenseThe `autodistill` package is licensed under an [Apache 2.0](LICENSE). Each Base or Target model plugin may use its own license corresponding with the license of its underlying model. Please refer to the license in each plugin repo for more information.</longdescription>
</pkgmetadata>